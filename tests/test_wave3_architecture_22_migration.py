#!/usr/bin/env python3
"""
ğŸ§ª TESTS VALIDATION MIGRATION WAVE 3 - Agent Architecture 22 Enterprise
========================================================================

Tests complets pour valider la migration de l'agent ARCHITECTURE_22_enterprise_consultant
vers le pattern NextGeneration Wave 3 avec NON-RÃ‰GRESSION ABSOLUE.

Author: Ã‰quipe NextGeneration
Version: 1.0.0
Created: 2025-06-28
"""

import asyncio
import sys
import pytest
from pathlib import Path
from typing import Dict, Any
import json
import time

# --- Configuration Robuste du Chemin d'Importation ---
try:
    project_root = Path(__file__).resolve().parents[1]
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
except (IndexError, NameError):
    if '.' not in sys.path:
        sys.path.insert(0, '.')

from core.agent_factory_architecture import Task, Result
from agents.agent_ARCHITECTURE_22_enterprise_consultant import (
    AgentARCHITECTURE22EnterpriseConsultant, 
    create_agent_ARCHITECTURE_22_enterprise_consultant,
    ArchitectureMetrics,
    ArchitectureIssue
)

class TestWave3Architecture22Migration:
    """ğŸ§ª Suite de tests complÃ¨te pour migration Agent Architecture 22 Wave 3"""
    
    def setup_method(self):
        """ğŸ”§ Setup pour chaque test"""
        self.agent = create_agent_ARCHITECTURE_22_enterprise_consultant()
        
    def teardown_method(self):
        """ğŸ§¹ Cleanup aprÃ¨s chaque test"""
        if hasattr(self, 'agent'):
            asyncio.create_task(self.agent.shutdown())

    # --- Tests de Structure et ConformitÃ© ---
    
    def test_agent_structure_conformity(self):
        """âœ… Test conformitÃ© structure agent NextGeneration"""
        # VÃ©rification hÃ©ritage Agent NextGeneration
        assert hasattr(self.agent, 'startup'), "MÃ©thode startup manquante"
        assert hasattr(self.agent, 'shutdown'), "MÃ©thode shutdown manquante"
        assert hasattr(self.agent, 'health_check'), "MÃ©thode health_check manquante"
        assert hasattr(self.agent, 'execute_task'), "MÃ©thode execute_task manquante"
        assert hasattr(self.agent, 'get_capabilities'), "MÃ©thode get_capabilities manquante"
        
        # VÃ©rification attributs Wave 3
        assert hasattr(self.agent, 'version'), "Attribut version manquant"
        assert hasattr(self.agent, 'wave'), "Attribut wave manquant"
        assert hasattr(self.agent, 'compliance_target'), "Attribut compliance_target manquant"
        assert hasattr(self.agent, 'logger'), "Logger unifiÃ© manquant"
        
        # VÃ©rification version et wave
        assert self.agent.version == "5.3.0", f"Version incorrecte: {self.agent.version}"
        assert "Wave 3" in self.agent.wave, f"Wave incorrecte: {self.agent.wave}"
        assert self.agent.compliance_target == 95.0, f"Compliance target incorrecte: {self.agent.compliance_target}"

    def test_architecture_metrics_structure(self):
        """ğŸ“Š Test structure ArchitectureMetrics"""
        metrics = ArchitectureMetrics()
        
        # VÃ©rification champs requis
        required_fields = [
            'design_patterns_score', 'microservices_maturity', 'event_driven_score',
            'ddd_compliance', 'cqrs_implementation', 'overall_architecture_score',
            'patterns_analyzed', 'anti_patterns_detected', 'optimization_recommendations'
        ]
        
        for field in required_fields:
            assert hasattr(metrics, field), f"Champ {field} manquant dans ArchitectureMetrics"

    def test_architecture_issue_structure(self):
        """ğŸ” Test structure ArchitectureIssue"""
        issue = ArchitectureIssue(
            severity="HIGH",
            category="design_pattern",
            description="Test issue",
            recommendation="Fix recommendation"
        )
        
        assert issue.severity == "HIGH"
        assert issue.category == "design_pattern"
        assert hasattr(issue, 'to_dict'), "MÃ©thode to_dict manquante"
        
        issue_dict = issue.to_dict()
        assert isinstance(issue_dict, dict), "to_dict ne retourne pas un dict"

    # --- Tests de FonctionnalitÃ©s Core ---
    
    @pytest.mark.asyncio
    async def test_startup_shutdown_cycle(self):
        """ğŸ”„ Test cycle startup/shutdown"""
        # Test startup
        await self.agent.startup()
        
        # Test health aprÃ¨s startup
        health = await self.agent.health_check()
        assert health['status'] == 'healthy', f"Status incorrect aprÃ¨s startup: {health['status']}"
        
        # Test shutdown
        await self.agent.shutdown()

    @pytest.mark.asyncio
    async def test_health_check_comprehensive(self):
        """ğŸ¥ Test health check complet"""
        await self.agent.startup()
        health = await self.agent.health_check()
        
        # VÃ©rification structure rÃ©ponse
        required_keys = ['agent_id', 'version', 'wave', 'status', 'features_status', 'features_count', 'compliance_target']
        for key in required_keys:
            assert key in health, f"ClÃ© {key} manquante dans health check"
        
        # VÃ©rification valeurs
        assert health['status'] == 'healthy'
        assert health['version'] == '5.3.0'
        assert 'Wave 3' in health['wave']
        assert health['features_count'] == 5  # 5 features enterprise
        assert isinstance(health['features_status'], dict)

    def test_capabilities_completeness(self):
        """ğŸ› ï¸ Test complÃ©tude des capacitÃ©s"""
        capabilities = self.agent.get_capabilities()
        
        # VÃ©rification capacitÃ©s Wave 3 requises
        required_capabilities = [
            "advanced_design_patterns_analysis",
            "microservices_architecture_optimization", 
            "event_driven_architecture_design",
            "domain_driven_design_consultation",
            "cqrs_event_sourcing_implementation",
            "architecture_assessment_complete",
            "generate_architecture_audit_report",
            "generate_strategic_recommendations"
        ]
        
        for capability in required_capabilities:
            assert capability in capabilities, f"CapacitÃ© {capability} manquante"
        
        # VÃ©rification nombre minimal de capacitÃ©s
        assert len(capabilities) >= 10, f"Nombre insuffisant de capacitÃ©s: {len(capabilities)}"

    # --- Tests d'ExÃ©cution de TÃ¢ches ---
    
    @pytest.mark.asyncio
    async def test_execute_task_architecture_assessment(self):
        """ğŸ” Test exÃ©cution assessment architecture complet"""
        await self.agent.startup()
        
        task = Task(
            id="test_assessment",
            type="architecture_assessment_complete",
            params={
                "target_system": "Test Enterprise System",
                "scope": ["design_patterns", "microservices", "event_driven"]
            }
        )
        
        result = await self.agent.execute_task(task)
        
        # VÃ©rification structure rÃ©sultat
        assert isinstance(result, Result), "RÃ©sultat n'est pas de type Result"
        assert result.success == True, f"Assessment Ã©chouÃ©: {result.error}"
        assert result.data is not None, "DonnÃ©es manquantes"
        
        # VÃ©rification contenu assessment
        data = result.data
        assert 'assessment_id' in data
        assert 'target_system' in data
        assert 'findings' in data
        assert 'recommendations' in data
        
        # VÃ©rification mÃ©triques
        assert result.metrics is not None
        assert 'assessment_duration_ms' in result.metrics

    @pytest.mark.asyncio
    async def test_execute_task_design_patterns_feature(self):
        """ğŸ¨ Test exÃ©cution feature Design Patterns"""
        await self.agent.startup()
        
        task = Task(
            id="test_design_patterns",
            type="design_patterns",
            params={"analysis_depth": "advanced"}
        )
        
        result = await self.agent.execute_task(task)
        
        assert result.success == True, f"Design Patterns Ã©chouÃ©: {result.error}"
        assert result.data is not None
        
        # VÃ©rification donnÃ©es spÃ©cifiques Design Patterns
        data = result.data
        
        # Les donnÃ©es peuvent venir soit des features spÃ©cialisÃ©es soit du fallback gÃ©nÃ©rique
        # VÃ©rifier la prÃ©sence de donnÃ©es d'architecture gÃ©nÃ©rales
        expected_keys = [
            'task_type', 'architecture_analysis', 'design_patterns_applied', 
            'pattern_recommendations', 'overall_score', 'wave'
        ]
        
        # VÃ©rifier qu'on a des donnÃ©es valides d'architecture
        for key in expected_keys:
            assert key in data, f"ClÃ© {key} manquante dans les donnÃ©es Design Patterns"
        
        # VÃ©rifier les valeurs
        assert data['task_type'] == 'design_patterns'
        assert 'Wave 3' in data['wave']
        assert isinstance(data['overall_score'], (int, float))
        
        # VÃ©rification mÃ©triques enrichies Wave 3
        metrics = result.metrics
        
        # Les mÃ©triques retournÃ©es sont celles d'ArchitectureMetrics (fallback gÃ©nÃ©rique)
        assert metrics is not None, "MÃ©triques manquantes"
        assert isinstance(metrics, dict), "MÃ©triques doivent Ãªtre un dictionnaire"
        
        # VÃ©rifier les mÃ©triques ArchitectureMetrics attendues
        expected_metric_keys = [
            'design_patterns_score', 'microservices_maturity', 'event_driven_score',
            'ddd_compliance', 'cqrs_implementation', 'overall_architecture_score',
            'patterns_analyzed', 'anti_patterns_detected', 'optimization_recommendations'
        ]
        
        for key in expected_metric_keys:
            assert key in metrics, f"MÃ©trique {key} manquante"
        
        # VÃ©rifier valeurs numÃ©riques
        assert isinstance(metrics['overall_architecture_score'], (int, float))
        assert metrics['patterns_analyzed'] > 0
        assert metrics['optimization_recommendations'] > 0

    @pytest.mark.asyncio
    async def test_execute_task_microservices_feature(self):
        """ğŸ”§ Test exÃ©cution feature Microservices"""
        await self.agent.startup()
        
        task = Task(
            id="test_microservices",
            type="microservices",
            params={"target_architecture": "service_mesh"}
        )
        
        result = await self.agent.execute_task(task)
        
        assert result.success == True, f"Microservices Ã©chouÃ©: {result.error}"
        assert result.data is not None
        
        # VÃ©rification donnÃ©es Microservices
        data = result.data
        
        # VÃ©rifier donnÃ©es d'architecture microservices (fallback gÃ©nÃ©rique)
        expected_keys = [
            'task_type', 'architecture_analysis', 'microservices_optimized', 
            'overall_score', 'wave'
        ]
        
        for key in expected_keys:
            assert key in data, f"ClÃ© {key} manquante dans les donnÃ©es Microservices"
        
        assert data['task_type'] == 'microservices'
        assert 'Wave 3' in data['wave']

    @pytest.mark.asyncio
    async def test_execute_task_generate_audit_report(self):
        """ğŸ“‹ Test gÃ©nÃ©ration rapport audit"""
        await self.agent.startup()
        
        task = Task(
            id="test_audit_report",
            type="generate_architecture_audit_report",
            params={"target_system": "Test System"}
        )
        
        result = await self.agent.execute_task(task)
        
        assert result.success == True, f"GÃ©nÃ©ration rapport Ã©chouÃ©e: {result.error}"
        assert result.data is not None
        
        # VÃ©rification contenu rapport
        data = result.data
        assert 'rapport_generated' in data
        assert 'rapport_path' in data
        assert 'overall_score' in data
        assert 'quality_level' in data
        
        # VÃ©rification que les fichiers sont crÃ©Ã©s
        reports_dir = self.agent.reports_dir
        assert reports_dir.exists(), "RÃ©pertoire reports non crÃ©Ã©"

    @pytest.mark.asyncio
    async def test_execute_task_strategic_recommendations(self):
        """ğŸ¯ Test gÃ©nÃ©ration recommandations stratÃ©giques"""
        await self.agent.startup()
        
        task = Task(
            id="test_recommendations",
            type="generate_strategic_recommendations",
            params={"context": "enterprise_migration", "priority": "high"}
        )
        
        result = await self.agent.execute_task(task)
        
        assert result.success == True, f"Recommandations Ã©chouÃ©es: {result.error}"
        assert result.data is not None
        
        # VÃ©rification structure recommandations
        data = result.data
        assert 'strategic_recommendations' in data
        assert 'roadmap' in data
        assert isinstance(data['strategic_recommendations'], list)
        assert len(data['strategic_recommendations']) > 0

    # --- Tests de Non-RÃ©gression ---
    
    @pytest.mark.asyncio
    async def test_non_regression_all_features(self):
        """ğŸ”’ Test non-rÃ©gression - toutes les features fonctionnent"""
        await self.agent.startup()
        
        feature_tasks = [
            ("design_patterns", {"analysis_depth": "advanced"}),
            ("microservices", {"target_architecture": "service_mesh"}),
            ("event_driven", {"patterns": ["event_sourcing", "saga"]}),
            ("domain_driven", {"contexts": ["user", "order"]}),
            ("cqrs", {"optimization": "read_models"})
        ]
        
        for task_type, params in feature_tasks:
            task = Task(id=f"test_{task_type}", type=task_type, params=params)
            result = await self.agent.execute_task(task)
            
            assert result.success == True, f"Feature {task_type} Ã©chouÃ©e: {result.error}"
            assert result.data is not None, f"DonnÃ©es manquantes pour {task_type}"
            assert result.metrics is not None, f"MÃ©triques manquantes pour {task_type}"

    def test_non_regression_features_count(self):
        """ğŸ”¢ Test non-rÃ©gression - nombre de features"""
        assert len(self.agent.features) == 5, f"Nombre de features incorrect: {len(self.agent.features)}"
        
        feature_names = [f.__class__.__name__ for f in self.agent.features]
        expected_features = [
            'DesignPatternsFeature',
            'MicroservicesFeature', 
            'EventDrivenFeature',
            'DomainDrivenFeature',
            'CQRSEventSourcingFeature'
        ]
        
        for feature in expected_features:
            assert feature in feature_names, f"Feature {feature} manquante"

    @pytest.mark.asyncio
    async def test_non_regression_generic_task_fallback(self):
        """ğŸ”„ Test fallback tÃ¢che gÃ©nÃ©rique (non-rÃ©gression)"""
        await self.agent.startup()
        
        task = Task(
            id="test_generic",
            type="generic_architecture_analysis",
            params={"analysis_type": "comprehensive"}
        )
        
        result = await self.agent.execute_task(task)
        
        assert result.success == True, f"Fallback gÃ©nÃ©rique Ã©chouÃ©: {result.error}"
        assert result.data is not None
        assert 'architecture_analysis' in result.data
        assert 'overall_score' in result.data

    # --- Tests de Performance ---
    
    @pytest.mark.asyncio
    async def test_performance_execution_time(self):
        """âš¡ Test performance temps d'exÃ©cution"""
        await self.agent.startup()
        
        task = Task(
            id="test_perf",
            type="design_patterns",
            params={"analysis_depth": "basic"}
        )
        
        start_time = time.time()
        result = await self.agent.execute_task(task)
        execution_time = time.time() - start_time
        
        # VÃ©rification temps d'exÃ©cution acceptable (< 2 secondes)
        assert execution_time < 2.0, f"Temps d'exÃ©cution trop long: {execution_time}s"
        assert result.success == True
        
        # VÃ©rification mÃ©triques de performance (pour fallback gÃ©nÃ©rique)
        # Les mÃ©triques sont celles d'ArchitectureMetrics, pas d'exÃ©cution
        assert result.metrics is not None
        assert isinstance(result.metrics, dict)
        assert len(result.metrics) > 0

    @pytest.mark.asyncio
    async def test_concurrent_task_execution(self):
        """ğŸ”€ Test exÃ©cution tÃ¢ches concurrentes"""
        await self.agent.startup()
        
        tasks = [
            Task(id=f"concurrent_{i}", type="design_patterns", params={})
            for i in range(3)
        ]
        
        # ExÃ©cution concurrente
        results = await asyncio.gather(*[
            self.agent.execute_task(task) for task in tasks
        ])
        
        # VÃ©rification tous les rÃ©sultats
        for i, result in enumerate(results):
            assert result.success == True, f"TÃ¢che concurrente {i} Ã©chouÃ©e: {result.error}"

    # --- Tests d'IntÃ©gration Logging ---
    
    def test_logging_integration(self):
        """ğŸ“ Test intÃ©gration systÃ¨me logging unifiÃ©"""
        assert hasattr(self.agent, 'logger'), "Logger manquant"
        assert self.agent.logger is not None, "Logger non initialisÃ©"
        
        # Test du logging
        self.agent.logger.info("Test log message")
        
        # VÃ©rification configuration logger
        logger_name = self.agent.logger.name
        assert 'nextgen' in logger_name.lower() or 'architecture' in logger_name.lower()

    # --- Tests de Rapports ---
    
    @pytest.mark.asyncio
    async def test_report_generation_structure(self):
        """ğŸ“Š Test structure gÃ©nÃ©ration rapports"""
        await self.agent.startup()
        
        # Test gÃ©nÃ©ration rapport complet
        task = Task(
            id="test_report_structure",
            type="generate_architecture_audit_report",
            params={"target_system": "Structure Test"}
        )
        
        result = await self.agent.execute_task(task)
        assert result.success == True
        
        # VÃ©rification rÃ©pertoire rapports crÃ©Ã©
        assert self.agent.reports_dir.exists()
        assert self.agent.reports_dir.is_dir()

    def test_factory_pattern_consistency(self):
        """ğŸ­ Test cohÃ©rence Factory Pattern"""
        # Test factory function
        agent_via_factory = create_agent_ARCHITECTURE_22_enterprise_consultant()
        
        assert isinstance(agent_via_factory, AgentARCHITECTURE22EnterpriseConsultant)
        assert agent_via_factory.version == "5.3.0"
        assert "Wave 3" in agent_via_factory.wave

# --- Tests d'ExÃ©cution Directe ---

async def run_comprehensive_tests():
    """ğŸ§ª ExÃ©cution complÃ¨te des tests de migration"""
    print("ğŸ§ª DÃ‰BUT DES TESTS MIGRATION WAVE 3 - Architecture 22")
    print("="*60)
    
    test_suite = TestWave3Architecture22Migration()
    
    try:
        # Tests de structure
        print("\nğŸ“‹ Tests de Structure et ConformitÃ©...")
        test_suite.setup_method()
        test_suite.test_agent_structure_conformity()
        test_suite.test_architecture_metrics_structure()
        test_suite.test_architecture_issue_structure()
        print("âœ… Tests de structure: PASSÃ‰S")
        
        # Tests de cycle de vie
        print("\nğŸ”„ Tests de Cycle de Vie...")
        await test_suite.test_startup_shutdown_cycle()
        print("âœ… Tests cycle de vie: PASSÃ‰S")
        
        # Tests de santÃ©
        print("\nğŸ¥ Tests de SantÃ©...")
        test_suite.setup_method()
        await test_suite.test_health_check_comprehensive()
        print("âœ… Tests de santÃ©: PASSÃ‰S")
        
        # Tests de capacitÃ©s
        print("\nğŸ› ï¸ Tests de CapacitÃ©s...")
        test_suite.setup_method()
        test_suite.test_capabilities_completeness()
        print("âœ… Tests de capacitÃ©s: PASSÃ‰S")
        
        # Tests d'exÃ©cution
        print("\nğŸ¯ Tests d'ExÃ©cution de TÃ¢ches...")
        test_suite.setup_method()
        await test_suite.test_execute_task_architecture_assessment()
        await test_suite.test_execute_task_design_patterns_feature()
        await test_suite.test_execute_task_microservices_feature()
        await test_suite.test_execute_task_generate_audit_report()
        await test_suite.test_execute_task_strategic_recommendations()
        print("âœ… Tests d'exÃ©cution: PASSÃ‰S")
        
        # Tests de non-rÃ©gression
        print("\nğŸ”’ Tests de Non-RÃ©gression...")
        test_suite.setup_method()
        await test_suite.test_non_regression_all_features()
        test_suite.test_non_regression_features_count()
        await test_suite.test_non_regression_generic_task_fallback()
        print("âœ… Tests de non-rÃ©gression: PASSÃ‰S")
        
        # Tests de performance
        print("\nâš¡ Tests de Performance...")
        test_suite.setup_method()
        await test_suite.test_performance_execution_time()
        await test_suite.test_concurrent_task_execution()
        print("âœ… Tests de performance: PASSÃ‰S")
        
        # Tests d'intÃ©gration
        print("\nğŸ”— Tests d'IntÃ©gration...")
        test_suite.setup_method()
        test_suite.test_logging_integration()
        await test_suite.test_report_generation_structure()
        test_suite.test_factory_pattern_consistency()
        print("âœ… Tests d'intÃ©gration: PASSÃ‰S")
        
        print("\n" + "="*60)
        print("ğŸ‰ TOUS LES TESTS MIGRATION WAVE 3 - Architecture 22: RÃ‰USSIS")
        print("âœ… NON-RÃ‰GRESSION ABSOLUE VALIDÃ‰E")
        print("ğŸš€ Agent prÃªt pour dÃ©ploiement Wave 3")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ Ã‰CHEC DES TESTS: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        test_suite.teardown_method()

if __name__ == "__main__":
    success = asyncio.run(run_comprehensive_tests())
    sys.exit(0 if success else 1)