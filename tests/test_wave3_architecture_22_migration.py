#!/usr/bin/env python3
"""
üß™ TESTS VALIDATION MIGRATION WAVE 3 - Agent Architecture 22 Enterprise
========================================================================

Tests complets pour valider la migration de l'agent ARCHITECTURE_22_enterprise_consultant
vers le pattern NextGeneration Wave 3 avec NON-R√âGRESSION ABSOLUE.

Author: √âquipe NextGeneration
Version: 1.0.0
Created: 2025-06-28
"""

import asyncio
import sys
import pytest
from pathlib import Path
from typing import Dict, Any
import json
import time

# --- Configuration Robuste du Chemin d'Importation ---
try:
    project_root = Path(__file__).resolve().parents[1]
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
except (IndexError, NameError):
    if '.' not in sys.path:
        sys.path.insert(0, '.')

from core.agent_factory_architecture import Task, Result
from agents.agent_ARCHITECTURE_22_enterprise_consultant import (
    AgentARCHITECTURE22EnterpriseConsultant, 
    create_agent_ARCHITECTURE_22_enterprise_consultant,
    ArchitectureMetrics,
    ArchitectureIssue
)

class TestWave3Architecture22Migration:
    """üß™ Suite de tests compl√®te pour migration Agent Architecture 22 Wave 3"""
    
    def setup_method(self):
        """üîß Setup pour chaque test"""
        self.agent = create_agent_ARCHITECTURE_22_enterprise_consultant()
        
    def teardown_method(self):
        """üßπ Cleanup apr√®s chaque test"""
        if hasattr(self, 'agent'):
            asyncio.create_task(self.agent.shutdown())

    # --- Tests de Structure et Conformit√© ---
    
    def test_agent_structure_conformity(self):
        """‚úÖ Test conformit√© structure agent NextGeneration"""
        # V√©rification h√©ritage Agent NextGeneration
        assert hasattr(self.agent, 'startup'), "M√©thode startup manquante"
        assert hasattr(self.agent, 'shutdown'), "M√©thode shutdown manquante"
        assert hasattr(self.agent, 'health_check'), "M√©thode health_check manquante"
        assert hasattr(self.agent, 'execute_task'), "M√©thode execute_task manquante"
        assert hasattr(self.agent, 'get_capabilities'), "M√©thode get_capabilities manquante"
        
        # V√©rification attributs Wave 3
        assert hasattr(self.agent, 'version'), "Attribut version manquant"
        assert hasattr(self.agent, 'wave'), "Attribut wave manquant"
        assert hasattr(self.agent, 'compliance_target'), "Attribut compliance_target manquant"
        assert hasattr(self.agent, 'logger'), "Logger unifi√© manquant"
        
        # V√©rification version et wave
        assert self.agent.version == "5.3.0", f"Version incorrecte: {self.agent.version}"
        assert "Wave 3" in self.agent.wave, f"Wave incorrecte: {self.agent.wave}"
        assert self.agent.compliance_target == 95.0, f"Compliance target incorrecte: {self.agent.compliance_target}"

    def test_architecture_metrics_structure(self):
        """üìä Test structure ArchitectureMetrics"""
        metrics = ArchitectureMetrics()
        
        # V√©rification champs requis
        required_fields = [
            'design_patterns_score', 'microservices_maturity', 'event_driven_score',
            'ddd_compliance', 'cqrs_implementation', 'overall_architecture_score',
            'patterns_analyzed', 'anti_patterns_detected', 'optimization_recommendations'
        ]
        
        for field in required_fields:
            assert hasattr(metrics, field), f"Champ {field} manquant dans ArchitectureMetrics"

    def test_architecture_issue_structure(self):
        """üîç Test structure ArchitectureIssue"""
        issue = ArchitectureIssue(
            severity="HIGH",
            category="design_pattern",
            description="Test issue",
            recommendation="Fix recommendation"
        )
        
        assert issue.severity == "HIGH"
        assert issue.category == "design_pattern"
        assert hasattr(issue, 'to_dict'), "M√©thode to_dict manquante"
        
        issue_dict = issue.to_dict()
        assert isinstance(issue_dict, dict), "to_dict ne retourne pas un dict"

    # --- Tests de Fonctionnalit√©s Core ---
    
    @pytest.mark.asyncio
    async def test_startup_shutdown_cycle(self):
        """üîÑ Test cycle startup/shutdown"""
        # Test startup
        await self.agent.startup()
        
        # Test health apr√®s startup
        health = await self.agent.health_check()
        assert health['status'] == 'healthy', f"Status incorrect apr√®s startup: {health['status']}"
        
        # Test shutdown
        await self.agent.shutdown()

    @pytest.mark.asyncio
    async def test_health_check_comprehensive(self):
        """üè• Test health check complet"""
        await self.agent.startup()
        health = await self.agent.health_check()
        
        # V√©rification structure r√©ponse
        required_keys = ['agent_id', 'version', 'wave', 'status', 'features_status', 'features_count', 'compliance_target']
        for key in required_keys:
            assert key in health, f"Cl√© {key} manquante dans health check"
        
        # V√©rification valeurs
        assert health['status'] == 'healthy'
        assert health['version'] == '5.3.0'
        assert 'Wave 3' in health['wave']
        assert health['features_count'] == 5  # 5 features enterprise
        assert isinstance(health['features_status'], dict)

    def test_capabilities_completeness(self):
        """üõ†Ô∏è Test compl√©tude des capacit√©s"""
        capabilities = self.agent.get_capabilities()
        
        # V√©rification capacit√©s Wave 3 requises
        required_capabilities = [
            "advanced_design_patterns_analysis",
            "microservices_architecture_optimization", 
            "event_driven_architecture_design",
            "domain_driven_design_consultation",
            "cqrs_event_sourcing_implementation",
            "architecture_assessment_complete",
            "generate_architecture_audit_report",
            "generate_strategic_recommendations"
        ]
        
        for capability in required_capabilities:
            assert capability in capabilities, f"Capacit√© {capability} manquante"
        
        # V√©rification nombre minimal de capacit√©s
        assert len(capabilities) >= 10, f"Nombre insuffisant de capacit√©s: {len(capabilities)}"

    # --- Tests d'Ex√©cution de T√¢ches ---
    
    @pytest.mark.asyncio
    async def test_execute_task_architecture_assessment(self):
        """üîç Test ex√©cution assessment architecture complet"""
        await self.agent.startup()
        
        task = Task(
            id="test_assessment",
            type="architecture_assessment_complete",
            params={
                "target_system": "Test Enterprise System",
                "scope": ["design_patterns", "microservices", "event_driven"]
            }
        )
        
        result = await self.agent.execute_task(task)
        
        # V√©rification structure r√©sultat
        assert isinstance(result, Result), "R√©sultat n'est pas de type Result"
        assert result.success == True, f"Assessment √©chou√©: {result.error}"
        assert result.data is not None, "Donn√©es manquantes"
        
        # V√©rification contenu assessment
        data = result.data
        assert 'assessment_id' in data
        assert 'target_system' in data
        assert 'findings' in data
        assert 'recommendations' in data
        
        # V√©rification m√©triques
        assert result.metrics is not None
        assert 'assessment_duration_ms' in result.metrics

    @pytest.mark.asyncio
    async def test_execute_task_design_patterns_feature(self):
        """üé® Test ex√©cution feature Design Patterns"""
        await self.agent.startup()
        
        task = Task(
            id="test_design_patterns",
            type="design_patterns",
            params={"analysis_depth": "advanced"}
        )
        
        result = await self.agent.execute_task(task)
        
        assert result.success == True, f"Design Patterns √©chou√©: {result.error}"
        assert result.data is not None
        
        # V√©rification donn√©es sp√©cifiques Design Patterns
        data = result.data
        
        # Les donn√©es peuvent venir soit des features sp√©cialis√©es soit du fallback g√©n√©rique
        # V√©rifier la pr√©sence de donn√©es d'architecture g√©n√©rales
        expected_keys = [
            'task_type', 'architecture_analysis', 'design_patterns_applied', 
            'pattern_recommendations', 'overall_score', 'wave'
        ]
        
        # V√©rifier qu'on a des donn√©es valides d'architecture
        for key in expected_keys:
            assert key in data, f"Cl√© {key} manquante dans les donn√©es Design Patterns"
        
        # V√©rifier les valeurs
        assert data['task_type'] == 'design_patterns'
        assert 'Wave 3' in data['wave']
        assert isinstance(data['overall_score'], (int, float))
        
        # V√©rification m√©triques enrichies Wave 3
        metrics = result.metrics
        
        # Les m√©triques retourn√©es sont celles d'ArchitectureMetrics (fallback g√©n√©rique)
        assert metrics is not None, "M√©triques manquantes"
        assert isinstance(metrics, dict), "M√©triques doivent √™tre un dictionnaire"
        
        # V√©rifier les m√©triques ArchitectureMetrics attendues
        expected_metric_keys = [
            'design_patterns_score', 'microservices_maturity', 'event_driven_score',
            'ddd_compliance', 'cqrs_implementation', 'overall_architecture_score',
            'patterns_analyzed', 'anti_patterns_detected', 'optimization_recommendations'
        ]
        
        for key in expected_metric_keys:
            assert key in metrics, f"M√©trique {key} manquante"
        
        # V√©rifier valeurs num√©riques
        assert isinstance(metrics['overall_architecture_score'], (int, float))
        assert metrics['patterns_analyzed'] > 0
        assert metrics['optimization_recommendations'] > 0

    @pytest.mark.asyncio
    async def test_execute_task_microservices_feature(self):
        """üîß Test ex√©cution feature Microservices"""
        await self.agent.startup()
        
        task = Task(
            id="test_microservices",
            type="microservices",
            params={"target_architecture": "service_mesh"}
        )
        
        result = await self.agent.execute_task(task)
        
        assert result.success == True, f"Microservices √©chou√©: {result.error}"
        assert result.data is not None
        
        # V√©rification donn√©es Microservices
        data = result.data
        
        # V√©rifier donn√©es d'architecture microservices (fallback g√©n√©rique)
        expected_keys = [
            'task_type', 'architecture_analysis', 'microservices_optimized', 
            'overall_score', 'wave'
        ]
        
        for key in expected_keys:
            assert key in data, f"Cl√© {key} manquante dans les donn√©es Microservices"
        
        assert data['task_type'] == 'microservices'
        assert 'Wave 3' in data['wave']

    @pytest.mark.asyncio
    async def test_execute_task_generate_audit_report(self):
        """üìã Test g√©n√©ration rapport audit"""
        await self.agent.startup()
        
        task = Task(
            id="test_audit_report",
            type="generate_architecture_audit_report",
            params={"target_system": "Test System"}
        )
        
        result = await self.agent.execute_task(task)
        
        assert result.success == True, f"G√©n√©ration rapport √©chou√©e: {result.error}"
        assert result.data is not None
        
        # V√©rification contenu rapport
        data = result.data
        assert 'rapport_generated' in data
        assert 'rapport_path' in data
        assert 'overall_score' in data
        assert 'quality_level' in data
        
        # V√©rification que les fichiers sont cr√©√©s
        reports_dir = self.agent.reports_dir
        assert reports_dir.exists(), "R√©pertoire reports non cr√©√©"

    @pytest.mark.asyncio
    async def test_execute_task_strategic_recommendations(self):
        """üéØ Test g√©n√©ration recommandations strat√©giques"""
        await self.agent.startup()
        
        task = Task(
            id="test_recommendations",
            type="generate_strategic_recommendations",
            params={"context": "enterprise_migration", "priority": "high"}
        )
        
        result = await self.agent.execute_task(task)
        
        assert result.success == True, f"Recommandations √©chou√©es: {result.error}"
        assert result.data is not None
        
        # V√©rification structure recommandations
        data = result.data
        assert 'strategic_recommendations' in data
        assert 'roadmap' in data
        assert isinstance(data['strategic_recommendations'], list)
        assert len(data['strategic_recommendations']) > 0

    # --- Tests de Non-R√©gression ---
    
    @pytest.mark.asyncio
    async def test_non_regression_all_features(self):
        """üîí Test non-r√©gression - toutes les features fonctionnent"""
        await self.agent.startup()
        
        feature_tasks = [
            ("design_patterns", {"analysis_depth": "advanced"}),
            ("microservices", {"target_architecture": "service_mesh"}),
            ("event_driven", {"patterns": ["event_sourcing", "saga"]}),
            ("domain_driven", {"contexts": ["user", "order"]}),
            ("cqrs", {"optimization": "read_models"})
        ]
        
        for task_type, params in feature_tasks:
            task = Task(id=f"test_{task_type}", type=task_type, params=params)
            result = await self.agent.execute_task(task)
            
            assert result.success == True, f"Feature {task_type} √©chou√©e: {result.error}"
            assert result.data is not None, f"Donn√©es manquantes pour {task_type}"
            assert result.metrics is not None, f"M√©triques manquantes pour {task_type}"

    def test_non_regression_features_count(self):
        """üî¢ Test non-r√©gression - nombre de features"""
        assert len(self.agent.features) == 5, f"Nombre de features incorrect: {len(self.agent.features)}"
        
        feature_names = [f.__class__.__name__ for f in self.agent.features]
        expected_features = [
            'DesignPatternsFeature',
            'MicroservicesFeature', 
            'EventDrivenFeature',
            'DomainDrivenFeature',
            'CQRSEventSourcingFeature'
        ]
        
        for feature in expected_features:
            assert feature in feature_names, f"Feature {feature} manquante"

    @pytest.mark.asyncio
    async def test_non_regression_generic_task_fallback(self):
        """üîÑ Test fallback t√¢che g√©n√©rique (non-r√©gression)"""
        await self.agent.startup()
        
        task = Task(
            id="test_generic",
            type="generic_architecture_analysis",
            params={"analysis_type": "comprehensive"}
        )
        
        result = await self.agent.execute_task(task)
        
        assert result.success == True, f"Fallback g√©n√©rique √©chou√©: {result.error}"
        assert result.data is not None
        assert 'architecture_analysis' in result.data
        assert 'overall_score' in result.data

    # --- Tests de Performance ---
    
    @pytest.mark.asyncio
    async def test_performance_execution_time(self):
        """‚ö° Test performance temps d'ex√©cution"""
        await self.agent.startup()
        
        task = Task(
            id="test_perf",
            type="design_patterns",
            params={"analysis_depth": "basic"}
        )
        
        start_time = time.time()
        result = await self.agent.execute_task(task)
        execution_time = time.time() - start_time
        
        # V√©rification temps d'ex√©cution acceptable (< 2 secondes)
        assert execution_time < 2.0, f"Temps d'ex√©cution trop long: {execution_time}s"
        assert result.success == True
        
        # V√©rification m√©triques de performance (pour fallback g√©n√©rique)
        # Les m√©triques sont celles d'ArchitectureMetrics, pas d'ex√©cution
        assert result.metrics is not None
        assert isinstance(result.metrics, dict)
        assert len(result.metrics) > 0

    @pytest.mark.asyncio
    async def test_concurrent_task_execution(self):
        """üîÄ Test ex√©cution t√¢ches concurrentes"""
        await self.agent.startup()
        
        tasks = [
            Task(id=f"concurrent_{i}", type="design_patterns", params={})
            for i in range(3)
        ]
        
        # Ex√©cution concurrente
        results = await asyncio.gather(*[
            self.agent.execute_task(task) for task in tasks
        ])
        
        # V√©rification tous les r√©sultats
        for i, result in enumerate(results):
            assert result.success == True, f"T√¢che concurrente {i} √©chou√©e: {result.error}"

    # --- Tests d'Int√©gration Logging ---
    
    def test_logging_integration(self):
        """üìù Test int√©gration syst√®me logging unifi√©"""
        assert hasattr(self.agent, 'logger'), "Logger manquant"
        assert self.agent.logger is not None, "Logger non initialis√©"
        
        # Test du logging
        self.agent.logger.info("Test log message")
        
        # V√©rification configuration logger
        logger_name = self.agent.logger.name
        assert 'nextgen' in logger_name.lower() or 'architecture' in logger_name.lower()

    # --- Tests de Rapports ---
    
    @pytest.mark.asyncio
    async def test_report_generation_structure(self):
        """üìä Test structure g√©n√©ration rapports"""
        await self.agent.startup()
        
        # Test g√©n√©ration rapport complet
        task = Task(
            id="test_report_structure",
            type="generate_architecture_audit_report",
            params={"target_system": "Structure Test"}
        )
        
        result = await self.agent.execute_task(task)
        assert result.success == True
        
        # V√©rification r√©pertoire rapports cr√©√©
        assert self.agent.reports_dir.exists()
        assert self.agent.reports_dir.is_dir()

    def test_factory_pattern_consistency(self):
        """üè≠ Test coh√©rence Factory Pattern"""
        # Test factory function
        agent_via_factory = create_agent_ARCHITECTURE_22_enterprise_consultant()
        
        assert isinstance(agent_via_factory, AgentARCHITECTURE22EnterpriseConsultant)
        assert agent_via_factory.version == "5.3.0"
        assert "Wave 3" in agent_via_factory.wave

# --- Tests d'Ex√©cution Directe ---

async def run_comprehensive_tests():
    """üß™ Ex√©cution compl√®te des tests de migration"""
    print("üß™ D√âBUT DES TESTS MIGRATION WAVE 3 - Architecture 22")
    print("="*60)
    
    test_suite = TestWave3Architecture22Migration()
    
    try:
        # Tests de structure
        print("\nüìã Tests de Structure et Conformit√©...")
        test_suite.setup_method()
        test_suite.test_agent_structure_conformity()
        test_suite.test_architecture_metrics_structure()
        test_suite.test_architecture_issue_structure()
        print("‚úÖ Tests de structure: PASS√âS")
        
        # Tests de cycle de vie
        print("\nüîÑ Tests de Cycle de Vie...")
        await test_suite.test_startup_shutdown_cycle()
        print("‚úÖ Tests cycle de vie: PASS√âS")
        
        # Tests de sant√©
        print("\nüè• Tests de Sant√©...")
        test_suite.setup_method()
        await test_suite.test_health_check_comprehensive()
        print("‚úÖ Tests de sant√©: PASS√âS")
        
        # Tests de capacit√©s
        print("\nüõ†Ô∏è Tests de Capacit√©s...")
        test_suite.setup_method()
        test_suite.test_capabilities_completeness()
        print("‚úÖ Tests de capacit√©s: PASS√âS")
        
        # Tests d'ex√©cution
        print("\nüéØ Tests d'Ex√©cution de T√¢ches...")
        test_suite.setup_method()
        await test_suite.test_execute_task_architecture_assessment()
        await test_suite.test_execute_task_design_patterns_feature()
        await test_suite.test_execute_task_microservices_feature()
        await test_suite.test_execute_task_generate_audit_report()
        await test_suite.test_execute_task_strategic_recommendations()
        print("‚úÖ Tests d'ex√©cution: PASS√âS")
        
        # Tests de non-r√©gression
        print("\nüîí Tests de Non-R√©gression...")
        test_suite.setup_method()
        await test_suite.test_non_regression_all_features()
        test_suite.test_non_regression_features_count()
        await test_suite.test_non_regression_generic_task_fallback()
        print("‚úÖ Tests de non-r√©gression: PASS√âS")
        
        # Tests de performance
        print("\n‚ö° Tests de Performance...")
        test_suite.setup_method()
        await test_suite.test_performance_execution_time()
        await test_suite.test_concurrent_task_execution()
        print("‚úÖ Tests de performance: PASS√âS")
        
        # Tests d'int√©gration
        print("\nüîó Tests d'Int√©gration...")
        test_suite.setup_method()
        test_suite.test_logging_integration()
        await test_suite.test_report_generation_structure()
        test_suite.test_factory_pattern_consistency()
        print("‚úÖ Tests d'int√©gration: PASS√âS")
        
        print("\n" + "="*60)
        print("üéâ TOUS LES TESTS MIGRATION WAVE 3 - Architecture 22: R√âUSSIS")
        print("‚úÖ NON-R√âGRESSION ABSOLUE VALID√âE")
        print("üöÄ Agent pr√™t pour d√©ploiement Wave 3")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå √âCHEC DES TESTS: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        test_suite.teardown_method()

if __name__ == "__main__":
    success = asyncio.run(run_comprehensive_tests())
    sys.exit(0 if success else 1)