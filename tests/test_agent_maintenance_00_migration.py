import pytest
import time
import random
import os
import asyncio
from datetime import datetime, timedelta
import logging
from pathlib import Path
from typing import List, Dict
import numpy as np

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('tests/logs/agent_maintenance_00_migration_test.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('test_maintenance_00')

class TestAgentMaintenance00Migration:
    @pytest.fixture(scope="class")
    def setup_test_environment(self):
        """Configuration environnement de test production"""
        logger.info("üîß Configuration environnement de test production")
        
        # V√©rification environnement production
        assert os.getenv("ENV") == "production", "Tests doivent √™tre ex√©cut√©s en production"
        
        # Configuration charge x1.5
        self.load_factor = 1.5
        
        # Dur√©e minimale 7 jours
        self.start_time = datetime.now()
        self.min_duration = timedelta(days=7)
        
        yield
        
        # Validation dur√©e minimale
        test_duration = datetime.now() - self.start_time
        assert test_duration >= self.min_duration, f"Dur√©e test insuffisante: {test_duration} < 7 jours"

    @pytest.mark.asyncio
    async def test_orchestration_agents(self, setup_test_environment):
        """Test orchestration multi-agents"""
        logger.info("üß™ Test orchestration d√©marr√©")
        
        # Configuration orchestration
        orchestration = await self.setup_orchestration()
        
        # D√©marrage agents
        agents = await self.start_agents(orchestration)
        
        # Test coordination
        await self.test_agent_coordination(agents)
        
        # Test r√©cup√©ration erreurs
        await self.test_error_recovery(agents)
        
        # Test adaptation charge
        await self.test_load_adaptation(agents)
        
        logger.info("‚úÖ Test orchestration compl√©t√©")

    @pytest.mark.asyncio
    async def test_supervision_agents(self, setup_test_environment):
        """Test supervision agents"""
        logger.info("üß™ Test supervision d√©marr√©")
        
        # Configuration supervision
        supervision = await self.setup_supervision()
        
        # Test monitoring
        await self.test_agent_monitoring(supervision)
        
        # Test intervention
        await self.test_agent_intervention(supervision)
        
        # Test reporting
        await self.test_supervision_reporting(supervision)
        
        logger.info("‚úÖ Test supervision compl√©t√©")

    @pytest.mark.asyncio
    async def test_gestion_ressources(self, setup_test_environment):
        """Test gestion ressources"""
        logger.info("üß™ Test gestion ressources d√©marr√©")
        
        # Configuration ressources
        resources = await self.setup_resources()
        
        # Test allocation
        await self.test_resource_allocation(resources)
        
        # Test optimisation
        await self.test_resource_optimization(resources)
        
        # Test √©quilibrage
        await self.test_load_balancing(resources)
        
        logger.info("‚úÖ Test gestion ressources compl√©t√©")

    # M√©thodes auxiliaires orchestration
    async def setup_orchestration(self) -> Dict:
        """Configure orchestration agents"""
        return {
            "agents": self.generate_agent_configs(),
            "topology": self.generate_topology(),
            "protocols": self.generate_protocols()
        }

    def generate_agent_configs(self) -> List[Dict]:
        """G√©n√®re configurations agents"""
        return [
            {
                "id": f"agent_{i}",
                "type": random.choice(["worker", "coordinator", "monitor"]),
                "capacity": random.randint(50, 100),
                "specialization": random.choice(["io", "compute", "memory"])
            }
            for i in range(10)
        ]

    def generate_topology(self) -> Dict:
        """G√©n√®re topologie r√©seau agents"""
        topology = {"nodes": [], "edges": []}
        
        # Ajout noeuds
        for i in range(10):
            topology["nodes"].append({
                "id": f"node_{i}",
                "capacity": random.randint(100, 200),
                "location": f"zone_{random.randint(1, 3)}"
            })
        
        # Ajout connexions
        for i in range(10):
            for j in range(i + 1, 10):
                if random.random() < 0.3:  # 30% chance connexion
                    topology["edges"].append({
                        "source": f"node_{i}",
                        "target": f"node_{j}",
                        "latency": random.randint(5, 50)
                    })
        
        return topology

    def generate_protocols(self) -> Dict:
        """G√©n√®re protocoles communication"""
        return {
            "consensus": {
                "type": "raft",
                "timeout": 5000,
                "heartbeat": 1000
            },
            "messaging": {
                "type": "pub/sub",
                "qos": 2,
                "retry": 3
            },
            "sync": {
                "type": "2pc",
                "timeout": 10000
            }
        }

    async def start_agents(self, orchestration: Dict) -> List[Dict]:
        """D√©marre agents avec configuration"""
        agents = []
        for config in orchestration["agents"]:
            # Simulation d√©marrage agent
            agent = await self.start_agent(config)
            agents.append(agent)
            
            # Pause entre d√©marrages
            await asyncio.sleep(0.1)
            
        return agents

    async def start_agent(self, config: Dict) -> Dict:
        """D√©marre agent individuel"""
        # Simulation d√©marrage
        await asyncio.sleep(random.random())
        
        return {
            **config,
            "status": "running",
            "start_time": time.time(),
            "metrics": {
                "cpu": random.uniform(10, 30),
                "memory": random.uniform(20, 40)
            }
        }

    async def test_agent_coordination(self, agents: List[Dict]):
        """Test coordination entre agents"""
        # Test consensus
        consensus_results = await self.test_consensus(agents)
        assert consensus_results["success_rate"] > 0.95, "Taux consensus insuffisant"
        
        # Test synchronisation
        sync_results = await self.test_synchronization(agents)
        assert sync_results["sync_rate"] > 0.90, "Taux synchronisation insuffisant"
        
        # Test communication
        comm_results = await self.test_communication(agents)
        assert comm_results["delivery_rate"] > 0.98, "Taux livraison messages insuffisant"

    async def test_consensus(self, agents: List[Dict]) -> Dict:
        """Test consensus entre agents"""
        results = []
        for _ in range(100):  # 100 rounds consensus
            # Simulation round consensus
            success = await self.simulate_consensus_round(agents)
            results.append(success)
            
            # Pause entre rounds
            await asyncio.sleep(0.01)
            
        return {
            "success_rate": sum(results) / len(results),
            "rounds": len(results)
        }

    async def simulate_consensus_round(self, agents: List[Dict]) -> bool:
        """Simule round consensus"""
        # Simulation votes
        votes = [random.random() > 0.1 for _ in agents]  # 90% vote positif
        
        # Simulation latence r√©seau
        await asyncio.sleep(random.random() * 0.1)
        
        # Consensus atteint si >2/3 votes positifs
        return sum(votes) / len(votes) > 0.66

    async def test_synchronization(self, agents: List[Dict]) -> Dict:
        """Test synchronisation agents"""
        results = []
        for _ in range(50):  # 50 cycles sync
            # Simulation cycle synchronisation
            success = await self.simulate_sync_cycle(agents)
            results.append(success)
            
            # Pause entre cycles
            await asyncio.sleep(0.02)
            
        return {
            "sync_rate": sum(results) / len(results),
            "cycles": len(results)
        }

    async def simulate_sync_cycle(self, agents: List[Dict]) -> bool:
        """Simule cycle synchronisation"""
        # Simulation d√©lais r√©seau
        delays = [random.random() * 0.1 for _ in agents]
        
        # Simulation synchronisation
        await asyncio.sleep(max(delays))
        
        # Sync r√©ussie si tous d√©lais < 100ms
        return all(d < 0.1 for d in delays)

    async def test_communication(self, agents: List[Dict]) -> Dict:
        """Test communication entre agents"""
        messages = []
        for _ in range(200):  # 200 messages
            # Envoi message
            msg = await self.send_test_message(agents)
            messages.append(msg)
            
            # Pause entre messages
            await asyncio.sleep(0.005)
            
        # V√©rification livraison
        delivered = [msg["delivered"] for msg in messages]
        
        return {
            "delivery_rate": sum(delivered) / len(delivered),
            "messages": len(messages)
        }

    async def send_test_message(self, agents: List[Dict]) -> Dict:
        """Envoie message test"""
        # S√©lection agents source/destination
        source = random.choice(agents)
        dest = random.choice([a for a in agents if a != source])
        
        # Simulation envoi
        await asyncio.sleep(random.random() * 0.05)
        
        return {
            "source": source["id"],
            "destination": dest["id"],
            "delivered": random.random() > 0.02  # 98% succ√®s livraison
        }

    async def test_error_recovery(self, agents: List[Dict]):
        """Test r√©cup√©ration erreurs"""
        # Injection erreurs
        errors = await self.inject_errors(agents)
        
        # V√©rification r√©cup√©ration
        recovery = await self.verify_recovery(agents, errors)
        
        # Validation m√©triques
        assert recovery["recovery_rate"] > 0.9, "Taux r√©cup√©ration insuffisant"
        assert recovery["avg_time"] < 60, "Temps r√©cup√©ration trop long"

    async def inject_errors(self, agents: List[Dict]) -> List[Dict]:
        """Injecte erreurs test"""
        errors = []
        for agent in agents:
            if random.random() < 0.3:  # 30% agents impact√©s
                error = await self.inject_agent_error(agent)
                errors.append(error)
                
        return errors

    async def inject_agent_error(self, agent: Dict) -> Dict:
        """Injecte erreur agent"""
        error_types = ["crash", "timeout", "network", "resource"]
        error = {
            "agent": agent["id"],
            "type": random.choice(error_types),
            "timestamp": time.time()
        }
        
        # Simulation impact erreur
        await asyncio.sleep(random.random())
        
        return error

    async def verify_recovery(self, agents: List[Dict], errors: List[Dict]) -> Dict:
        """V√©rifie r√©cup√©ration erreurs"""
        recoveries = []
        for error in errors:
            # Tentative r√©cup√©ration
            recovery = await self.attempt_recovery(error)
            recoveries.append(recovery)
            
        # Calcul m√©triques
        successful = [r["success"] for r in recoveries]
        times = [r["time"] for r in recoveries if r["success"]]
        
        return {
            "recovery_rate": sum(successful) / len(successful),
            "avg_time": sum(times) / len(times) if times else float('inf')
        }

    async def attempt_recovery(self, error: Dict) -> Dict:
        """Tente r√©cup√©ration erreur"""
        start_time = time.time()
        
        # Simulation tentative r√©cup√©ration
        await asyncio.sleep(random.random() * 2)
        
        success = random.random() > 0.1  # 90% succ√®s r√©cup√©ration
        
        return {
            "error": error,
            "success": success,
            "time": time.time() - start_time
        }

    async def test_load_adaptation(self, agents: List[Dict]):
        """Test adaptation charge"""
        # Simulation variations charge
        variations = await self.simulate_load_variations()
        
        # Test adaptation
        adaptations = await self.test_adaptations(agents, variations)
        
        # Validation r√©ponses
        assert adaptations["success_rate"] > 0.85, "Taux adaptation insuffisant"
        assert adaptations["avg_latency"] < 30, "Latence adaptation trop √©lev√©e"

    async def simulate_load_variations(self) -> List[Dict]:
        """Simule variations charge"""
        variations = []
        base_load = 1000  # requ√™tes/s base
        
        for _ in range(20):  # 20 variations
            variation = {
                "timestamp": time.time(),
                "factor": random.uniform(0.5, 2.0),
                "duration": random.randint(10, 30)
            }
            variations.append(variation)
            
            # Pause entre variations
            await asyncio.sleep(0.1)
            
        return variations

    async def test_adaptations(self, agents: List[Dict], variations: List[Dict]) -> Dict:
        """Test adaptations charge"""
        responses = []
        for variation in variations:
            # Test adaptation variation
            response = await self.test_adaptation(agents, variation)
            responses.append(response)
            
        # Calcul m√©triques
        successful = [r["success"] for r in responses]
        latencies = [r["latency"] for r in responses if r["success"]]
        
        return {
            "success_rate": sum(successful) / len(successful),
            "avg_latency": sum(latencies) / len(latencies) if latencies else float('inf')
        }

    async def test_adaptation(self, agents: List[Dict], variation: Dict) -> Dict:
        """Test adaptation sp√©cifique"""
        start_time = time.time()
        
        # Simulation adaptation
        await asyncio.sleep(random.random() * 0.5)
        
        success = random.random() > 0.15  # 85% succ√®s adaptation
        
        return {
            "variation": variation,
            "success": success,
            "latency": time.time() - start_time
        }

    # M√©thodes auxiliaires supervision
    async def setup_supervision(self) -> Dict:
        """Configure syst√®me supervision"""
        return {
            "monitors": self.setup_monitors(),
            "alerts": self.setup_alerts(),
            "interventions": self.setup_interventions()
        }

    def setup_monitors(self) -> List[Dict]:
        """Configure moniteurs supervision"""
        return [
            {
                "id": f"monitor_{i}",
                "type": t,
                "interval": random.randint(10, 60),
                "threshold": random.uniform(0.7, 0.9)
            }
            for i, t in enumerate(["health", "performance", "security"])
        ]

    def setup_alerts(self) -> Dict:
        """Configure syst√®me alertes"""
        return {
            "levels": ["info", "warning", "error", "critical"],
            "channels": ["email", "slack", "sms"],
            "thresholds": {
                "warning": 0.7,
                "error": 0.85,
                "critical": 0.95
            }
        }

    def setup_interventions(self) -> List[Dict]:
        """Configure interventions automatiques"""
        return [
            {
                "trigger": "high_load",
                "action": "scale_up",
                "threshold": 0.8
            },
            {
                "trigger": "error_rate",
                "action": "restart",
                "threshold": 0.1
            },
            {
                "trigger": "memory_usage",
                "action": "cleanup",
                "threshold": 0.9
            }
        ]

    async def test_agent_monitoring(self, supervision: Dict):
        """Test monitoring agents"""
        # Collecte m√©triques
        metrics = await self.collect_agent_metrics()
        
        # Analyse m√©triques
        analysis = await self.analyze_metrics(metrics)
        
        # Validation monitoring
        assert analysis["coverage"] > 0.95, "Couverture monitoring insuffisante"
        assert analysis["accuracy"] > 0.9, "Pr√©cision monitoring insuffisante"

    async def collect_agent_metrics(self) -> List[Dict]:
        """Collecte m√©triques agents"""
        metrics = []
        for _ in range(100):  # 100 points donn√©es
            metric = {
                "timestamp": time.time(),
                "cpu": random.uniform(10, 90),
                "memory": random.uniform(20, 80),
                "latency": random.uniform(50, 200)
            }
            metrics.append(metric)
            
            # Pause entre collectes
            await asyncio.sleep(0.01)
            
        return metrics

    async def analyze_metrics(self, metrics: List[Dict]) -> Dict:
        """Analyse m√©triques collect√©es"""
        # Simulation analyse
        await asyncio.sleep(0.5)
        
        return {
            "coverage": random.uniform(0.9, 1.0),
            "accuracy": random.uniform(0.85, 0.98),
            "samples": len(metrics)
        }

    async def test_agent_intervention(self, supervision: Dict):
        """Test intervention sur agents"""
        # Simulation probl√®mes
        issues = await self.simulate_agent_issues()
        
        # Test interventions
        interventions = await self.test_interventions(issues)
        
        # Validation r√©sultats
        assert interventions["success_rate"] > 0.8, "Taux succ√®s interventions insuffisant"
        assert interventions["avg_response"] < 45, "Temps r√©ponse intervention trop long"

    async def simulate_agent_issues(self) -> List[Dict]:
        """Simule probl√®mes agents"""
        issues = []
        for _ in range(10):  # 10 probl√®mes
            issue = {
                "type": random.choice(["performance", "error", "resource"]),
                "severity": random.uniform(0.5, 1.0),
                "timestamp": time.time()
            }
            issues.append(issue)
            
            # Pause entre probl√®mes
            await asyncio.sleep(0.1)
            
        return issues

    async def test_interventions(self, issues: List[Dict]) -> Dict:
        """Test interventions automatiques"""
        responses = []
        for issue in issues:
            # Tentative intervention
            response = await self.attempt_intervention(issue)
            responses.append(response)
            
        # Calcul m√©triques
        successful = [r["success"] for r in responses]
        times = [r["response_time"] for r in responses if r["success"]]
        
        return {
            "success_rate": sum(successful) / len(successful),
            "avg_response": sum(times) / len(times) if times else float('inf')
        }

    async def attempt_intervention(self, issue: Dict) -> Dict:
        """Tente intervention sur probl√®me"""
        start_time = time.time()
        
        # Simulation intervention
        await asyncio.sleep(random.random() * 1.5)
        
        success = random.random() > 0.2  # 80% succ√®s intervention
        
        return {
            "issue": issue,
            "success": success,
            "response_time": time.time() - start_time
        }

    async def test_supervision_reporting(self, supervision: Dict):
        """Test reporting supervision"""
        # G√©n√©ration rapports
        reports = await self.generate_supervision_reports()
        
        # Validation rapports
        validation = await self.validate_reports(reports)
        
        # V√©rification m√©triques
        assert validation["completeness"] > 0.9, "Compl√©tude rapports insuffisante"
        assert validation["accuracy"] > 0.95, "Pr√©cision rapports insuffisante"

    async def generate_supervision_reports(self) -> List[Dict]:
        """G√©n√®re rapports supervision"""
        reports = []
        for _ in range(5):  # 5 types rapports
            report = await self.generate_report()
            reports.append(report)
            
        return reports

    async def generate_report(self) -> Dict:
        """G√©n√®re rapport individuel"""
        # Simulation g√©n√©ration
        await asyncio.sleep(random.random())
        
        return {
            "type": random.choice(["daily", "weekly", "monthly"]),
            "metrics": {
                "coverage": random.uniform(0.8, 1.0),
                "accuracy": random.uniform(0.9, 1.0),
                "completeness": random.uniform(0.85, 1.0)
            },
            "timestamp": time.time()
        }

    async def validate_reports(self, reports: List[Dict]) -> Dict:
        """Valide rapports g√©n√©r√©s"""
        validations = []
        for report in reports:
            # Validation rapport
            validation = await self.validate_report(report)
            validations.append(validation)
            
        # Calcul m√©triques globales
        completeness = [v["metrics"]["completeness"] for v in validations]
        accuracy = [v["metrics"]["accuracy"] for v in validations]
        
        return {
            "completeness": sum(completeness) / len(completeness),
            "accuracy": sum(accuracy) / len(accuracy)
        }

    async def validate_report(self, report: Dict) -> Dict:
        """Valide rapport individuel"""
        # Simulation validation
        await asyncio.sleep(random.random() * 0.5)
        
        return {
            "report": report["type"],
            "metrics": {
                "completeness": random.uniform(0.85, 1.0),
                "accuracy": random.uniform(0.9, 1.0)
            }
        }

    # M√©thodes auxiliaires ressources
    async def setup_resources(self) -> Dict:
        """Configure gestion ressources"""
        return {
            "pools": self.setup_resource_pools(),
            "policies": self.setup_resource_policies(),
            "constraints": self.setup_resource_constraints()
        }

    def setup_resource_pools(self) -> List[Dict]:
        """Configure pools ressources"""
        return [
            {
                "id": f"pool_{i}",
                "type": t,
                "capacity": random.randint(1000, 2000),
                "allocated": 0
            }
            for i, t in enumerate(["compute", "memory", "storage", "network"])
        ]

    def setup_resource_policies(self) -> Dict:
        """Configure politiques ressources"""
        return {
            "allocation": {
                "strategy": "best-fit",
                "overcommit": 0.2
            },
            "optimization": {
                "strategy": "cost-aware",
                "interval": 300
            },
            "scaling": {
                "strategy": "predictive",
                "threshold": 0.8
            }
        }

    def setup_resource_constraints(self) -> List[Dict]:
        """Configure contraintes ressources"""
        return [
            {
                "type": "capacity",
                "limit": 0.9,
                "action": "scale"
            },
            {
                "type": "cost",
                "limit": 1000,
                "action": "optimize"
            },
            {
                "type": "performance",
                "limit": 0.8,
                "action": "balance"
            }
        ]

    async def test_resource_allocation(self, resources: Dict):
        """Test allocation ressources"""
        # G√©n√©ration demandes
        requests = await self.generate_resource_requests()
        
        # Test allocations
        allocations = await self.test_allocations(resources, requests)
        
        # Validation r√©sultats
        assert allocations["success_rate"] > 0.9, "Taux succ√®s allocation insuffisant"
        assert allocations["efficiency"] > 0.8, "Efficacit√© allocation insuffisante"

    async def generate_resource_requests(self) -> List[Dict]:
        """G√©n√®re demandes ressources"""
        requests = []
        for _ in range(50):  # 50 demandes
            request = {
                "type": random.choice(["compute", "memory", "storage", "network"]),
                "amount": random.randint(10, 100),
                "priority": random.randint(1, 5)
            }
            requests.append(request)
            
            # Pause entre demandes
            await asyncio.sleep(0.02)
            
        return requests

    async def test_allocations(self, resources: Dict, requests: List[Dict]) -> Dict:
        """Test allocations ressources"""
        results = []
        for request in requests:
            # Tentative allocation
            result = await self.attempt_allocation(resources, request)
            results.append(result)
            
        # Calcul m√©triques
        successful = [r["success"] for r in results]
        efficiency = [r["efficiency"] for r in results if r["success"]]
        
        return {
            "success_rate": sum(successful) / len(successful),
            "efficiency": sum(efficiency) / len(efficiency) if efficiency else 0
        }

    async def attempt_allocation(self, resources: Dict, request: Dict) -> Dict:
        """Tente allocation ressources"""
        # Simulation allocation
        await asyncio.sleep(random.random() * 0.2)
        
        success = random.random() > 0.1  # 90% succ√®s allocation
        
        return {
            "request": request,
            "success": success,
            "efficiency": random.uniform(0.7, 1.0) if success else 0
        }

    async def test_resource_optimization(self, resources: Dict):
        """Test optimisation ressources"""
        # √âtat initial
        initial_state = await self.capture_resource_state(resources)
        
        # Optimisation
        optimization = await self.optimize_resources(resources)
        
        # √âtat final
        final_state = await self.capture_resource_state(resources)
        
        # Validation am√©lioration
        improvement = self.calculate_improvement(initial_state, final_state)
        assert improvement > 0.1, "Am√©lioration optimisation insuffisante"

    async def capture_resource_state(self, resources: Dict) -> Dict:
        """Capture √©tat ressources"""
        # Simulation capture
        await asyncio.sleep(0.1)
        
        return {
            "utilization": random.uniform(0.4, 0.9),
            "fragmentation": random.uniform(0.1, 0.4),
            "efficiency": random.uniform(0.5, 0.8)
        }

    async def optimize_resources(self, resources: Dict) -> Dict:
        """Optimise allocation ressources"""
        # Simulation optimisation
        await asyncio.sleep(1.0)
        
        return {
            "actions": random.randint(5, 15),
            "improved": random.uniform(0.1, 0.3),
            "cost_saved": random.uniform(100, 500)
        }

    def calculate_improvement(self, initial: Dict, final: Dict) -> float:
        """Calcule am√©lioration optimisation"""
        metrics = ["utilization", "fragmentation", "efficiency"]
        improvements = []
        
        for metric in metrics:
            if metric == "fragmentation":
                # Pour fragmentation, diminution est positive
                imp = initial[metric] - final[metric]
            else:
                # Pour autres m√©triques, augmentation est positive
                imp = final[metric] - initial[metric]
            improvements.append(imp)
            
        return sum(improvements) / len(improvements)

    async def test_load_balancing(self, resources: Dict):
        """Test √©quilibrage charge"""
        # Configuration initiale
        initial_distribution = await self.measure_load_distribution()
        
        # √âquilibrage
        balancing = await self.balance_load(resources)
        
        # Distribution finale
        final_distribution = await self.measure_load_distribution()
        
        # Validation √©quilibrage
        assert balancing["variance_reduction"] > 0.3, "R√©duction variance insuffisante"
        assert balancing["balance_score"] > 0.8, "Score √©quilibrage insuffisant"

    async def measure_load_distribution(self) -> Dict:
        """Mesure distribution charge"""
        # Simulation mesure
        await asyncio.sleep(0.2)
        
        loads = [random.uniform(0.3, 0.9) for _ in range(10)]
        return {
            "loads": loads,
            "variance": np.var(loads),
            "mean": np.mean(loads)
        }

    async def balance_load(self, resources: Dict) -> Dict:
        """√âquilibre charge ressources"""
        # Simulation √©quilibrage
        await asyncio.sleep(0.5)
        
        initial_var = random.uniform(0.2, 0.4)
        final_var = initial_var * random.uniform(0.4, 0.7)
        
        return {
            "variance_reduction": (initial_var - final_var) / initial_var,
            "balance_score": random.uniform(0.75, 0.95),
            "migrations": random.randint(3, 8)
        } 