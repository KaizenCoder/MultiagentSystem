#!/usr/bin/env python3
"""
üß™ Tests Unitaires et d'Int√©gration - TaskMaster NextGeneration
============================================================

Suite compl√®te de tests pour valider toutes les fonctionnalit√©s
du TaskMaster NextGen hybride.

Auteur: NextGeneration Team
Version: 1.0.0
Date: 2025-06-23
"""

import asyncio
import pytest
import json
import tempfile
from pathlib import Path
from datetime import datetime, timedelta
from unittest.mock import Mock, patch, AsyncMock
import sys
import os

# Ajout du chemin pour les imports
sys.path.append(str(Path(__file__).parent.parent))

from agents.taskmaster_nextgen import (
    TaskMasterNextGen, NLPProcessor, ComplexityAnalyzer, 
    DependencyResolver, ValidationEngine, TaskRepository,
    ExportManager, TaskDefinition, TaskType, TaskPriority,
    ValidationStatus, ComplexityLevel, SubTask, EvidenceEntry,
    RealityCheck, TaskMetrics
)

# ==========================================
# 1. TESTS NLP PROCESSOR
# ==========================================

class TestNLPProcessor:
    """Tests pour le processeur NLP"""
    
    def setup_method(self):
        """Setup avant chaque test"""
        self.nlp_processor = NLPProcessor()
    
    def test_initialize_nlp_without_spacy(self):
        """Test initialisation sans spaCy install√©"""
        with patch('spacy.load', side_effect=OSError("Model not found")):
            processor = NLPProcessor()
            assert processor.nlp is None
    
    def test_parse_simple_request(self):
        """Test parsing d'une requ√™te simple"""
        request = "Faire un audit de s√©curit√©"
        task_def = self.nlp_processor.parse_request(request)
        
        assert isinstance(task_def, TaskDefinition)
        assert task_def.task_type == TaskType.AUDIT
        assert "audit" in task_def.title.lower()
        assert task_def.description == request
        assert "security_analysis" in task_def.required_capabilities
    
    def test_parse_complex_request(self):
        """Test parsing d'une requ√™te complexe"""
        request = "Optimiser urgent les performances de la base de donn√©es PostgreSQL avec analyse ML"
        task_def = self.nlp_processor.parse_request(request)
        
        assert task_def.task_type == TaskType.OPTIMIZATION
        assert task_def.priority == TaskPriority.CRITICAL  # "urgent"
        assert "database_management" in task_def.required_capabilities
        assert "machine_learning" in task_def.required_capabilities
    
    def test_classify_task_types(self):
        """Test classification des diff√©rents types de t√¢ches"""
        test_cases = [
            ("Cr√©er la documentation", TaskType.DOCUMENTATION),
            ("Tester l'API", TaskType.TESTING),
            ("Refactoriser le code", TaskType.REFACTORING),
            ("Surveiller les performances", TaskType.MONITORING),
            ("D√©ployer l'application", TaskType.DEPLOYMENT),
            ("Parser le document", TaskType.PARSING),
            ("Planifier le projet", TaskType.PLANNING)
        ]
        
        for request, expected_type in test_cases:
            task_def = self.nlp_processor.parse_request(request)
            assert task_def.task_type == expected_type, f"Failed for: {request}"
    
    def test_detect_priority_levels(self):
        """Test d√©tection des niveaux de priorit√©"""
        test_cases = [
            ("T√¢che critique urgente", TaskPriority.CRITICAL),
            ("Important audit de s√©curit√©", TaskPriority.HIGH),
            ("Faire plus tard si possible", TaskPriority.LOW),
            ("T√¢che normale", TaskPriority.MEDIUM)
        ]
        
        for request, expected_priority in test_cases:
            task_def = self.nlp_processor.parse_request(request)
            assert task_def.priority == expected_priority, f"Failed for: {request}"
    
    @pytest.mark.skipif(not hasattr(NLPProcessor(), 'nlp') or NLPProcessor().nlp is None, 
                       reason="spaCy not available")
    def test_extract_entities_with_spacy(self):
        """Test extraction d'entit√©s avec spaCy (si disponible)"""
        request = "Analyser les donn√©es de Paris pour Microsoft"
        task_def = self.nlp_processor.parse_request(request)
        
        # V√©rification que les entit√©s sont extraites
        entities = task_def.metadata.get("entities", {})
        assert isinstance(entities, dict)


# ==========================================
# 2. TESTS COMPLEXITY ANALYZER
# ==========================================

class TestComplexityAnalyzer:
    """Tests pour l'analyseur de complexit√©"""
    
    def setup_method(self):
        """Setup avant chaque test"""
        self.analyzer = ComplexityAnalyzer()
    
    def test_simple_task_complexity(self):
        """Test complexit√© d'une t√¢che simple"""
        task = TaskDefinition(
            id="test1",
            title="Test simple",
            description="T√¢che de test simple",
            task_type=TaskType.DOCUMENTATION,
            priority=TaskPriority.LOW,
            required_capabilities=["writing"]
        )
        
        analysis = self.analyzer.analyze_task_complexity(task)
        
        assert analysis["complexity_level"] == ComplexityLevel.SIMPLE
        assert analysis["complexity_score"] < 15
        assert analysis["estimated_duration_minutes"] < 20
        assert "justification" in analysis
    
    def test_complex_task_complexity(self):
        """Test complexit√© d'une t√¢che complexe"""
        task = TaskDefinition(
            id="test2",
            title="Refactoring complexe",
            description="Refactoring complet avec ML",
            task_type=TaskType.REFACTORING,  # Type complexe
            priority=TaskPriority.CRITICAL,
            required_capabilities=["refactoring", "ml", "database", "security", "testing"],
            constraints={"deadline": "1 week", "quality": "high"},
            expected_outputs=["code", "tests", "documentation", "benchmarks"]
        )
        
        analysis = self.analyzer.analyze_task_complexity(task)
        
        assert analysis["complexity_level"] in [ComplexityLevel.COMPLEX, ComplexityLevel.VERY_COMPLEX]
        assert analysis["complexity_score"] > 35
        assert analysis["estimated_duration_minutes"] > 30
    
    def test_complexity_factors(self):
        """Test des facteurs de complexit√© individuels"""
        base_task = TaskDefinition(
            id="test3",
            title="Test factors",
            description="Test",
            task_type=TaskType.ANALYSIS,
            priority=TaskPriority.MEDIUM,
            required_capabilities=["analysis"]
        )
        
        analysis = self.analyzer.analyze_task_complexity(base_task)
        base_score = analysis["complexity_score"]
        
        # Ajouter plus de capacit√©s augmente la complexit√©
        complex_task = TaskDefinition(
            id="test4",
            title="Test factors complex",
            description="Test",
            task_type=TaskType.ANALYSIS,
            priority=TaskPriority.MEDIUM,
            required_capabilities=["analysis", "db", "ml", "security"]
        )
        
        complex_analysis = self.analyzer.analyze_task_complexity(complex_task)
        assert complex_analysis["complexity_score"] > base_score
    
    def test_task_type_complexity_mapping(self):
        """Test du mapping complexit√© par type de t√¢che"""
        # REFACTORING doit √™tre plus complexe que DOCUMENTATION
        refactoring_task = TaskDefinition(
            id="test5", title="Refactor", description="Test",
            task_type=TaskType.REFACTORING, priority=TaskPriority.MEDIUM,
            required_capabilities=["refactoring"]
        )
        
        doc_task = TaskDefinition(
            id="test6", title="Document", description="Test",
            task_type=TaskType.DOCUMENTATION, priority=TaskPriority.MEDIUM,
            required_capabilities=["writing"]
        )
        
        refactor_analysis = self.analyzer.analyze_task_complexity(refactoring_task)
        doc_analysis = self.analyzer.analyze_task_complexity(doc_task)
        
        assert refactor_analysis["complexity_score"] > doc_analysis["complexity_score"]


# ==========================================
# 3. TESTS DEPENDENCY RESOLVER
# ==========================================

class TestDependencyResolver:
    """Tests pour le r√©solveur de d√©pendances"""
    
    def setup_method(self):
        """Setup avant chaque test"""
        self.resolver = DependencyResolver()
    
    def test_analyze_audit_dependencies(self):
        """Test analyse des d√©pendances pour un audit"""
        task = TaskDefinition(
            id="audit_test",
            title="Security Audit",
            description="Audit de s√©curit√©",
            task_type=TaskType.AUDIT,
            priority=TaskPriority.HIGH,
            required_capabilities=["security_analysis"]
        )
        
        dependencies = self.resolver.analyze_dependencies(task)
        
        assert len(dependencies) > 0
        assert any("scan" in dep.get("task", "") for dep in dependencies)
        assert any("analyze" in dep.get("task", "") for dep in dependencies)
        assert any("report" in dep.get("task", "") for dep in dependencies)
    
    def test_analyze_optimization_dependencies(self):
        """Test analyse des d√©pendances pour optimisation"""
        task = TaskDefinition(
            id="optim_test",
            title="Performance Optimization",
            description="Optimisation des performances",
            task_type=TaskType.OPTIMIZATION,
            priority=TaskPriority.HIGH,
            required_capabilities=["performance_tuning"]
        )
        
        dependencies = self.resolver.analyze_dependencies(task)
        
        # V√©rifier la s√©quence logique
        task_names = [dep["task"] for dep in dependencies]
        assert "baseline_measurement" in task_names
        assert "profile_performance" in task_names
        assert "optimize_code" in task_names
        assert "validate_improvements" in task_names
    
    def test_create_execution_plan(self):
        """Test cr√©ation du plan d'ex√©cution"""
        dependencies = [
            {"task": "prepare", "parallel": True},
            {"task": "analyze", "parallel": False},
            {"task": "optimize", "parallel": False},
            {"task": "test", "parallel": True}
        ]
        
        execution_plan = self.resolver.create_execution_plan(dependencies)
        
        assert len(execution_plan) == len(dependencies)
        for i, step in enumerate(execution_plan):
            assert "order" in step
            assert step["order"] == i
    
    def test_unknown_task_type_dependencies(self):
        """Test gestion des types de t√¢ches inconnus"""
        task = TaskDefinition(
            id="unknown_test",
            title="Unknown Task",
            description="T√¢che de type inconnu",
            task_type=TaskType.ANALYSIS,  # Type non d√©fini dans les templates
            priority=TaskPriority.MEDIUM,
            required_capabilities=["unknown"]
        )
        
        dependencies = self.resolver.analyze_dependencies(task)
        
        # Doit retourner un template g√©n√©rique
        assert len(dependencies) >= 3  # prepare, execute, validate minimum
        task_names = [dep["task"] for dep in dependencies]
        assert "prepare" in task_names
        assert "execute" in task_names
        assert "validate" in task_names


# ==========================================
# 4. TESTS VALIDATION ENGINE
# ==========================================

class TestValidationEngine:
    """Tests pour le moteur de validation anti-hallucination"""
    
    def setup_method(self):
        """Setup avant chaque test"""
        self.engine = ValidationEngine()
    
    @pytest.mark.asyncio
    async def test_validate_good_output(self):
        """Test validation d'une sortie correcte"""
        agent_id = "test_agent"
        output = {
            "status": "success",
            "findings": ["Issue 1", "Issue 2"],
            "recommendations": ["Fix 1", "Fix 2"],
            "timestamp": datetime.now().isoformat()
        }
        expected = {"findings": [], "recommendations": []}
        context = {"task_type": TaskType.AUDIT}
        
        result = await self.engine.validate_agent_output(agent_id, output, expected, context)
        
        assert result["validated"] == True
        assert result["confidence_score"] >= 0.8
        assert len(result["checks"]) >= 4
    
    @pytest.mark.asyncio
    async def test_validate_bad_output(self):
        """Test validation d'une sortie probl√©matique"""
        agent_id = "test_agent"
        output = {
            "duration": -100,  # Valeur aberrante
            "percentage": 150,  # Pourcentage > 100
            "result": ""  # Champ critique vide
        }
        expected = {"duration": 60, "percentage": 80, "result": "some result"}
        context = {"task_type": TaskType.TESTING}
        
        result = await self.engine.validate_agent_output(agent_id, output, expected, context)
        
        assert result["validated"] == False
        assert result["confidence_score"] < 0.8
        
        # V√©rifier que les outliers sont d√©tect√©s
        outlier_check = next((check for check in result["checks"] 
                            if check.check_type == "outlier_detection"), None)
        assert outlier_check is not None
        assert not outlier_check.passed
    
    @pytest.mark.asyncio
    async def test_structural_consistency_check(self):
        """Test v√©rification de coh√©rence structurelle"""
        good_output = {"key1": "value1", "key2": "value2"}
        expected = {"key1": "expected1", "key2": "expected2"}
        
        check = self.engine._check_structural_consistency(good_output, expected)
        
        assert check.passed == True
        assert check.confidence > 0.8
        
        # Test avec structure incorrecte
        bad_output = {"wrong_key": "value"}
        check_bad = self.engine._check_structural_consistency(bad_output, expected)
        
        assert check_bad.passed == False
        assert check_bad.confidence < 0.8
    
    @pytest.mark.asyncio
    async def test_semantic_consistency_check(self):
        """Test v√©rification de coh√©rence s√©mantique"""
        # Test pour un audit
        audit_output = {"findings": ["security issue"], "recommendations": ["fix this"]}
        audit_context = {"task_type": TaskType.AUDIT}
        
        check = self.engine._check_semantic_consistency(audit_output, audit_context)
        assert check.passed == True
        
        # Test pour un audit sans findings
        bad_audit_output = {"other_data": "value"}
        check_bad = self.engine._check_semantic_consistency(bad_audit_output, audit_context)
        assert check_bad.confidence < 1.0
    
    def test_outlier_detection(self):
        """Test d√©tection des valeurs aberrantes"""
        outlier_output = {
            "duration": -50,  # N√©gatif pour dur√©e
            "percentage": 150,  # > 100 pour pourcentage
            "description": "",  # Vide pour champ critique
            "very_long_text": "x" * 15000  # Texte anormalement long
        }
        
        check = self.engine._check_for_outliers(outlier_output)
        
        assert not check.passed
        assert len(check.details["outliers"]) >= 3
        assert check.confidence < 0.4
    
    def test_temporal_consistency_check(self):
        """Test v√©rification de coh√©rence temporelle"""
        good_temporal_output = {
            "timestamp": datetime.now().isoformat(),
            "duration": 3600,  # 1 heure
            "date_created": "2025-06-23T10:00:00"
        }
        
        check = self.engine._check_temporal_consistency(good_temporal_output)
        assert check.passed == True
        
        # Test avec donn√©es temporelles incorrectes
        bad_temporal_output = {
            "timestamp": "2030-01-01T00:00:00Z",  # Future lointain
            "duration": -100,  # Dur√©e n√©gative
            "invalid_date": "not-a-date"
        }
        
        check_bad = self.engine._check_temporal_consistency(bad_temporal_output)
        assert not check_bad.passed
        assert len(check_bad.details["temporal_issues"]) > 0


# ==========================================
# 5. TESTS TASK REPOSITORY
# ==========================================

class TestTaskRepository:
    """Tests pour le repository de t√¢ches"""
    
    def setup_method(self):
        """Setup avant chaque test"""
        # Utiliser un fichier temporaire pour les tests
        self.temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.json')
        self.temp_path = Path(self.temp_file.name)
        self.temp_file.close()
        
        self.repo = TaskRepository(self.temp_path)
    
    def teardown_method(self):
        """Cleanup apr√®s chaque test"""
        if self.temp_path.exists():
            self.temp_path.unlink()
    
    def test_create_and_get_task(self):
        """Test cr√©ation et r√©cup√©ration de t√¢che"""
        task = TaskDefinition(
            id="test_task_001",
            title="Test Task",
            description="Test description",
            task_type=TaskType.TESTING,
            priority=TaskPriority.MEDIUM,
            required_capabilities=["testing"]
        )
        
        # Cr√©ation
        task_id = self.repo.create_task(task)
        assert task_id == "test_task_001"
        
        # R√©cup√©ration
        retrieved_task = self.repo.get_task(task_id)
        assert retrieved_task is not None
        assert retrieved_task.title == "Test Task"
        assert retrieved_task.task_type == TaskType.TESTING
    
    def test_update_task(self):
        """Test mise √† jour de t√¢che"""
        task = TaskDefinition(
            id="update_test",
            title="Original Title",
            description="Original description",
            task_type=TaskType.TESTING,
            priority=TaskPriority.LOW,
            required_capabilities=["testing"]
        )
        
        task_id = self.repo.create_task(task)
        
        # Mise √† jour
        updates = {
            "title": "Updated Title",
            "priority": TaskPriority.HIGH,
            "complexity_score": 75.5
        }
        success = self.repo.update_task(task_id, updates)
        assert success == True
        
        # V√©rification
        updated_task = self.repo.get_task(task_id)
        assert updated_task.title == "Updated Title"
        assert updated_task.priority == TaskPriority.HIGH
        assert updated_task.complexity_score == 75.5
    
    def test_delete_task(self):
        """Test suppression de t√¢che"""
        task = TaskDefinition(
            id="delete_test",
            title="To Delete",
            description="Will be deleted",
            task_type=TaskType.TESTING,
            priority=TaskPriority.MEDIUM,
            required_capabilities=["testing"]
        )
        
        task_id = self.repo.create_task(task)
        assert self.repo.get_task(task_id) is not None
        
        # Suppression
        success = self.repo.delete_task(task_id)
        assert success == True
        
        # V√©rification
        assert self.repo.get_task(task_id) is None
    
    def test_list_tasks_with_filters(self):
        """Test listing de t√¢ches avec filtres"""
        # Cr√©er plusieurs t√¢ches
        tasks = [
            TaskDefinition(
                id=f"filter_test_{i}",
                title=f"Task {i}",
                description=f"Description {i}",
                task_type=TaskType.TESTING if i % 2 == 0 else TaskType.AUDIT,
                priority=TaskPriority.HIGH if i < 2 else TaskPriority.LOW,
                required_capabilities=["testing"]
            )
            for i in range(4)
        ]
        
        for task in tasks:
            self.repo.create_task(task)
        
        # Test sans filtre
        all_tasks = self.repo.list_tasks()
        assert len(all_tasks) == 4
        
        # Test avec filtre sur le type
        testing_tasks = self.repo.list_tasks({"task_type": TaskType.TESTING})
        assert len(testing_tasks) == 2
        
        # Test avec filtre sur la priorit√©
        high_priority_tasks = self.repo.list_tasks({"priority": TaskPriority.HIGH})
        assert len(high_priority_tasks) == 2
    
    def test_subtasks_management(self):
        """Test gestion des sous-t√¢ches"""
        # Cr√©er une t√¢che parent
        parent_task = TaskDefinition(
            id="parent_test",
            title="Parent Task",
            description="Parent description",
            task_type=TaskType.ANALYSIS,
            priority=TaskPriority.MEDIUM,
            required_capabilities=["analysis"]
        )
        
        parent_id = self.repo.create_task(parent_task)
        
        # Ajouter des sous-t√¢ches
        subtasks = [
            SubTask(
                id=f"sub_{i}",
                parent_id=parent_id,
                title=f"Subtask {i}",
                status=ValidationStatus.PENDING
            )
            for i in range(3)
        ]
        
        for subtask in subtasks:
            success = self.repo.add_subtask(parent_id, subtask)
            assert success == True
        
        # R√©cup√©rer les sous-t√¢ches
        retrieved_subtasks = self.repo.get_subtasks(parent_id)
        assert len(retrieved_subtasks) == 3
        assert all(st.parent_id == parent_id for st in retrieved_subtasks)
    
    def test_persistence(self):
        """Test persistance des donn√©es"""
        # Cr√©er une t√¢che
        task = TaskDefinition(
            id="persistence_test",
            title="Persistent Task",
            description="Will persist",
            task_type=TaskType.DOCUMENTATION,
            priority=TaskPriority.MEDIUM,
            required_capabilities=["writing"]
        )
        
        self.repo.create_task(task)
        
        # Cr√©er un nouveau repository avec le m√™me fichier
        new_repo = TaskRepository(self.temp_path)
        
        # V√©rifier que la t√¢che existe
        retrieved_task = new_repo.get_task("persistence_test")
        assert retrieved_task is not None
        assert retrieved_task.title == "Persistent Task"


# ==========================================
# 6. TESTS EXPORT MANAGER
# ==========================================

class TestExportManager:
    """Tests pour le gestionnaire d'export"""
    
    def setup_method(self):
        """Setup avant chaque test"""
        self.export_manager = ExportManager()
        self.sample_tasks = [
            TaskDefinition(
                id="export_test_1",
                title="Export Test Task 1",
                description="First test task for export",
                task_type=TaskType.TESTING,
                priority=TaskPriority.HIGH,
                required_capabilities=["testing", "analysis"],
                complexity_score=45.5,
                estimated_duration=120
            ),
            TaskDefinition(
                id="export_test_2",
                title="Export Test Task 2",
                description="Second test task for export",
                task_type=TaskType.DOCUMENTATION,
                priority=TaskPriority.MEDIUM,
                required_capabilities=["writing"],
                complexity_score=25.0,
                estimated_duration=60
            )
        ]
    
    def test_export_json(self):
        """Test export au format JSON"""
        result = self.export_manager.export_tasks(self.sample_tasks, "json")
        
        assert isinstance(result, str)
        
        # Parser le JSON pour v√©rifier la structure
        data = json.loads(result)
        assert "export_timestamp" in data
        assert "tasks_count" in data
        assert data["tasks_count"] == 2
        assert "tasks" in data
        assert len(data["tasks"]) == 2
        
        # V√©rifier le contenu
        first_task = data["tasks"][0]
        assert first_task["title"] == "Export Test Task 1"
        assert first_task["task_type"] == "testing"
    
    def test_export_markdown(self):
        """Test export au format Markdown"""
        result = self.export_manager.export_tasks(self.sample_tasks, "markdown")
        
        assert isinstance(result, str)
        assert "# üìã Export des T√¢ches TaskMaster" in result
        assert "Export Test Task 1" in result
        assert "Export Test Task 2" in result
        assert "**Type :** Testing" in result
        assert "**Priorit√© :** HIGH" in result
    
    def test_export_csv(self):
        """Test export au format CSV"""
        result = self.export_manager.export_tasks(self.sample_tasks, "csv")
        
        assert isinstance(result, str)
        lines = result.strip().split('\n')
        
        # V√©rifier l'en-t√™te
        header = lines[0]
        assert "ID" in header
        assert "Titre" in header
        assert "Type" in header
        
        # V√©rifier les donn√©es
        assert len(lines) == 3  # Header + 2 t√¢ches
        assert "export_test_1" in lines[1]
        assert "Export Test Task 1" in lines[1]
    
    def test_export_html(self):
        """Test export au format HTML"""
        result = self.export_manager.export_tasks(self.sample_tasks, "html")
        
        assert isinstance(result, str)
        assert "<!DOCTYPE html>" in result
        assert "TaskMaster - Export des T√¢ches" in result
        assert "Export Test Task 1" in result
        assert "priority-high" in result  # Classe CSS pour priorit√© haute
        assert "capability" in result  # Classe CSS pour capacit√©s
    
    def test_export_to_file(self):
        """Test export vers fichier"""
        with tempfile.NamedTemporaryFile(delete=False, suffix='.json') as temp_file:
            temp_path = Path(temp_file.name)
        
        try:
            result = self.export_manager.export_tasks(self.sample_tasks, "json", temp_path)
            
            assert result == temp_path
            assert temp_path.exists()
            
            # V√©rifier le contenu du fichier
            with open(temp_path, 'r', encoding='utf-8') as f:
                content = f.read()
                data = json.loads(content)
                assert data["tasks_count"] == 2
        
        finally:
            if temp_path.exists():
                temp_path.unlink()
    
    def test_unsupported_format(self):
        """Test gestion des formats non support√©s"""
        with pytest.raises(ValueError, match="Format non support√©"):
            self.export_manager.export_tasks(self.sample_tasks, "unsupported_format")


# ==========================================
# 7. TESTS D'INT√âGRATION TASKMASTER NEXTGEN
# ==========================================

class TestTaskMasterNextGenIntegration:
    """Tests d'int√©gration pour TaskMaster NextGen"""
    
    def setup_method(self):
        """Setup avant chaque test"""
        # Utiliser un r√©pertoire temporaire pour les tests
        self.temp_dir = Path(tempfile.mkdtemp())
        
        # Mock de l'environnement pour utiliser le r√©pertoire temporaire
        with patch.dict(os.environ, {"TASKMASTER_WORK_DIR": str(self.temp_dir)}):
            self.taskmaster = TaskMasterNextGen("test_taskmaster")
    
    def teardown_method(self):
        """Cleanup apr√®s chaque test"""
        import shutil
        if self.temp_dir.exists():
            shutil.rmtree(self.temp_dir)
    
    @pytest.mark.asyncio
    async def test_startup_and_agent_discovery(self):
        """Test d√©marrage et d√©couverte d'agents"""
        # Mock de la d√©couverte d'agents
        mock_agents = {
            "test_agent_1": {
                "class": Mock,
                "capabilities": ["testing", "analysis"],
                "module_path": Path("test_agent_1.py"),
                "pattern_factory_compatible": True,
                "validation_ready": True
            }
        }
        
        with patch.object(self.taskmaster, '_discover_agents') as mock_discover:
            mock_discover.return_value = None
            self.taskmaster.agents = mock_agents
            
            result = await self.taskmaster.startup()
            
            assert result["status"] == "ready"
            assert result["agents_count"] == 1
            assert result["components"]["validation_ready"] == True
    
    @pytest.mark.asyncio
    async def test_create_task_full_pipeline(self):
        """Test pipeline complet de cr√©ation de t√¢che"""
        request = "Faire un audit de s√©curit√© urgent du module d'authentification"
        
        result = await self.taskmaster.create_task_from_natural_language(request)
        
        assert result["status"] == "success"
        assert "task_id" in result
        
        # V√©rifier la t√¢che cr√©√©e
        task_def = result["task_definition"]
        assert task_def["task_type"] == "audit"
        assert task_def["priority"] == "CRITICAL"  # "urgent" d√©tect√©
        assert len(task_def["required_capabilities"]) > 0
        
        # V√©rifier l'analyse de complexit√©
        complexity = result["complexity_analysis"]
        assert "complexity_score" in complexity
        assert "estimated_duration_minutes" in complexity
        
        # V√©rifier les sous-t√¢ches
        subtasks = result["subtasks"]
        assert len(subtasks) > 0
        assert all("parent_id" in st for st in subtasks)
    
    @pytest.mark.asyncio
    async def test_execute_task_with_validation(self):
        """Test ex√©cution de t√¢che avec validation"""
        # D'abord cr√©er une t√¢che
        request = "Tester l'API REST"
        create_result = await self.taskmaster.create_task_from_natural_language(request)
        task_id = create_result["task_id"]
        
        # Mock d'un agent pour l'ex√©cution
        mock_agent_class = Mock()
        mock_agent_instance = Mock()
        mock_agent_instance.run.return_value = {
            "status": "success",
            "tests_passed": 15,
            "tests_failed": 2,
            "coverage": 85.5
        }
        mock_agent_class.return_value = mock_agent_instance
        
        # Mock des agents disponibles
        self.taskmaster.agents = {
            "test_agent": {
                "class": mock_agent_class,
                "capabilities": ["testing", "api_testing"],
                "module_path": Path("test_agent.py"),
                "pattern_factory_compatible": True,
                "validation_ready": True
            }
        }
        
        # Ex√©cution
        result = await self.taskmaster.execute_task(task_id, agent_validation=True)
        
        assert result["status"] == "success"
        assert result["agent_used"] == "test_agent"
        assert "execution_time" in result
        
        # V√©rifier la validation
        agent_result = result["result"]
        assert "validation" in agent_result
        assert "confidence_score" in agent_result["validation"]
    
    def test_get_task_status(self):
        """Test r√©cup√©ration du statut de t√¢che"""
        # Cr√©er une t√¢che directement dans le repository
        task = TaskDefinition(
            id="status_test",
            title="Status Test Task",
            description="Test task for status",
            task_type=TaskType.TESTING,
            priority=TaskPriority.MEDIUM,
            required_capabilities=["testing"]
        )
        
        self.taskmaster.task_repository.create_task(task)
        
        # R√©cup√©rer le statut
        status = self.taskmaster.get_task_status("status_test")
        
        assert status["task"]["id"] == "status_test"
        assert status["task"]["title"] == "Status Test Task"
        assert status["status"] == "stored"
        assert "subtasks" in status
    
    def test_list_and_filter_tasks(self):
        """Test listing et filtrage des t√¢ches"""
        # Cr√©er plusieurs t√¢ches
        tasks = [
            TaskDefinition(
                id=f"list_test_{i}",
                title=f"List Test Task {i}",
                description=f"Test task {i}",
                task_type=TaskType.TESTING if i % 2 == 0 else TaskType.AUDIT,
                priority=TaskPriority.HIGH if i < 2 else TaskPriority.LOW,
                required_capabilities=["testing"]
            )
            for i in range(4)
        ]
        
        for task in tasks:
            self.taskmaster.task_repository.create_task(task)
        
        # Test listing complet
        result = self.taskmaster.list_tasks()
        assert result["tasks_count"] == 4
        
        # Test avec filtres
        filtered_result = self.taskmaster.list_tasks({"task_type": TaskType.TESTING})
        assert filtered_result["tasks_count"] == 2
    
    def test_export_functionality(self):
        """Test fonctionnalit√© d'export"""
        # Cr√©er quelques t√¢ches
        for i in range(3):
            task = TaskDefinition(
                id=f"export_integration_{i}",
                title=f"Export Integration Task {i}",
                description=f"Task for export testing {i}",
                task_type=TaskType.DOCUMENTATION,
                priority=TaskPriority.MEDIUM,
                required_capabilities=["writing"]
            )
            self.taskmaster.task_repository.create_task(task)
        
        # Test export JSON
        result = self.taskmaster.export_tasks("json")
        assert result["status"] == "success"
        assert result["tasks_exported"] == 3
        assert result["format"] == "json"
        
        # Test export vers fichier
        result_file = self.taskmaster.export_tasks("markdown", 
                                                  output_path=str(self.temp_dir / "test_export.md"))
        assert result_file["status"] == "success"
        assert Path(result_file["output_path"]).exists()
    
    def test_dashboard_data(self):
        """Test donn√©es du dashboard"""
        # Cr√©er des t√¢ches vari√©es
        task_configs = [
            (TaskType.TESTING, TaskPriority.HIGH, 45.0),
            (TaskType.AUDIT, TaskPriority.CRITICAL, 85.0),
            (TaskType.DOCUMENTATION, TaskPriority.LOW, 15.0),
            (TaskType.OPTIMIZATION, TaskPriority.MEDIUM, 65.0)
        ]
        
        for i, (task_type, priority, complexity) in enumerate(task_configs):
            task = TaskDefinition(
                id=f"dashboard_test_{i}",
                title=f"Dashboard Task {i}",
                description=f"Task {i} for dashboard",
                task_type=task_type,
                priority=priority,
                required_capabilities=["testing"],
                complexity_score=complexity
            )
            self.taskmaster.task_repository.create_task(task)
        
        # R√©cup√©rer les donn√©es du dashboard
        dashboard = self.taskmaster.get_dashboard_data()
        
        assert dashboard["summary"]["total_tasks"] == 4
        assert len(dashboard["distributions"]["task_types"]) > 0
        assert len(dashboard["distributions"]["priorities"]) > 0
        assert len(dashboard["distributions"]["complexity_levels"]) > 0
        assert dashboard["system_status"]["storage_healthy"] == True


# ==========================================
# 8. TESTS DE PERFORMANCE
# ==========================================

class TestTaskMasterPerformance:
    """Tests de performance pour TaskMaster NextGen"""
    
    def setup_method(self):
        """Setup avant chaque test"""
        self.temp_dir = Path(tempfile.mkdtemp())
        with patch.dict(os.environ, {"TASKMASTER_WORK_DIR": str(self.temp_dir)}):
            self.taskmaster = TaskMasterNextGen("perf_test_taskmaster")
    
    def teardown_method(self):
        """Cleanup apr√®s chaque test"""
        import shutil
        if self.temp_dir.exists():
            shutil.rmtree(self.temp_dir)
    
    @pytest.mark.asyncio
    async def test_bulk_task_creation_performance(self):
        """Test performance de cr√©ation en masse de t√¢ches"""
        import time
        
        start_time = time.time()
        
        # Cr√©er 100 t√¢ches
        tasks_created = []
        for i in range(100):
            request = f"T√¢che de test num√©ro {i} pour validation des performances"
            result = await self.taskmaster.create_task_from_natural_language(request)
            if result["status"] == "success":
                tasks_created.append(result["task_id"])
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # V√©rifications de performance
        assert len(tasks_created) == 100
        assert total_time < 30.0  # Moins de 30 secondes pour 100 t√¢ches
        
        avg_time_per_task = total_time / 100
        assert avg_time_per_task < 0.3  # Moins de 300ms par t√¢che
        
        print(f"Performance: {total_time:.2f}s total, {avg_time_per_task*1000:.1f}ms/t√¢che")
    
    def test_repository_performance_large_dataset(self):
        """Test performance du repository avec beaucoup de donn√©es"""
        import time
        
        # Cr√©er 1000 t√¢ches
        start_time = time.time()
        
        for i in range(1000):
            task = TaskDefinition(
                id=f"perf_test_{i:04d}",
                title=f"Performance Test Task {i}",
                description=f"Task number {i} for performance testing",
                task_type=TaskType.TESTING,
                priority=TaskPriority.MEDIUM,
                required_capabilities=["testing"]
            )
            self.taskmaster.task_repository.create_task(task)
        
        creation_time = time.time() - start_time
        
        # Test de r√©cup√©ration
        start_time = time.time()
        all_tasks = self.taskmaster.task_repository.list_tasks()
        retrieval_time = time.time() - start_time
        
        # Test de recherche avec filtre
        start_time = time.time()
        filtered_tasks = self.taskmaster.task_repository.list_tasks({"task_type": TaskType.TESTING})
        filter_time = time.time() - start_time
        
        # V√©rifications
        assert len(all_tasks) == 1000
        assert len(filtered_tasks) == 1000
        assert creation_time < 10.0  # Moins de 10 secondes pour cr√©er 1000 t√¢ches
        assert retrieval_time < 1.0   # Moins de 1 seconde pour r√©cup√©rer 1000 t√¢ches
        assert filter_time < 2.0      # Moins de 2 secondes pour filtrer 1000 t√¢ches
        
        print(f"Repository performance: cr√©ation={creation_time:.2f}s, "
              f"r√©cup√©ration={retrieval_time:.2f}s, filtre={filter_time:.2f}s")


# ==========================================
# 9. CONFIGURATION PYTEST
# ==========================================

def pytest_configure(config):
    """Configuration des marqueurs pytest"""
    config.addinivalue_line(
        "markers", "integration: marque les tests d'int√©gration"
    )
    config.addinivalue_line(
        "markers", "performance: marque les tests de performance"
    )


# ==========================================
# 10. FIXTURES COMMUNES
# ==========================================

@pytest.fixture
def sample_task_definition():
    """Fixture pour une d√©finition de t√¢che de test"""
    return TaskDefinition(
        id="fixture_test_task",
        title="Fixture Test Task",
        description="Task created by pytest fixture",
        task_type=TaskType.TESTING,
        priority=TaskPriority.MEDIUM,
        required_capabilities=["testing", "validation"],
        constraints={"deadline": "1 week"},
        expected_outputs=["test_report", "coverage_report"],
        success_criteria=["all_tests_pass", "coverage_80_percent"]
    )

@pytest.fixture
def mock_agent_class():
    """Fixture pour une classe d'agent mock√©e"""
    class MockAgent:
        CAPABILITIES = ["testing", "analysis", "validation"]
        
        def __init__(self):
            self.name = "MockAgent"
        
        def run(self, task_prompt):
            return {
                "status": "success",
                "message": f"Mock execution of: {task_prompt[:50]}...",
                "timestamp": datetime.now().isoformat()
            }
        
        def shutdown(self):
            pass
    
    return MockAgent


# ==========================================
# 11. MAIN POUR EX√âCUTION DIRECTE
# ==========================================

if __name__ == "__main__":
    print("üß™ Tests TaskMaster NextGeneration")
    print("=" * 50)
    print("Ex√©cution des tests avec pytest...")
    print()
    
    # Ex√©cution avec pytest
    import subprocess
    result = subprocess.run([
        "python", "-m", "pytest", __file__, 
        "-v", "--tb=short", "--color=yes"
    ])
    
    if result.returncode == 0:
        print("\n‚úÖ Tous les tests sont pass√©s!")
    else:
        print("\n‚ùå Certains tests ont √©chou√©.")
    
    exit(result.returncode)