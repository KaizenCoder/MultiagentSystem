#!/usr/bin/env python3
"""
Test configuration multi-GPU RTX 3090 + Ollama - NextGeneration
Valide votre setup particulier avec RTX 5060 Ti + RTX 3090.
"""

import os
import asyncio
import httpx
import subprocess
import time

# Configuration RTX 3090 selon standards SuperWhisper V6
os.environ['CUDA_VISIBLE_DEVICES'] = '1'  # RTX 3090 uniquement
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:1024'

try:
    import torch
    torch_available = True
except ImportError:
    torch_available = False

def test_gpu_detection():
    """Test la dÃ©tection de votre configuration multi-GPU."""
    print("ğŸ® DÃ‰TECTION CONFIGURATION MULTI-GPU")
    print("=" * 50)
    
    # Test nvidia-smi
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,pci.bus_id', '--format=csv,noheader,nounits'], 
                              capture_output=True, text=True)
        
        if result.returncode == 0:
            gpu_lines = result.stdout.strip().split('\n')
            print(f"ğŸ“Š {len(gpu_lines)} GPU(s) dÃ©tectÃ©e(s):")
            
            for i, line in enumerate(gpu_lines):
                name, memory, bus_id = line.split(', ')
                print(f"   GPU {i}: {name} ({memory}MB) - Bus {bus_id}")
                
                if "RTX 5060" in name:
                    print(f"      âš ï¸  RTX 5060 Ti dÃ©tectÃ©e (non compatible PyTorch)")
                elif "RTX 3090" in name:
                    print(f"      âœ… RTX 3090 dÃ©tectÃ©e (GPU principale)")
            
            return True
        else:
            print("âŒ nvidia-smi non disponible")
            return False
            
    except Exception as e:
        print(f"âŒ Erreur nvidia-smi: {e}")
        return False

def test_pytorch_gpu():
    """Test PyTorch avec configuration RTX 3090."""
    print("\nğŸ”¥ TEST PYTORCH AVEC RTX 3090")
    print("=" * 50)
    
    if not torch_available:
        print("âŒ PyTorch non installÃ©")
        return False
    
    # Test CUDA disponible
    if not torch.cuda.is_available():
        print("âŒ CUDA non disponible dans PyTorch")
        return False
    
    print(f"âœ… CUDA disponible dans PyTorch")
    
    # Test configuration GPU
    device_count = torch.cuda.device_count()
    print(f"ğŸ“Š {device_count} GPU(s) visible(s) par PyTorch")
    
    if device_count == 0:
        print("âŒ Aucune GPU visible")
        return False
    
    # Test GPU active (doit Ãªtre RTX 3090)
    current_device = torch.cuda.current_device()
    gpu_name = torch.cuda.get_device_name(current_device)
    gpu_props = torch.cuda.get_device_properties(current_device)
    
    print(f"ğŸ® GPU active: {gpu_name}")
    print(f"   Compute Capability: {gpu_props.major}.{gpu_props.minor}")
    print(f"   VRAM: {gpu_props.total_memory / 1024**3:.1f}GB")
    
    # Validation RTX 3090
    if "RTX 3090" in gpu_name:
        print("âœ… RTX 3090 correctement configurÃ©e pour PyTorch")
        
        # Test allocation simple
        try:
            test_tensor = torch.randn(1000, 1000, device='cuda')
            result = torch.matmul(test_tensor, test_tensor.t())
            del test_tensor, result
            torch.cuda.empty_cache()
            print("âœ… Test allocation GPU rÃ©ussi")
            return True
        except Exception as e:
            print(f"âŒ Erreur allocation GPU: {e}")
            return False
    else:
        print(f"âŒ GPU incorrecte dÃ©tectÃ©e: {gpu_name} (RTX 3090 attendue)")
        return False

async def test_ollama_service():
    """Test le service Ollama."""
    print("\nğŸ¦™ TEST SERVICE OLLAMA")
    print("=" * 50)
    
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get("http://localhost:11434/api/tags")
            
        if response.status_code == 200:
            models_data = response.json()
            models = models_data.get("models", [])
            
            print(f"âœ… Ollama connectÃ©")
            print(f"ğŸ“‹ {len(models)} modÃ¨le(s) installÃ©(s):")
            
            # Vos modÃ¨les recommandÃ©s
            your_models = {
                "nous-hermes-2-mistral-7b-dpo": "Vitesse",
                "llama3:8b-instruct-q6_k": "Usage quotidien", 
                "mixtral-8x7b": "QualitÃ© maximum",
                "qwen-coder-32b": "Code spÃ©cialisÃ©",
                "qwen2.5-coder:1.5b": "Tests rapides"
            }
            
            available_models = [m["name"] for m in models]
            
            for model_name, description in your_models.items():
                if any(model_name in available for available in available_models):
                    print(f"   âœ… {description}: {model_name}")
                else:
                    print(f"   âŒ {description}: {model_name} (absent)")
            
            return len(models) > 0
            
        else:
            print(f"âŒ Ollama erreur: HTTP {response.status_code}")
            return False
            
    except Exception as e:
        print(f"âŒ Ollama non accessible: {e}")
        print("ğŸ’¡ VÃ©rifiez qu'Ollama est dÃ©marrÃ©: ollama serve")
        return False

async def test_ollama_performance():
    """Test performance Ollama avec vos modÃ¨les."""
    print("\nâš¡ TEST PERFORMANCE OLLAMA")
    print("=" * 50)
    
    # ModÃ¨les Ã  tester par ordre de vitesse
    test_models = [
        ("qwen2.5-coder:1.5b", "Test ultra-rapide"),
        ("llama3:8b-instruct-q6_k", "Test quotidien"),
        ("nous-hermes-2-mistral-7b-dpo", "Test vitesse")
    ]
    
    for model_name, description in test_models:
        print(f"\nğŸ” {description}: {model_name}")
        
        payload = {
            "model": model_name,
            "prompt": "Dis 'Bonjour RTX 3090' en une ligne",
            "stream": False,
            "options": {
                "num_gpu": 1,
                "gpu_layers": -1,
                "temperature": 0.7
            }
        }
        
        try:
            start_time = time.time()
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    "http://localhost:11434/api/generate",
                    json=payload
                )
            
            duration = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                response_text = result.get("response", "").strip()
                
                # MÃ©triques de performance
                eval_count = result.get("eval_count", 0)
                eval_duration = result.get("eval_duration", 0) / 1e9  # ns to s
                tokens_per_sec = eval_count / eval_duration if eval_duration > 0 else 0
                
                print(f"   âœ… RÃ©ponse: {response_text}")
                print(f"   â±ï¸  Temps total: {duration:.2f}s")
                print(f"   ğŸš€ Vitesse: {tokens_per_sec:.1f} tokens/sec")
                
            else:
                print(f"   âŒ Erreur: HTTP {response.status_code}")
                
        except Exception as e:
            print(f"   âŒ Erreur: {e}")
        
        # Pause entre tests
        await asyncio.sleep(1)

async def test_ollama_worker():
    """Test du worker Ollama intÃ©grÃ©."""
    print("\nğŸ¤– TEST WORKER OLLAMA INTÃ‰GRÃ‰")
    print("=" * 50)
    
    try:
        # Import du worker
        import sys
        sys.path.append('orchestrator')
        
        from orchestrator.app.agents.ollama_worker import OllamaLocalWorker
        
        class MockConfig:
            OLLAMA_BASE_URL = "http://localhost:11434"
            OLLAMA_GPU_DEVICE = "1"
        
        worker = OllamaLocalWorker(MockConfig())
        
        # Test santÃ©
        health = await worker.health_check()
        print(f"ğŸ¥ SantÃ©: {health.get('status', 'unknown')}")
        print(f"ğŸ“Š ModÃ¨les disponibles: {health.get('our_models_available', 0)}/{health.get('our_models_total', 0)}")
        
        # Test traitement
        if health.get('status') == 'healthy':
            test_tasks = [
                ("Dis bonjour", ["fast"]),
                ("Ã‰cris une fonction Python pour additionner", ["code"]),
                ("Analyse les avantages de l'IA", ["quality"])
            ]
            
            for task, requirements in test_tasks:
                print(f"\nğŸ§ª Test: {task}")
                result = await worker.process_task(task, requirements)
                
                if result.get('success'):
                    print(f"   âœ… ModÃ¨le: {result.get('model_category')} ({result.get('model_used')})")
                    print(f"   â±ï¸  Temps: {result.get('processing_time', 0):.2f}s")
                    print(f"   ğŸ’¬ RÃ©ponse: {result.get('result', '')[:100]}...")
                else:
                    print(f"   âŒ Erreur: {result.get('error', 'Inconnue')}")
                
                await asyncio.sleep(2)
        
        return health.get('status') == 'healthy'
        
    except Exception as e:
        print(f"âŒ Erreur worker: {e}")
        return False

async def main():
    """Test complet de votre configuration."""
    print("ğŸ¯ TEST CONFIGURATION MULTI-GPU RTX 3090 + OLLAMA")
    print("ğŸ® NextGeneration - Configuration ParticuliÃ¨re")
    print("=" * 70)
    
    results = {}
    
    # Test 1: DÃ©tection GPU
    results['gpu_detection'] = test_gpu_detection()
    
    # Test 2: PyTorch GPU
    results['pytorch_gpu'] = test_pytorch_gpu()
    
    # Test 3: Service Ollama
    results['ollama_service'] = await test_ollama_service()
    
    # Test 4: Performance Ollama
    if results['ollama_service']:
        await test_ollama_performance()
    
    # Test 5: Worker intÃ©grÃ©
    results['ollama_worker'] = await test_ollama_worker()
    
    # RÃ©sumÃ© final
    print("\n" + "=" * 70)
    print("ğŸ“Š RÃ‰SUMÃ‰ CONFIGURATION")
    print("=" * 70)
    
    print(f"ğŸ® DÃ©tection GPU: {'âœ…' if results['gpu_detection'] else 'âŒ'}")
    print(f"ğŸ”¥ PyTorch RTX 3090: {'âœ…' if results['pytorch_gpu'] else 'âŒ'}")
    print(f"ğŸ¦™ Service Ollama: {'âœ…' if results['ollama_service'] else 'âŒ'}")
    print(f"ğŸ¤– Worker Ollama: {'âœ…' if results['ollama_worker'] else 'âŒ'}")
    
    all_ok = all(results.values())
    
    if all_ok:
        print("\nğŸ‰ CONFIGURATION PARFAITE !")
        print("ğŸ’¡ Votre setup multi-GPU RTX 3090 + Ollama est opÃ©rationnel")
        print("ğŸš€ PrÃªt pour l'intÃ©gration avec l'orchestrateur NextGeneration")
        
        print("\nğŸ“‹ ModÃ¨les recommandÃ©s selon usage:")
        print("   âš¡ Vitesse: nous-hermes-2-mistral-7b-dpo")
        print("   ğŸ’» Code: qwen-coder-32b") 
        print("   ğŸŒŸ QualitÃ©: mixtral-8x7b")
        print("   ğŸ“± Quotidien: llama3:8b-instruct-q6_k")
    else:
        print("\nâš ï¸ CONFIGURATION Ã€ CORRIGER")
        print("ğŸ’¡ VÃ©rifiez les Ã©lÃ©ments marquÃ©s âŒ ci-dessus")
        
        if not results['ollama_service']:
            print("   - DÃ©marrez Ollama: ollama serve")
        if not results['pytorch_gpu']:
            print("   - VÃ©rifiez configuration CUDA/PyTorch")

if __name__ == "__main__":
    asyncio.run(main())
