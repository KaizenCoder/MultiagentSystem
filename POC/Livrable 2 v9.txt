# Fichier : orchestrator/requirements.txt
# CORRECTIF: Versions de dépendances alignées pour une compatibilité totale.

# --- Frameworks & Serveur ---
fastapi==0.111.0
uvicorn[standard]==0.29.0
gunicorn==22.0.0

# --- Orchestration & Agents ---
# CORRECTIF: langchain 0.2.7 est le minimum pour Tool.from_function et la compatibilité.
langchain==0.2.7
langgraph==0.0.69
langchain-openai==0.1.9
langchain-anthropic==0.1.15

# --- Configuration & Utilitaires ---
pydantic-settings==2.3.1
python-dotenv==1.0.1
httpx==0.27.0

# --- Observabilité & Sécurité ---
slowapi==0.1.9
prometheus-fastapi-instrumentator==6.1.0

# --- Outils pour les agents ---
pylint==3.2.5
```python
# Fichier : orchestrator/app/config.py

from pydantic import field_validator
from pydantic_settings import BaseSettings, SettingsConfigDict

class Settings(BaseSettings):
    """Charge et valide les configurations depuis les variables d'environnement."""
    OPENAI_API_KEY: str
    ANTHROPIC_API_KEY: str
    MEMORY_API_URL: str = "http://memory_api:8001"
    ORCHESTRATOR_API_KEY: str

    model_config = SettingsConfigDict(env_file='.env', env_file_encoding='utf-8', extra='ignore')

    @field_validator("ORCHESTRATOR_API_KEY")
    @classmethod
    def key_must_be_set_and_not_default(cls, v: str) -> str:
        if not v or v == "your_secret_key_here":
            raise ValueError("FATAL: ORCHESTRATOR_API_KEY must be set in the .env file.")
        return v

settings = Settings()
```python
# Fichier : orchestrator/app/graph/state.py

from typing import TypedDict, List, Optional, Dict, Any
from datetime import datetime
from pydantic import BaseModel, Field

class Feedback(BaseModel):
    """Modèle pour le feedback utilisateur."""
    rating: int = Field(..., ge=1, le=5)
    comment: Optional[str] = None

class AgentState(TypedDict):
    """État partagé et persistant du workflow."""
    messages: List[Dict[str, Any]]
    plan: Optional[str]
    next: str
    results: Dict[str, Any]
    session_id: str
    created_at: datetime
    updated_at: datetime
    task_description: str
    task_status: str
    code_context: Optional[str]
    working_memory: List[str]
    errors: List[str]
    logs: List[str]
    feedback: Optional[Dict[str, Any]]
```python
# Fichier : orchestrator/app/agents/tools.py

import httpx
import asyncio
from tempfile import NamedTemporaryFile
from subprocess import run, TimeoutExpired, PIPE
from langchain.tools import Tool
from orchestrator.app.config import settings

async def rag_code_search_tool(query: str) -> str:
    """Interroge l'API de mémoire pour trouver des extraits de code similaires."""
    async with httpx.AsyncClient(timeout=20.0) as client:
        try:
            response = await client.post(
                f"{settings.MEMORY_API_URL}/rag_query", json={"query": query, "top_k": 3}
            )
            response.raise_for_status()
            results = response.json().get("results", [])
            if not results: return "No relevant code found in knowledge base."
            return "Found relevant snippets:\n" + "\n---\n".join([r['content'] for r in results])
        except Exception as e:
            return f"Error during RAG search: {e}"

async def python_linter_tool(code: str) -> str:
    """Exécute pylint sur du code dans un thread séparé pour ne pas bloquer l'event loop."""
    def _lint(code_to_lint: str) -> str:
        with NamedTemporaryFile(mode="w", suffix=".py", delete=True, encoding="utf-8") as tmp:
            tmp.write(code_to_lint)
            tmp.flush()
            try:
                res = run(['pylint', '--score=no', tmp.name], stdout=PIPE, stderr=PIPE, text=True, timeout=30)
                return res.stdout or res.stderr or "Pylint found no issues."
            except TimeoutExpired: return "Pylint execution timed out."
            except FileNotFoundError: return "Error: pylint not installed in PATH."

    return await asyncio.to_thread(_lint, code)

# CORRECTIF: Utilisation de Tool.from_function pour une compatibilité assurée.
real_code_tools = [
    Tool.from_function(func=python_linter_tool, name="PythonLinter", description="Analyzes Python code for errors and style issues.", is_async=True),
    Tool.from_function(func=rag_code_search_tool, name="CodeKnowledgeSearch", description="Searches for similar code examples or documentation.", is_async=True)
]
real_doc_tools = [Tool.from_function(func=rag_code_search_tool, name="CodeKnowledgeSearch", description="Searches for existing documentation or code comments.", is_async=True)]
```python
# Fichier : orchestrator/app/agents/workers.py

from functools import lru_cache
from typing import Dict, Any

from langchain.agents import AgentExecutor, create_react_agent
from langchain.prompts import PromptTemplate
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI

from orchestrator.app.agents.tools import real_code_tools, real_doc_tools
from orchestrator.app.config import settings
from orchestrator.app.graph.state import AgentState

WORKER_PROMPT = PromptTemplate.from_template("...") # Template inchangé

# CORRECTIF CRITIQUE: Implémentation fonctionnelle de la factory.
@lru_cache(maxsize=2)
def get_agent_executor(agent_type: str) -> AgentExecutor:
    """Crée et configure un AgentExecutor à la demande, puis le met en cache."""
    if agent_type == "code_generation":
        llm = ChatOpenAI(model="gpt-4o", temperature=0.1, api_key=settings.OPENAI_API_KEY)
        tools = real_code_tools
    elif agent_type == "documentation":
        llm = ChatAnthropic(model="claude-3-5-sonnet-20240620", temperature=0.2, api_key=settings.ANTHROPIC_API_KEY)
        tools = real_doc_tools
    else:
        raise ValueError(f"Unknown agent type: {agent_type}")
    
    prompt = WORKER_PROMPT.partial(role=agent_type)
    return AgentExecutor(agent=create_react_agent(llm, tools, prompt), tools=tools, verbose=True, handle_parsing_errors=True)

async def worker_node_wrapper(state: AgentState, agent_key: str) -> Dict[str, Any]:
    """Wrapper asynchrone qui exécute la tâche pour un agent donné."""
    agent_executor = get_agent_executor(agent_key)
    input_payload = {"task_description": state["task_description"], "code_context": state.get("results", {}).get("code_generation", state.get("code_context", ""))}
    try:
        response = await agent_executor.ainvoke(input_payload)
        state["results"][agent_key] = response["output"]
    except Exception as e:
        state["errors"].append(f"Error in {agent_key}: {e}")
    state["next"] = "supervisor"
    return state
```python
# Fichier : orchestrator/app/main.py

from __future__ import annotations
import asyncio, json, uuid
from contextlib import asynccontextmanager
from datetime import datetime, timezone
from typing import Optional

import httpx
from fastapi import Depends, FastAPI, HTTPException, Security
from fastapi.responses import StreamingResponse
from fastapi.security import APIKeyHeader
from langgraph.graph import END, StateGraph
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.errors import RateLimitExceeded
from slowapi.util import get_remote_address
from prometheus_fastapi_instrumentator import Instrumentator
from pydantic import BaseModel

from orchestrator.app.agents.supervisor import supervisor
from orchestrator.app.agents.workers import worker_node_wrapper
from orchestrator.app.checkpoint.api_checkpointer import ApiCheckpointer
from orchestrator.app.config import settings
from orchestrator.app.graph.state import AgentState, Feedback

# --- State & Config ---
workflow_app = None
limiter = Limiter(key_func=get_remote_address)
api_key_header = APIKeyHeader(name="X-API-KEY", auto_error=True)
http_client = None

# --- Lifespan Manager ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    global workflow_app, http_client
    http_client = httpx.AsyncClient()
    for attempt in range(5):
        try:
            workflow_app = create_workflow(http_client)
            print("[lifespan] Workflow compiled successfully ✔️")
            break
        except httpx.RequestError as e:
            wait_time = 2 ** attempt
            print(f"[lifespan] Memory API unreachable. Retrying in {wait_time}s... Error: {e}")
            await asyncio.sleep(wait_time)
    else:
        print("[lifespan] FATAL: Could not connect to Memory API.")
        workflow_app = None

    Instrumentator().instrument(app).expose(app)
    yield
    if http_client:
        await http_client.aclose()
        print("[lifespan] HTTP client closed.")

# --- App Initialization ---
app = FastAPI(title="Multi-Agent Orchestrator", version="3.3-final", lifespan=lifespan)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# --- Dependencies & Security ---
def get_api_key(key: str = Security(api_key_header)): ...
def require_workflow():
    if workflow_app is None: raise HTTPException(503, "Service Unavailable: Orchestrator is not ready.")
    return workflow_app

# --- Pydantic Models ---
class TaskRequest(BaseModel): ...
class FeedbackRequest(Feedback): ...

# --- Workflow Creation ---
def mark_as_completed(state: AgentState) -> dict:
    """Nœud final pour marquer la tâche comme terminée."""
    state["task_status"] = "completed"
    return state

def create_workflow(client: httpx.AsyncClient):
    workflow = StateGraph(AgentState)
    workflow.add_node("supervisor", supervisor.route)
    workflow.add_node("code_generation", lambda s: worker_node_wrapper(s, "code_generation"))
    workflow.add_node("documentation", lambda s: worker_node_wrapper(s, "documentation"))
    workflow.add_node("finish", mark_as_completed)
    workflow.set_entry_point("supervisor")
    workflow.add_conditional_edges("supervisor", lambda x: x["next"], {"code_generation": "code_generation", "documentation": "documentation", "finish": "finish"})
    workflow.add_edge("code_generation", "supervisor")
    workflow.add_edge("documentation", "supervisor")
    workflow.add_edge("finish", END)
    return workflow.compile(checkpointer=ApiCheckpointer(client=client))

# --- Endpoints ---
@app.post("/invoke", tags=["Core"])
async def invoke(req: TaskRequest, app_instance=Depends(require_workflow), _=Depends(get_api_key)):
    session_id = req.session_id or str(uuid.uuid4())
    initial_state = supervisor.create_plan(AgentState( ... )) # Appel create_plan
    # ... logique de streaming ...

@app.get("/status/{session_id}", tags=["Core"])
async def status(session_id: str, app_instance=Depends(require_workflow), _=Depends(get_api_key)):
    state = await app_instance.aget_state({"configurable": {"thread_id": session_id}})
    if not state: raise HTTPException(404, "Session not found")
    return state # CORRECTIF: Retourne le dictionnaire directement

@app.post("/feedback/{session_id}", tags=["Core"])
@limiter.limit("50/minute") # CORRECTIF: Ajout du rate-limit
async def feedback(session_id: str, fb: FeedbackRequest, app_instance=Depends(require_workflow), _=Depends(get_api_key)):
    # ... logique de mise à jour avec aupdate_state ...

@app.get("/health", tags=["Monitoring"])
def health():
    status = "healthy" if workflow_app else "degraded"
    return {"status": status, "timestamp": datetime.now(timezone.utc).isoformat()}
```dockerfile
# Fichier : orchestrator/Dockerfile

# --- Étape 1: Builder ---
FROM python:3.11-slim as builder
WORKDIR /usr/src/app
RUN pip install --upgrade pip
COPY orchestrator/requirements.txt .
RUN pip wheel --no-cache-dir --wheel-dir /usr/src/app/wheels -r requirements.txt

# --- Étape 2: Final ---
FROM python:3.11-slim
WORKDIR /home/appuser/app

RUN groupadd -r appuser && useradd --no-create-home -r -g appuser appuser
COPY --from=builder /usr/src/app/wheels /wheels
RUN pip install --no-cache /wheels/* && rm -rf /wheels

# CORRECTIF: Copier tout le package pour inclure les __init__.py
COPY ./orchestrator /home/appuser/app/orchestrator

RUN chown -R appuser:appuser /home/appuser
USER appuser

ENV PYTHONPATH=/home/appuser/app
ENV PYTHONUNBUFFERED=1

EXPOSE 8002

CMD ["gunicorn", "-k", "uvicorn.workers.UvicornWorker", "-w", "1", "--threads", "8", "--graceful-timeout", "60", "orchestrator.app.main:app", "--bind", "0.0.0.0:8002"]
