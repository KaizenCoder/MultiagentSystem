# üß™ PROMPT SPRINT 2 - SUITE DE TESTS COMPL√àTE

**R√¥le** : D√©veloppeur Senior sp√©cialis√© en Testing & QA  
**Mission** : Corriger et compl√©ter la suite de tests pour atteindre 75% de couverture  
**Contexte** : Orchestrateur multi-agent avec 34 tests partiels existants  
**Dur√©e** : 13 jours d√©veloppeur (3 phases)  

---

## üéØ OBJECTIF PRINCIPAL

Transformer les **34 tests partiels non-fonctionnels** en **suite compl√®te de 80+ tests** avec **75% de couverture**, pr√™te pour CI/CD et d√©ploiement production.

### **√âtat actuel analys√© :**
- ‚úÖ **34 tests collect√©s** (principalement SSRF prevention)
- ‚úÖ **Framework configur√©** : `conftest.py` complet (521 lignes)
- ‚ùå **Tests non-fonctionnels** : Erreurs configuration et imports
- ‚ùå **Couverture partielle** : ~20% (s√©curit√© uniquement)
- ‚ùå **Tests core manquants** : Supervisor, Workers, API, E2E

---

## üìã PHASE 1 : CORRECTION TESTS EXISTANTS (3 jours)

### **üîß Jour 1 : Fix Configuration**

#### **Probl√®me identifi√© :**
```bash
ValidationError: 3 validation errors for Settings
- OPENAI_API_KEY: Field required
- ANTHROPIC_API_KEY: Field required  
- ORCHESTRATOR_API_KEY: Field required
```

#### **Actions requises :**
1. **Installation d√©pendances manquantes**
```bash
pip install locust pytest-env pytest-mock pytest-timeout pytest-xdist
```

2. **Cr√©ation fichier .env.test**
```env
# .env.test
OPENAI_API_KEY=sk-test-fake-openai-key-for-testing-only-12345
ANTHROPIC_API_KEY=sk-ant-test-fake-anthropic-key-for-testing-67890
ORCHESTRATOR_API_KEY=test-orchestrator-api-key-secure-12345
MEMORY_API_URL=http://localhost:8001
ENVIRONMENT=testing
DEBUG=true
LOG_LEVEL=INFO
SERVICE_VERSION=v9-test
```

3. **Configuration pytest.ini**
```ini
[tool:pytest]
testpaths = tests
python_files = test_*.py *_test.py
python_classes = Test*
python_functions = test_*
addopts = 
    --cov=orchestrator
    --cov-report=html:htmlcov
    --cov-report=term-missing
    --cov-fail-under=75
    --timeout=30
    --tb=short
    -v
    --strict-markers
asyncio_mode = auto
env_files = .env.test
markers =
    unit: Unit tests
    integration: Integration tests
    security: Security tests
    performance: Performance tests
    slow: Slow running tests
```

### **üîß Jour 2 : Fix Imports et Modules**

#### **Modification conftest.py**
```python
# Ajout au d√©but de tests/conftest.py
import os
import sys
from pathlib import Path

# Ajout du r√©pertoire racine au PYTHONPATH
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

@pytest.fixture(autouse=True)
def setup_test_environment():
    """Configure l'environnement de test avant chaque test."""
    test_env = {
        'OPENAI_API_KEY': 'sk-test-fake-openai-key',
        'ANTHROPIC_API_KEY': 'sk-ant-test-fake-anthropic-key',
        'ORCHESTRATOR_API_KEY': 'test-orchestrator-api-key',
        'ENVIRONMENT': 'testing',
        'DEBUG': 'true',
        'MEMORY_API_URL': 'http://localhost:8001'
    }
    
    # Sauvegarde environnement original
    original_env = {key: os.environ.get(key) for key in test_env.keys()}
    
    # Application environnement test
    os.environ.update(test_env)
    
    yield
    
    # Restauration environnement original
    for key, value in original_env.items():
        if value is None:
            os.environ.pop(key, None)
        else:
            os.environ[key] = value
```

### **üîß Jour 3 : Validation Tests S√©curit√©**

#### **Commandes de validation :**
```bash
# Test que les tests de s√©curit√© passent
python -m pytest tests/security/ -v --tb=short

# V√©rification couverture s√©curit√©
python -m pytest tests/security/ --cov=orchestrator.app.security --cov-report=term-missing

# Test performance des validations
python -m pytest tests/security/ -m "not slow" --timeout=10
```

#### **Crit√®res d'acceptation Phase 1 :**
- [ ] **34 tests passent** sans erreur
- [ ] **Couverture s√©curit√© ‚â• 90%**
- [ ] **Temps ex√©cution < 30s** pour tests s√©curit√©
- [ ] **Aucune fuite de secrets** dans les logs

---

## üìã PHASE 2 : COMPL√âTION TESTS MANQUANTS (7 jours)

### **üß™ Jour 4-5 : Tests Unitaires Core**

#### **tests/unit/test_supervisor.py** (NOUVEAU - CRITIQUE)
```python
import pytest
from unittest.mock import Mock, patch, AsyncMock
from orchestrator.app.agents.supervisor import Supervisor
from orchestrator.app.graph.state import AgentState

@pytest.mark.unit
class TestSupervisor:
    """Tests unitaires Supervisor - CRITIQUE."""
    
    def setup_method(self):
        """Setup pour chaque test."""
        self.supervisor = Supervisor()
    
    def test_create_plan_code_generation_task(self, sample_agent_state):
        """Test planification t√¢che g√©n√©ration code."""
        # ARRANGE
        sample_agent_state.task_description = "Generate Python sorting function"
        
        # ACT
        result = self.supervisor.create_plan(sample_agent_state)
        
        # ASSERT
        assert result.plan is not None
        assert "code_generation" in result.plan
        assert result.next == "code_generation"
        assert result.task_status == "in_progress"
    
    def test_create_plan_documentation_task(self, sample_agent_state):
        """Test planification t√¢che documentation."""
        # ARRANGE
        sample_agent_state.task_description = "Create API documentation"
        
        # ACT
        result = self.supervisor.create_plan(sample_agent_state)
        
        # ASSERT
        assert result.plan is not None
        assert "documentation" in result.plan
        assert result.next == "documentation"
    
    def test_route_with_code_results(self, sample_agent_state):
        """Test routage avec r√©sultats code."""
        # ARRANGE
        sample_agent_state.results = {"code_generation": "def sort_list(): pass"}
        sample_agent_state.plan = "existing plan"
        
        # ACT
        result = self.supervisor.route(sample_agent_state)
        
        # ASSERT
        assert result.next == "documentation"
    
    def test_route_completion_all_results(self, sample_agent_state):
        """Test routage vers fin avec tous r√©sultats."""
        # ARRANGE
        sample_agent_state.results = {
            "code_generation": "def sort_list(): pass",
            "documentation": "Function documentation"
        }
        sample_agent_state.plan = "existing plan"
        
        # ACT
        result = self.supervisor.route(sample_agent_state)
        
        # ASSERT
        assert result.next == "finish"
    
    def test_error_handling_invalid_state(self):
        """Test gestion erreur √©tat invalide."""
        # ARRANGE
        invalid_state = AgentState()  # √âtat vide
        
        # ACT & ASSERT
        with pytest.raises(ValueError, match="Invalid state"):
            self.supervisor.create_plan(invalid_state)
    
    @pytest.mark.asyncio
    async def test_supervisor_with_llm_timeout(self, sample_agent_state):
        """Test gestion timeout LLM."""
        # ARRANGE
        with patch('orchestrator.app.agents.supervisor.llm_service') as mock_llm:
            mock_llm.generate_response.side_effect = asyncio.TimeoutError()
            
            # ACT & ASSERT
            with pytest.raises(TimeoutError):
                await self.supervisor.create_plan_async(sample_agent_state)
```

#### **tests/unit/test_workers.py** (NOUVEAU - CRITIQUE)
```python
import pytest
from unittest.mock import AsyncMock, patch, MagicMock
from orchestrator.app.agents.workers import get_agent_executor, worker_node_wrapper

@pytest.mark.unit
class TestWorkers:
    """Tests workers - CRITIQUE."""
    
    @pytest.mark.asyncio
    async def test_get_agent_executor_code_generation(self):
        """Test cr√©ation agent g√©n√©ration code."""
        # ACT
        executor = get_agent_executor("code_generation")
        
        # ASSERT
        assert executor is not None
        assert hasattr(executor, 'agent')
        assert hasattr(executor, 'tools')
    
    @pytest.mark.asyncio
    async def test_worker_node_wrapper_success(self, sample_agent_state, mock_llm_service):
        """Test wrapper worker avec succ√®s."""
        # ARRANGE
        sample_agent_state.task_description = "Generate code"
        
        with patch('orchestrator.app.agents.workers.get_agent_executor') as mock_get_executor:
            mock_executor = AsyncMock()
            mock_executor.ainvoke.return_value = {
                "output": "Generated code successfully"
            }
            mock_get_executor.return_value = mock_executor
            
            # ACT
            result = await worker_node_wrapper(sample_agent_state, "code_generation")
            
            # ASSERT
            assert result.results["code_generation"] is not None
            assert result.task_status == "completed"
```

### **üîÑ Jour 6-7 : Tests Int√©gration API**

#### **tests/integration/test_api_endpoints.py** (NOUVEAU)
```python
import pytest
from fastapi.testclient import TestClient
from unittest.mock import patch, AsyncMock
from orchestrator.app.main import app

@pytest.mark.integration
class TestAPIIntegration:
    """Tests int√©gration API."""
    
    def setup_method(self):
        """Setup client test."""
        self.client = TestClient(app)
    
    def test_health_endpoint(self):
        """Test endpoint sant√©."""
        # ACT
        response = self.client.get("/health")
        
        # ASSERT
        assert response.status_code == 200
        data = response.json()
        assert "status" in data
        assert data["status"] in ["healthy", "degraded"]
    
    @patch('orchestrator.app.main.graph')
    def test_invoke_endpoint_success(self, mock_graph):
        """Test endpoint invoke avec succ√®s."""
        # ARRANGE
        mock_graph.ainvoke = AsyncMock(return_value={
            "results": {"code_generation": "def test(): pass"},
            "task_status": "completed"
        })
        
        payload = {
            "task_description": "Generate test function",
            "session_id": "test-session"
        }
        
        # ACT
        response = self.client.post("/invoke", json=payload)
        
        # ASSERT
        assert response.status_code == 200
        data = response.json()
        assert "results" in data
        assert data["task_status"] == "completed"
```

### **üöÄ Jour 8-10 : Tests End-to-End**

#### **tests/integration/test_workflow_complete.py** (NOUVEAU)
```python
@pytest.mark.integration
@pytest.mark.slow
class TestWorkflowIntegration:
    """Tests int√©gration bout-en-bout."""
    
    @pytest.mark.asyncio
    async def test_complete_code_generation_workflow(self):
        """Test workflow complet g√©n√©ration code."""
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "http://localhost:8002/invoke",
                json={
                    "task_description": "Generate a Python function to sort a list",
                    "session_id": "test-session-1"
                }
            )
            
            assert response.status_code == 200
            response_data = response.json()
            assert "results" in response_data
            assert "code_generation" in response_data["results"]
```

---

## üìã PHASE 3 : CI/CD ET AUTOMATISATION (3 jours)

### **üöÄ Jour 11 : Pipeline CI/CD**

#### **.github/workflows/tests.yml** (NOUVEAU)
```yaml
name: Tests Suite Compl√®te
on: 
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  tests:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.11]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-asyncio pytest-mock locust
    
    - name: Run security tests
      run: |
        pytest tests/security/ -v --tb=short
    
    - name: Run unit tests
      run: |
        pytest tests/unit/ -v --cov=orchestrator --cov-report=xml
    
    - name: Run integration tests
      run: |
        pytest tests/integration/ -v --timeout=60
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: true
```

### **üöÄ Jour 12-13 : Optimisation et Documentation**

#### **Makefile** (NOUVEAU)
```makefile
.PHONY: test test-unit test-integration test-security test-coverage

test:
	python -m pytest tests/ -v

test-unit:
	python -m pytest tests/unit/ -v --tb=short

test-integration:
	python -m pytest tests/integration/ -v --timeout=60

test-security:
	python -m pytest tests/security/ -v

test-coverage:
	python -m pytest tests/ --cov=orchestrator --cov-report=html --cov-report=term-missing

test-fast:
	python -m pytest tests/ -x --tb=short -m "not slow"

install-test-deps:
	pip install pytest pytest-cov pytest-asyncio pytest-mock pytest-timeout locust
```

---

## üìä CRIT√àRES DE SUCC√àS SPRINT 2

### **M√©triques obligatoires :**
- [ ] **Couverture ‚â• 75%** (pytest-cov)
- [ ] **80+ tests** au total (vs 34 actuels)
- [ ] **Tests unitaires** : 40+ tests pour composants core
- [ ] **Tests int√©gration** : 20+ tests API endpoints
- [ ] **Tests E2E** : 8+ workflows complets
- [ ] **Performance** : Temps r√©ponse < 100ms health checks
- [ ] **S√©curit√©** : 0 vuln√©rabilit√© dans tests
- [ ] **CI/CD** : Pipeline automatis√© fonctionnel

### **Commandes de validation finale :**
```bash
# Validation compl√®te
make test-coverage
pytest tests/ --cov=orchestrator --cov-fail-under=75

# Performance
pytest tests/ --timeout=300 --tb=short

# S√©curit√©
pytest tests/security/ -v --tb=short

# CI/CD simulation
act -j tests  # GitHub Actions local
```

---

## üéØ LIVRABLES ATTENDUS

### **Structure finale attendue :**
```
tests/
‚îú‚îÄ‚îÄ conftest.py (‚úÖ existant, am√©lior√©)
‚îú‚îÄ‚îÄ pytest.ini (üÜï nouveau)
‚îú‚îÄ‚îÄ .env.test (üÜï nouveau)
‚îú‚îÄ‚îÄ unit/ (üÜï nouveau)
‚îÇ   ‚îú‚îÄ‚îÄ test_supervisor.py (15+ tests)
‚îÇ   ‚îú‚îÄ‚îÄ test_workers.py (12+ tests)
‚îÇ   ‚îú‚îÄ‚îÄ test_graph.py (8+ tests)
‚îÇ   ‚îî‚îÄ‚îÄ test_state.py (5+ tests)
‚îú‚îÄ‚îÄ integration/ (üÜï nouveau)
‚îÇ   ‚îú‚îÄ‚îÄ test_api_endpoints.py (20+ tests)
‚îÇ   ‚îî‚îÄ‚îÄ test_workflow_complete.py (8+ tests)
‚îú‚îÄ‚îÄ security/ (‚úÖ existant, corrig√©)
‚îÇ   ‚îú‚îÄ‚îÄ test_rce_prevention.py (‚úÖ 20+ tests)
‚îÇ   ‚îî‚îÄ‚îÄ test_ssrf_prevention.py (‚úÖ 32+ tests)
‚îî‚îÄ‚îÄ load/ (‚úÖ existant, corrig√©)
    ‚îî‚îÄ‚îÄ basic_load_test.py (‚úÖ tests performance)
```

### **Documentation requise :**
- [ ] **README_TESTS.md** : Guide utilisation tests
- [ ] **TESTING_STRATEGY.md** : Strat√©gie et patterns
- [ ] **CI_CD_GUIDE.md** : Guide pipeline automatis√©

---

## üèÜ R√âSULTAT ATTENDU

**Transformation compl√®te** : De 34 tests partiels non-fonctionnels √† **suite robuste de 80+ tests** avec **75% de couverture**, **CI/CD automatis√©**, et **confiance totale** pour √©volutions futures.

**Impact business** :
- üõ°Ô∏è **Filet de s√©curit√©** pour √©volutions
- üöÄ **D√©ploiement confiant** en production
- üìà **Qualit√© code** maintenue
- üîß **Maintenance** facilit√©e
- üí∞ **ROI d√©veloppement** optimis√©

---

**üéØ MISSION : Cr√©er la suite de tests la plus robuste et compl√®te possible pour s√©curiser l'√©volution de l'orchestrateur multi-agent vers la production.** 