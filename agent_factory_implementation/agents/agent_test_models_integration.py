#!/usr/bin/env python3
"""
üß™ AGENT TEST - INT√âGRATION MOD√àLES IA
=====================================

Agent de test pour valider la nouvelle architecture de gestion des mod√®les IA
avec support complet des mod√®les locaux Ollama RTX3090.

MISSION :
- Tester configuration centralis√©e des mod√®les
- Valider fallback automatique local/cloud
- Benchmarker performance mod√®les locaux RTX3090
- V√©rifier int√©gration Pattern Factory

MOD√àLES TEST√âS :
- Claude Sonnet 4 (cloud)
- Llama 3.1 8B (local RTX3090)
- Qwen-Coder 32B (local RTX3090)
- Mixtral 8x7B (local RTX3090)

Version: 1.0.0
Cr√©√©: 19 juin 2025 - 17h30
"""

import asyncio
import json
import logging
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import uuid

# Import Pattern Factory
sys.path.insert(0, str(Path(__file__).parent.parent))
try:
    from core.base_agent_template import BaseAgent, AgentConfig
    PATTERN_FACTORY_AVAILABLE = True
except ImportError:
    # Fallback si Pattern Factory non disponible
    from abc import ABC, abstractmethod
    
    class BaseAgent(ABC):
        def __init__(self, agent_id: str):
            self.agent_id = agent_id
            
        @abstractmethod
        async def execute_task(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
            pass
            
        @abstractmethod
        def get_capabilities(self) -> List[str]:
            pass
            
        @abstractmethod 
        async def health_check(self) -> Dict[str, Any]:
            pass
    
    PATTERN_FACTORY_AVAILABLE = False

# Imports locaux
try:
    from core.model_manager import ModelManager
except ImportError:
    ModelManager = None

# Configuration logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("AgentTestModels")

class AgentTestModelsIntegration(BaseAgent):
    """
    üß™ Agent de test pour validation de l'architecture mod√®les IA
    
    Cet agent teste l'int√©gration compl√®te entre:
    - Gestionnaire de mod√®les centralis√©
    - Ollama RTX3090 local
    - Fallback cloud providers
    - Pattern Factory
    """
    
    def __init__(self, agent_id: str = None):
        """Initialisation de l'agent de test mod√®les"""
        
        # Cr√©ation de la configuration pour BaseAgent
        if PATTERN_FACTORY_AVAILABLE:
            config_data = {
                "name": agent_id or f"test_models_integration_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}",
                "version": "1.0.0",
                "role": "test_integration",
                "domain": "models_validation",
                "description": "Agent de test pour validation architecture mod√®les IA",
                "capabilities": [
                    "test_models_integration",
                    "validate_ollama_connection", 
                    "benchmark_performance",
                    "test_agent_model_compatibility",
                    "generate_validation_report"
                ],
                "tools": ["model_manager", "ollama_client", "performance_tester"],
                "dependencies": [],
                "security_requirements": {},
                "performance_targets": {"response_time": 30},
                "default_config": {"log_level": "INFO"},
                "pattern_factory": {
                    "template_version": "1.0.0",
                    "factory_compatible": True,
                    "auto_registration": False,
                    "hot_reload": False
                }
            }
            config = AgentConfig.from_template(config_data)
            super().__init__(config)
        else:
            # Fallback pour tests sans Pattern Factory
            agent_id = agent_id or f"test_models_integration_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
            super().__init__(agent_id)
            self.agent_id = agent_id
            self.version = "1.0.0"
            self.description = "Agent de test pour validation architecture mod√®les IA"
            
            # Configuration logger
            self.logger = logging.getLogger(f"Agent.{self.agent_id}")
        
        # Configuration agent
        self.model_manager = None
        
        logger.info(f"üß™ Agent Test Mod√®les IA v{self.version} initialis√©")
        logger.info(f"üéØ Pattern Factory: {'‚úÖ Disponible' if PATTERN_FACTORY_AVAILABLE else '‚ùå Fallback'}")
        
        # Configuration test avec toutes les structures n√©cessaires
        self.test_results = {
            "configuration": {},
            "ollama_integration": {},
            "cloud_providers": {},
            "fallback_mechanisms": {},
            "performance_metrics": {},
            "cost_analysis": {},
            "model_manager_init": {},
            "agent_model_configs": {},
            "ollama_status": {},
            "fallback_tests": {},
            "development_challenges": {},
            "debug_challenges": {},
            "performance_challenges": {},
            "model_responses": {}
        }
        
        # üöÄ VRAIES QUESTIONS DE D√âVELOPPEMENT INFORMATIQUE
        self.development_challenges = {
            "architecture_complexe": {
                "prompt": """Tu es un architecte logiciel senior. Analyse cette architecture et identifie les probl√®mes :

```python
class UserService:
    def __init__(self):
        self.db = Database()
        self.cache = Redis()
        self.email = EmailService()
        
    def create_user(self, data):
        user = self.db.save(User(data))
        self.cache.set(f"user:{user.id}", user)
        self.email.send_welcome(user.email)
        return user
```

QUESTIONS TECHNIQUES :
1. Quels patterns SOLID sont viol√©s ?
2. Comment impl√©menter l'injection de d√©pendances ?
3. Que se passe-t-il si Redis est down ?
4. Comment g√©rer les transactions distribu√©es ?
5. Propose une refactorisation avec DDD.""",
                "evaluation_criteria": [
                    "Identifie Single Responsibility Principle violation",
                    "Propose Dependency Injection",
                    "√âvoque Circuit Breaker pattern",
                    "Mentionne Saga pattern ou 2PC",
                    "Suggest Repository/Service layers"
                ],
                "min_score": 3,
                "complexity": "senior"
            },
            
            "algorithme_optimisation": {
                "prompt": """Optimise cet algorithme de recherche dans un graphe :

```python
def find_shortest_path(graph, start, end):
    visited = []
    queue = [(start, [start])]
    
    while queue:
        node, path = queue.pop(0)
        if node not in visited:
            visited.append(node)
            if node == end:
                return path
            for neighbor in graph[node]:
                queue.append((neighbor, path + [neighbor]))
    return None
```

D√âFIS TECHNIQUES :
1. Quelle est la complexit√© temporelle actuelle ?
2. Pourquoi cette impl√©mentation est inefficace ?
3. Impl√©mente Dijkstra avec heap
4. G√®re les poids n√©gatifs (Bellman-Ford)
5. Optimise pour graphes tr√®s larges (A*)""",
                "evaluation_criteria": [
                    "Identifie O(V+E) mais inefficace √† cause de la liste",
                    "Explique pourquoi BFS na√Øf est lent",
                    "Impl√©mente correctement Dijkstra avec heapq",
                    "Mentionne Bellman-Ford pour poids n√©gatifs",
                    "Propose A* avec heuristique"
                ],
                "min_score": 3,
                "complexity": "expert"
            },
            
            "securite_crypto": {
                "prompt": """Audit de s√©curit√© de ce syst√®me d'authentification :

```python
import hashlib
import secrets

class AuthSystem:
    def hash_password(self, password):
        salt = secrets.token_hex(16)
        return hashlib.sha256((password + salt).encode()).hexdigest() + ":" + salt
    
    def verify_password(self, password, stored_hash):
        hash_part, salt = stored_hash.split(":")
        return hashlib.sha256((password + salt).encode()).hexdigest() == hash_part
```

AUDIT S√âCURIT√â :
1. Quelles vuln√©rabilit√©s d√©tectes-tu ?
2. Pourquoi SHA-256 est inappropri√© pour les mots de passe ?
3. Impl√©mente avec bcrypt/scrypt/Argon2
4. G√®re le timing attack
5. Ajoute 2FA et rate limiting""",
                "evaluation_criteria": [
                    "Identifie que SHA-256 est trop rapide",
                    "Explique les attaques par rainbow tables",
                    "Propose bcrypt/scrypt/Argon2",
                    "Mentionne constant-time comparison",
                    "Suggest TOTP/HOTP pour 2FA"
                ],
                "min_score": 4,
                "complexity": "security_expert"
            },
            
            "concurrence_avancee": {
                "prompt": """D√©bugge ce code concurrent qui a des race conditions :

```python
import threading
import time

class Counter:
    def __init__(self):
        self.value = 0
        self.lock = threading.Lock()
    
    def increment(self):
        with self.lock:
            temp = self.value
            time.sleep(0.001)  # Simule traitement
            self.value = temp + 1
    
    def get_stats(self):
        return {"value": self.value, "threads": threading.active_count()}

# Usage avec 100 threads
counter = Counter()
threads = [threading.Thread(target=counter.increment) for _ in range(100)]
for t in threads: t.start()
for t in threads: t.join()
```

PROBL√àMES CONCURRENCE :
1. Pourquoi le r√©sultat n'est pas 100 ?
2. Le lock prot√®ge-t-il vraiment ?
3. Impl√©mente avec atomic operations
4. G√®re deadlocks potentiels
5. Optimise avec lock-free structures""",
                "evaluation_criteria": [
                    "Identifie que le lock prot√®ge mais logique est fausse",
                    "Explique que temp=value puis value=temp+1 est atomique mais logique fausse",
                    "Propose self.value += 1 directement",
                    "Mentionne threading.Atomic ou multiprocessing.Value",
                    "√âvoque lock-free queues/stacks"
                ],
                "min_score": 3,
                "complexity": "concurrency_expert"
            },
            
            "performance_database": {
                "prompt": """Optimise cette requ√™te SQL qui timeout :

```sql
SELECT u.name, u.email, COUNT(o.id) as order_count, 
       AVG(o.total) as avg_order, MAX(o.created_at) as last_order
FROM users u
LEFT JOIN orders o ON u.id = o.user_id
WHERE u.created_at > '2023-01-01'
AND EXISTS (SELECT 1 FROM orders o2 WHERE o2.user_id = u.id AND o2.total > 100)
GROUP BY u.id, u.name, u.email
HAVING COUNT(o.id) > 5
ORDER BY avg_order DESC;
```

OPTIMISATION BDD :
1. Identifie les goulots d'√©tranglement
2. Quels index cr√©er ?
3. R√©√©cris avec window functions
4. Propose partitioning strategy
5. Impl√©mente avec cache Redis""",
                "evaluation_criteria": [
                    "Identifie EXISTS subquery comme probl√®me",
                    "Propose index sur (user_id, total) et (created_at)",
                    "R√©√©crit avec JOIN au lieu de EXISTS",
                    "Mentionne partitioning par date",
                    "Propose cache avec TTL appropri√©"
                ],
                "min_score": 3,
                "complexity": "database_expert"
            },
            
            "microservices_distributed": {
                "prompt": """Design cette architecture microservices distribu√©e :

CONTEXTE : E-commerce avec 1M+ users, 10K+ orders/day

SERVICES EXISTANTS :
- User Service (auth, profiles)
- Product Service (catalog, inventory)  
- Order Service (orders, payments)
- Notification Service (email, SMS)

D√âFIS ARCHITECTURE :
1. Comment g√©rer les transactions distribu√©es ?
2. Impl√©mente circuit breaker pattern
3. G√®re eventual consistency
4. Design event sourcing pour orders
5. Propose monitoring/observability stack""",
                "evaluation_criteria": [
                    "Propose Saga pattern ou 2PC",
                    "Impl√©mente circuit breaker avec hystrix-like",
                    "Explique CQRS + event sourcing",
                    "Design event store avec snapshots",
                    "Propose Prometheus + Grafana + Jaeger"
                ],
                "min_score": 4,
                "complexity": "system_architect"
            },
            
            "machine_learning_production": {
                "prompt": """Industrialise ce mod√®le ML en production :

```python
import pickle
import pandas as pd
from sklearn.ensemble import RandomForestClassifier

# Mod√®le entra√Æn√©
with open('model.pkl', 'rb') as f:
    model = pickle.load(f)

def predict(data):
    df = pd.DataFrame(data)
    return model.predict(df)
```

PRODUCTION ML :
1. Quels probl√®mes de s√©curit√© avec pickle ?
2. Comment g√©rer model versioning ?
3. Impl√©mente A/B testing pour mod√®les
4. G√®re data drift detection
5. Design MLOps pipeline complet""",
                "evaluation_criteria": [
                    "Identifie pickle security issues",
                    "Propose MLflow ou DVC pour versioning",
                    "Impl√©mente feature flags pour A/B",
                    "Mentionne monitoring distribution shift",
                    "Design CI/CD avec Kubeflow/MLflow"
                ],
                "min_score": 3,
                "complexity": "ml_engineer"
            }
        }
        
        # Questions de debug r√©el
        self.debug_challenges = {
            "memory_leak": """Ce service Python consomme de plus en plus de RAM. Debug :

```python
class DataProcessor:
    def __init__(self):
        self.cache = {}
        self.results = []
    
    def process_file(self, filename):
        if filename in self.cache:
            return self.cache[filename]
        
        with open(filename, 'r') as f:
            data = f.read()
        
        result = self.expensive_computation(data)
        self.cache[filename] = result
        self.results.append(result)
        return result
```

O√π est le memory leak ? Comment le fixer ?""",
            
            "race_condition": """Cette API REST a des bugs al√©atoires. Debug :

```python
user_sessions = {}

@app.route('/login', methods=['POST'])
def login():
    user_id = request.json['user_id']
    session_id = generate_session()
    user_sessions[user_id] = session_id
    return {'session_id': session_id}

@app.route('/logout', methods=['POST']) 
def logout():
    user_id = request.json['user_id']
    if user_id in user_sessions:
        del user_sessions[user_id]
    return {'status': 'logged_out'}
```

Quels bugs peuvent survenir ? Solutions ?"""
        }
        
        # Tests de performance r√©els
        self.performance_challenges = {
            "optimization": """Optimise cette fonction qui traite 1M+ records :

```python
def process_transactions(transactions):
    result = []
    for transaction in transactions:
        if transaction['amount'] > 1000:
            tax = transaction['amount'] * 0.1
            fee = 5.0 if transaction['type'] == 'wire' else 2.0
            result.append({
                'id': transaction['id'],
                'net_amount': transaction['amount'] - tax - fee,
                'tax': tax,
                'fee': fee
            })
    return result
```

Comment optimiser pour 1M+ records ?"""
        }
    
    def get_capabilities(self) -> List[str]:
        """Retourne les capacit√©s de l'agent de test"""
        return [
            "test_models_integration",
            "validate_ollama_connection", 
            "benchmark_performance",
            "test_agent_model_compatibility",
            "generate_validation_report"
        ]
    
    async def startup(self):
        """Initialisation de l'agent de test"""
        self.logger.info("üöÄ Agent Test Mod√®les - D√©marrage")
        
        # Initialisation du gestionnaire de mod√®les
        self.model_manager = ModelManager()
        
        # Validation configuration
        await self._validate_configuration()
        
        self.logger.info("‚úÖ Agent Test Mod√®les - Pr√™t")
    
    async def execute_task(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """üß™ Ex√©cute une t√¢che de test avec les nouveaux tests de d√©veloppement informatique"""
        try:
            task_type = task_data.get("type", "complete_test_suite")
            
            if task_type == "complete_test_suite":
                # Ex√©cute la suite compl√®te de tests avanc√©s
                return await self._run_complete_test_suite()
            elif task_type == "integration_test":
                return await self._run_integration_test(task_data)
            elif task_type == "performance_test":
                return await self._run_performance_test(task_data)
            elif task_type == "model_compatibility":
                return await self._test_model_compatibility(task_data)
            else:
                return {
                    "success": False,
                    "error": f"Type de test non support√©: {task_type}",
                    "available_types": ["complete_test_suite", "integration_test", "performance_test", "model_compatibility"]
                }
                
        except Exception as e:
            self.logger.error(f"‚ùå Erreur ex√©cution test: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def _run_complete_test_suite(self) -> Dict[str, Any]:
        """üß™ Lance la suite compl√®te de tests avec les nouveaux tests de d√©veloppement informatique"""
        start_time = time.time()
        
        try:
            self.logger.info("üß™ D√©marrage suite compl√®te de tests avanc√©s...")
            
            # Initialisation du gestionnaire de mod√®les si n√©cessaire
            if not hasattr(self, 'model_manager'):
                self.model_manager = ModelManager()
            
            # Tests s√©quentiels avec nouveaux tests avanc√©s
            await self._test_model_manager_initialization()
            await self._test_agent_configurations()
            await self._test_ollama_integration()
            await self._test_fallback_mechanisms()
            await self._benchmark_model_performance()
            
            # NOUVEAUX TESTS DE D√âVELOPPEMENT INFORMATIQUE
            await self._test_response_generation()  # Tests challenges d√©veloppement
            await self._test_debug_challenges()     # Tests debugging r√©el
            await self._test_performance_challenges()  # Tests optimisation
            
            # Analyse des co√ªts
            await self._analyze_costs()
            
            # Calcul dur√©e totale
            test_duration = time.time() - start_time
            
            # G√©n√©ration rapport final avec nouvelles m√©triques
            report = self._generate_test_report(test_duration)
            
            self.logger.info(f"‚úÖ Suite de tests avanc√©s termin√©e avec succ√®s en {test_duration:.2f}s")
            return {
                "success": True,
                "data": report,
                "test_type": "complete_suite_advanced",
                "duration": test_duration
            }
            
        except Exception as e:
            test_duration = time.time() - start_time
            self.logger.error(f"‚ùå Erreur suite de tests: {e}")
            return {
                "success": False,
                "error": str(e),
                "test_type": "complete_suite_advanced",
                "duration": test_duration
            }
    
    async def _test_model_manager_initialization(self):
        """üîß Test initialisation du gestionnaire de mod√®les"""
        try:
            if not hasattr(self, 'model_manager'):
                self.model_manager = ModelManager()
            
            # Test configuration charg√©e
            config_valid = hasattr(self.model_manager, 'config') and self.model_manager.config is not None
            
            self.test_results["model_manager_init"] = {
                "initialized": True,
                "config_loaded": config_valid,
                "ollama_available": hasattr(self.model_manager, 'ollama_client'),
                "timestamp": time.time()
            }
            
            self.logger.info("‚úÖ ModelManager initialis√© avec succ√®s")
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur initialisation ModelManager: {e}")
            self.test_results["model_manager_init"] = {"error": str(e)}
    
    async def _test_agent_configurations(self):
        """Test configuration des mod√®les pour diff√©rents agents"""
        
        test_agents = [
            "agent_01_coordinateur_principal",
            "agent_02_architecte_code_expert", 
            "agent_04_expert_securite_crypto",
            "agent_21_security_supply_chain_enterprise",
            "agent_22_architecture_consultant_enterprise"
        ]
        
        for agent_id in test_agents:
            try:
                # Test s√©lection mod√®le g√©n√©ral
                model_general, provider_general = await self.model_manager.get_model_for_agent(agent_id, "general")
                
                # Test s√©lection mod√®le code
                model_code, provider_code = await self.model_manager.get_model_for_agent(agent_id, "code")
                
                # Test s√©lection mod√®le privacy
                model_privacy, provider_privacy = await self.model_manager.get_model_for_agent(agent_id, "privacy")
                
                self.test_results["agent_model_configs"][agent_id] = {
                    "general": {"model": model_general, "provider": provider_general.value},
                    "code": {"model": model_code, "provider": provider_code.value},
                    "privacy": {"model": model_privacy, "provider": provider_privacy.value}
                }
                
                logger.info(f"‚úÖ {agent_id}: G√©n√©ral={model_general}, Code={model_code}, Privacy={model_privacy}")
                
            except Exception as e:
                logger.error(f"‚ùå Erreur config {agent_id}: {e}")
                self.test_results["agent_model_configs"][agent_id] = {"error": str(e)}
    
    async def _test_ollama_integration(self):
        """Test int√©gration Ollama et mod√®les locaux RTX3090"""
        
        try:
            # V√©rification disponibilit√© Ollama
            ollama_models = await self.model_manager.ollama_client.list_models()
            gpu_usage = await self.model_manager.ollama_client.get_gpu_usage()
            
            self.test_results["ollama_status"] = {
                "available": len(ollama_models) > 0,
                "models_count": len(ollama_models),
                "models_list": ollama_models,
                "gpu_usage": gpu_usage
            }
            
            # Test g√©n√©ration avec mod√®les locaux
            if ollama_models:
                for model in ollama_models[:3]:  # Test max 3 mod√®les
                    try:
                        test_prompt = "Bonjour, peux-tu me dire l'heure ?"
                        
                        start_time = time.time()
                        result = await self.model_manager.ollama_client.generate(model, test_prompt)
                        end_time = time.time()
                        
                        self.test_results["ollama_status"][f"test_{model}"] = {
                            "success": result.get("success", False),
                            "response_length": len(result.get("response", "")),
                            "response_time": end_time - start_time,
                            "tokens_per_sec": result.get("tokens_per_sec", 0)
                        }
                        
                        logger.info(f"‚úÖ Test Ollama {model}: {result.get('tokens_per_sec', 0):.1f} tokens/sec")
                        
                    except Exception as e:
                        logger.error(f"‚ùå Erreur test {model}: {e}")
                        self.test_results["ollama_status"][f"test_{model}"] = {"error": str(e)}
            
        except Exception as e:
            logger.error(f"‚ùå Erreur test Ollama: {e}")
            self.test_results["ollama_status"] = {"error": str(e)}
    
    async def _test_fallback_mechanisms(self):
        """Test m√©canismes de fallback local/cloud"""
        
        test_scenarios = [
            {
                "agent": "agent_02_architecte_code_expert",
                "task_type": "code",
                "description": "Test fallback code vers local"
            },
            {
                "agent": "agent_04_expert_securite_crypto", 
                "task_type": "privacy",
                "description": "Test fallback privacy vers local"
            },
            {
                "agent": "agent_21_security_supply_chain_enterprise",
                "task_type": "general",
                "description": "Test fallback enterprise vers cloud"
            }
        ]
        
        for scenario in test_scenarios:
            try:
                prompt = self.test_prompts.get(scenario["task_type"], "Test prompt")
                
                # Test g√©n√©ration normale
                result = await self.model_manager.generate_response(
                    scenario["agent"], 
                    prompt, 
                    scenario["task_type"]
                )
                
                self.test_results["fallback_tests"][scenario["agent"]] = {
                    "task_type": scenario["task_type"],
                    "success": result.get("success", False),
                    "model_used": result.get("model", "unknown"),
                    "provider": result.get("provider", "unknown"),
                    "response_time": result.get("response_time", 0),
                    "cost": result.get("cost", 0)
                }
                
                logger.info(f"‚úÖ Fallback {scenario['agent']}: {result.get('model')} ({result.get('provider')})")
                
            except Exception as e:
                logger.error(f"‚ùå Erreur fallback {scenario['agent']}: {e}")
                self.test_results["fallback_tests"][scenario["agent"]] = {"error": str(e)}
    
    async def _benchmark_model_performance(self):
        """Benchmark performance des diff√©rents mod√®les"""
        
        benchmark_prompts = [
            "√âcris un algorithme de tri rapide en Python.",
            "Explique le machine learning en 3 phrases.",
            "Analyse les avantages de Docker pour le d√©veloppement."
        ]
        
        test_agents = [
            "agent_02_architecte_code_expert",  # Pr√©f√®re local pour code
            "agent_22_architecture_consultant_enterprise"  # Pr√©f√®re cloud pour qualit√©
        ]
        
        for agent_id in test_agents:
            self.test_results["performance_metrics"][agent_id] = {}
            
            for i, prompt in enumerate(benchmark_prompts):
                try:
                    start_time = time.time()
                    
                    result = await self.model_manager.generate_response(
                        agent_id,
                        prompt,
                        "code" if "algorithme" in prompt else "general"
                    )
                    
                    end_time = time.time()
                    
                    self.test_results["performance_metrics"][agent_id][f"test_{i+1}"] = {
                        "prompt_length": len(prompt),
                        "response_length": len(result.get("response", "")),
                        "response_time": end_time - start_time,
                        "model_used": result.get("model", "unknown"),
                        "provider": result.get("provider", "unknown"),
                        "tokens": result.get("tokens", 0),
                        "tokens_per_sec": result.get("tokens_per_sec", 0),
                        "cost": result.get("cost", 0)
                    }
                    
                except Exception as e:
                    logger.error(f"‚ùå Erreur benchmark {agent_id} test {i+1}: {e}")
                    self.test_results["performance_metrics"][agent_id][f"test_{i+1}"] = {"error": str(e)}
    
    async def _test_response_generation(self):
        """üß† Test g√©n√©ration de r√©ponses avec VRAIES questions de d√©veloppement informatique"""
        
        self.test_results["development_challenges"] = {}
        
        logger.info("üöÄ D√âMARRAGE TESTS D√âVELOPPEMENT INFORMATIQUE R√âELS")
        
        for challenge_name, challenge_data in self.development_challenges.items():
            try:
                logger.info(f"üß™ Test {challenge_name} (complexit√©: {challenge_data['complexity']})")
                
                # S√©lection agent selon la complexit√©
                if challenge_data['complexity'] in ['security_expert']:
                    agent_id = "agent_04_expert_securite_crypto"
                elif challenge_data['complexity'] in ['senior', 'expert', 'system_architect']:
                    agent_id = "agent_02_architecte_code_expert"
                elif challenge_data['complexity'] in ['database_expert']:
                    agent_id = "agent_24_gestionnaire_stockage_enterprise"
                elif challenge_data['complexity'] in ['ml_engineer']:
                    agent_id = "agent_25_monitoring_performance_enterprise"
                else:
                    agent_id = "agent_01_coordinateur_principal"
                
                # Mesure performance
                start_time = time.time()
                
                result = await self.model_manager.generate_response(
                    agent_id, 
                    challenge_data['prompt'], 
                    "development_challenge"
                )
                
                response_time = time.time() - start_time
                
                # √âvaluation qualitative de la r√©ponse
                response_text = result.get("response", "")
                evaluation_score = await self._evaluate_technical_response(
                    response_text, 
                    challenge_data['evaluation_criteria'],
                    challenge_data['min_score']
                )
                
                self.test_results["development_challenges"][challenge_name] = {
                    "agent_used": agent_id,
                    "model": result.get("model", "unknown"),
                    "provider": result.get("provider", "unknown"),
                    "complexity": challenge_data['complexity'],
                    "success": result.get("success", False),
                    "response_time": response_time,
                    "tokens": result.get("tokens", 0),
                    "cost": result.get("cost", 0),
                    "evaluation_score": evaluation_score,
                    "min_required_score": challenge_data['min_score'],
                    "passed": evaluation_score >= challenge_data['min_score'],
                    "response_preview": response_text[:200] + "..." if len(response_text) > 200 else response_text,
                    "technical_depth": self._analyze_technical_depth(response_text)
                }
                
                status = "‚úÖ R√âUSSI" if evaluation_score >= challenge_data['min_score'] else "‚ùå √âCHOU√â"
                logger.info(f"{status} {challenge_name}: {evaluation_score}/{len(challenge_data['evaluation_criteria'])} - {result.get('model')} - {response_time:.2f}s")
                
            except Exception as e:
                logger.error(f"‚ùå Erreur test {challenge_name}: {e}")
                self.test_results["development_challenges"][challenge_name] = {
                    "error": str(e),
                    "complexity": challenge_data['complexity'],
                    "passed": False
                }
        
        # Test des questions de debug
        await self._test_debug_challenges()
        
        # Test des optimisations de performance
        await self._test_performance_challenges()
    
    async def _evaluate_technical_response(self, response: str, criteria: List[str], min_score: int) -> int:
        """üéØ √âvalue la qualit√© technique d'une r√©ponse"""
        score = 0
        response_lower = response.lower()
        
        for criterion in criteria:
            # Recherche de mots-cl√©s techniques dans la r√©ponse
            criterion_lower = criterion.lower()
            
            # V√©rifications sp√©cifiques par crit√®re
            if "solid" in criterion_lower and ("single responsibility" in response_lower or "dependency injection" in response_lower):
                score += 1
            elif "dijkstra" in criterion_lower and ("dijkstra" in response_lower or "heap" in response_lower):
                score += 1
            elif "bcrypt" in criterion_lower and ("bcrypt" in response_lower or "scrypt" in response_lower or "argon2" in response_lower):
                score += 1
            elif "circuit breaker" in criterion_lower and ("circuit breaker" in response_lower or "hystrix" in response_lower):
                score += 1
            elif "saga" in criterion_lower and ("saga" in response_lower or "2pc" in response_lower):
                score += 1
            elif "index" in criterion_lower and ("index" in response_lower or "btree" in response_lower):
                score += 1
            elif "prometheus" in criterion_lower and ("prometheus" in response_lower or "grafana" in response_lower):
                score += 1
            elif "mlflow" in criterion_lower and ("mlflow" in response_lower or "versioning" in response_lower):
                score += 1
            elif "pickle" in criterion_lower and ("pickle" in response_lower and "security" in response_lower):
                score += 1
            elif "timing attack" in criterion_lower and ("timing" in response_lower or "constant time" in response_lower):
                score += 1
            # Crit√®res g√©n√©riques
            elif any(keyword in response_lower for keyword in criterion_lower.split() if len(keyword) > 3):
                score += 1
        
        return score
    
    def _analyze_technical_depth(self, response: str) -> Dict[str, Any]:
        """üìä Analyse la profondeur technique d'une r√©ponse"""
        
        # Comptage de termes techniques
        technical_terms = [
            "pattern", "algorithm", "complexity", "optimization", "architecture",
            "security", "encryption", "thread", "concurrency", "database",
            "index", "performance", "scalability", "microservices", "api",
            "class", "function", "method", "interface", "inheritance",
            "polymorphism", "abstraction", "encapsulation", "solid", "dry",
            "kiss", "yagni", "dependency injection", "inversion of control"
        ]
        
        response_lower = response.lower()
        terms_found = [term for term in technical_terms if term in response_lower]
        
        # D√©tection de code
        code_blocks = response.count("```") // 2
        
        # Longueur et structure
        word_count = len(response.split())
        line_count = len(response.split('\n'))
        
        return {
            "technical_terms_count": len(terms_found),
            "technical_terms_found": terms_found[:10],  # Top 10
            "code_blocks": code_blocks,
            "word_count": word_count,
            "line_count": line_count,
            "has_examples": "exemple" in response_lower or "example" in response_lower,
            "has_implementation": code_blocks > 0 or "implement" in response_lower,
            "depth_score": min(10, len(terms_found) + code_blocks * 2)
        }
    
    async def _test_debug_challenges(self):
        """üêõ Test des capacit√©s de debugging avec vrais probl√®mes"""
        
        self.test_results["debug_challenges"] = {}
        
        for challenge_name, challenge_prompt in self.debug_challenges.items():
            try:
                logger.info(f"üêõ Test debug: {challenge_name}")
                
                start_time = time.time()
                
                result = await self.model_manager.generate_response(
                    "agent_02_architecte_code_expert",
                    challenge_prompt,
                    "debug_challenge"
                )
                
                response_time = time.time() - start_time
                response_text = result.get("response", "")
                
                # √âvaluation sp√©cifique au debug
                debug_score = self._evaluate_debug_response(challenge_name, response_text)
                
                self.test_results["debug_challenges"][challenge_name] = {
                    "model": result.get("model", "unknown"),
                    "response_time": response_time,
                    "debug_score": debug_score,
                    "identified_issue": debug_score >= 2,
                    "proposed_solution": debug_score >= 3,
                    "response_preview": response_text[:150] + "..."
                }
                
                status = "‚úÖ IDENTIFI√â" if debug_score >= 2 else "‚ùå RAT√â"
                logger.info(f"{status} Debug {challenge_name}: score {debug_score}/4 - {response_time:.2f}s")
                
            except Exception as e:
                logger.error(f"‚ùå Erreur debug {challenge_name}: {e}")
                self.test_results["debug_challenges"][challenge_name] = {"error": str(e)}
    
    def _evaluate_debug_response(self, challenge_name: str, response: str) -> int:
        """üéØ √âvalue la capacit√© de debugging"""
        score = 0
        response_lower = response.lower()
        
        if challenge_name == "memory_leak":
            if "cache" in response_lower and ("unbounded" in response_lower or "growing" in response_lower):
                score += 1  # Identifie le cache qui grandit
            if "results" in response_lower and ("append" in response_lower or "accumul" in response_lower):
                score += 1  # Identifie l'accumulation dans results
            if "lru" in response_lower or "max size" in response_lower or "ttl" in response_lower:
                score += 1  # Propose solution cache
            if "clear" in response_lower or "del" in response_lower or "pop" in response_lower:
                score += 1  # Propose nettoyage
                
        elif challenge_name == "race_condition":
            if "concurrent" in response_lower or "thread" in response_lower or "race" in response_lower:
                score += 1  # Identifie probl√®me concurrence
            if "dict" in response_lower and ("shared" in response_lower or "global" in response_lower):
                score += 1  # Identifie dict partag√©
            if "lock" in response_lower or "thread-safe" in response_lower:
                score += 1  # Propose synchronisation
            if "redis" in response_lower or "database" in response_lower or "session store" in response_lower:
                score += 1  # Propose stockage externe
        
        return score
    
    async def _test_performance_challenges(self):
        """‚ö° Test des optimisations de performance avec vrais cas"""
        
        self.test_results["performance_challenges"] = {}
        
        for challenge_name, challenge_prompt in self.performance_challenges.items():
            try:
                logger.info(f"‚ö° Test performance: {challenge_name}")
                
                start_time = time.time()
                
                result = await self.model_manager.generate_response(
                    "agent_25_monitoring_performance_enterprise",
                    challenge_prompt,
                    "performance_challenge"
                )
                
                response_time = time.time() - start_time
                response_text = result.get("response", "")
                
                # √âvaluation optimisations propos√©es
                perf_score = self._evaluate_performance_response(response_text)
                
                self.test_results["performance_challenges"][challenge_name] = {
                    "model": result.get("model", "unknown"),
                    "response_time": response_time,
                    "performance_score": perf_score,
                    "optimizations_proposed": perf_score >= 3,
                    "advanced_techniques": perf_score >= 5,
                    "response_preview": response_text[:150] + "..."
                }
                
                status = "‚úÖ OPTIMIS√â" if perf_score >= 3 else "‚ùå BASIQUE"
                logger.info(f"{status} Performance {challenge_name}: score {perf_score}/6 - {response_time:.2f}s")
                
            except Exception as e:
                logger.error(f"‚ùå Erreur performance {challenge_name}: {e}")
                self.test_results["performance_challenges"][challenge_name] = {"error": str(e)}
    
    def _evaluate_performance_response(self, response: str) -> int:
        """‚ö° √âvalue les optimisations de performance propos√©es"""
        score = 0
        response_lower = response.lower()
        
        # Techniques d'optimisation
        optimizations = [
            ("numpy", "vectorization"),  # Vectorisation
            ("pandas", "vectorized"),    # Pandas vectoris√©
            ("list comprehension", "comprehension"),  # List comprehensions
            ("generator", "yield"),      # G√©n√©rateurs
            ("multiprocessing", "parallel"),  # Parall√©lisation
            ("cython", "numba"),        # Compilation
            ("batch", "chunk"),         # Traitement par batch
            ("index", "database"),      # Optimisations DB
            ("cache", "memoiz"),        # Cache/memoization
            ("algorithm", "complexity") # Am√©lioration algorithmique
        ]
        
        for opt1, opt2 in optimizations:
            if opt1 in response_lower or opt2 in response_lower:
                score += 1
        
        return min(score, 6)  # Max 6 points
    
    async def _analyze_costs(self):
        """Analyse des co√ªts d'utilisation"""
        
        system_status = await self.model_manager.get_system_status()
        usage_stats = system_status.get("usage_stats", {})
        
        total_cost = 0
        total_requests = 0
        total_tokens = 0
        
        for model, stats in usage_stats.items():
            total_cost += stats.get("cost", 0)
            total_requests += stats.get("requests", 0)
            total_tokens += stats.get("tokens", 0)
        
        self.test_results["cost_analysis"] = {
            "total_cost_usd": total_cost,
            "total_requests": total_requests,
            "total_tokens": total_tokens,
            "avg_cost_per_request": total_cost / max(total_requests, 1),
            "avg_cost_per_token": total_cost / max(total_tokens, 1),
            "local_models_savings": "100% pour mod√®les locaux",
            "usage_by_model": usage_stats
        }
        
        logger.info(f"üí∞ Co√ªt total: ${total_cost:.4f} - {total_requests} requ√™tes - {total_tokens} tokens")
    
    def _generate_test_report(self, test_duration: float) -> Dict[str, Any]:
        """üìä G√©n√®re le rapport final de tests avec √©valuation technique approfondie"""
        
        # Calcul scores par cat√©gorie
        development_stats = self._calculate_development_stats()
        debug_stats = self._calculate_debug_stats()
        performance_stats = self._calculate_performance_stats()
        
        # Score global technique
        total_technical_score = (
            development_stats["avg_score"] * 0.5 +
            debug_stats["avg_score"] * 0.3 +
            performance_stats["avg_score"] * 0.2
        )
        
        # Rapport final enrichi
        report = {
            "test_summary": {
                "agent": self.agent_id,
                "version": self.version,
                "timestamp": datetime.now().isoformat(),
                "duration_seconds": test_duration,
                "pattern_factory_available": PATTERN_FACTORY_AVAILABLE,
                "test_type": "D√âVELOPPEMENT INFORMATIQUE R√âEL",
                "technical_score": round(total_technical_score, 2),
                "evaluation_level": self._get_evaluation_level(total_technical_score)
            },
            "development_challenges": {
                "total_challenges": development_stats["total"],
                "passed_challenges": development_stats["passed"],
                "success_rate": development_stats["success_rate"],
                "avg_score": development_stats["avg_score"],
                "avg_response_time": development_stats["avg_time"],
                "complexities_tested": development_stats["complexities"]
            },
            "debug_challenges": {
                "total_debug_tests": debug_stats["total"],
                "issues_identified": debug_stats["identified"],
                "solutions_proposed": debug_stats["solutions"],
                "debug_success_rate": debug_stats["success_rate"],
                "avg_debug_score": debug_stats["avg_score"]
            },
            "performance_challenges": {
                "total_perf_tests": performance_stats["total"],
                "optimizations_found": performance_stats["optimized"],
                "advanced_techniques": performance_stats["advanced"],
                "perf_success_rate": performance_stats["success_rate"],
                "avg_perf_score": performance_stats["avg_score"]
            },
            "detailed_results": self.test_results,
            "technical_recommendations": self._generate_technical_recommendations(),
            "model_capabilities_analysis": self._analyze_model_capabilities()
        }
        
        # Sauvegarde rapport
        report_file = Path(__file__).parent.parent / "reports" / f"test_models_integration_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        report_file.parent.mkdir(exist_ok=True)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üìä Rapport sauvegard√©: {report_file}")
        
        return report
    
    def _calculate_development_stats(self) -> Dict[str, Any]:
        """üìä Calcule les statistiques des tests de d√©veloppement"""
        challenges = self.test_results.get("development_challenges", {})
        
        if not challenges:
            return {"total": 0, "passed": 0, "success_rate": 0, "avg_score": 0, "avg_time": 0, "complexities": []}
        
        total = len(challenges)
        passed = sum(1 for result in challenges.values() if result.get("passed", False))
        scores = [result.get("evaluation_score", 0) for result in challenges.values() if "evaluation_score" in result]
        times = [result.get("response_time", 0) for result in challenges.values() if "response_time" in result]
        complexities = list(set(result.get("complexity", "") for result in challenges.values()))
        
        return {
            "total": total,
            "passed": passed,
            "success_rate": (passed / total * 100) if total > 0 else 0,
            "avg_score": sum(scores) / len(scores) if scores else 0,
            "avg_time": sum(times) / len(times) if times else 0,
            "complexities": complexities
        }
    
    def _calculate_debug_stats(self) -> Dict[str, Any]:
        """üêõ Calcule les statistiques des tests de debug"""
        debug_tests = self.test_results.get("debug_challenges", {})
        
        if not debug_tests:
            return {"total": 0, "identified": 0, "solutions": 0, "success_rate": 0, "avg_score": 0}
        
        total = len(debug_tests)
        identified = sum(1 for result in debug_tests.values() if result.get("identified_issue", False))
        solutions = sum(1 for result in debug_tests.values() if result.get("proposed_solution", False))
        scores = [result.get("debug_score", 0) for result in debug_tests.values() if "debug_score" in result]
        
        return {
            "total": total,
            "identified": identified,
            "solutions": solutions,
            "success_rate": (identified / total * 100) if total > 0 else 0,
            "avg_score": sum(scores) / len(scores) if scores else 0
        }
    
    def _calculate_performance_stats(self) -> Dict[str, Any]:
        """‚ö° Calcule les statistiques des tests de performance"""
        perf_tests = self.test_results.get("performance_challenges", {})
        
        if not perf_tests:
            return {"total": 0, "optimized": 0, "advanced": 0, "success_rate": 0, "avg_score": 0}
        
        total = len(perf_tests)
        optimized = sum(1 for result in perf_tests.values() if result.get("optimizations_proposed", False))
        advanced = sum(1 for result in perf_tests.values() if result.get("advanced_techniques", False))
        scores = [result.get("performance_score", 0) for result in perf_tests.values() if "performance_score" in result]
        
        return {
            "total": total,
            "optimized": optimized,
            "advanced": advanced,
            "success_rate": (optimized / total * 100) if total > 0 else 0,
            "avg_score": sum(scores) / len(scores) if scores else 0
        }
    
    def _get_evaluation_level(self, score: float) -> str:
        """üéØ D√©termine le niveau d'√©valuation bas√© sur le score technique"""
        if score >= 8.0:
            return "üöÄ EXPERT - Niveau Senior/Architecte"
        elif score >= 6.0:
            return "‚úÖ AVANC√â - Niveau Confirm√©"
        elif score >= 4.0:
            return "‚ö†Ô∏è INTERM√âDIAIRE - Niveau Junior+"
        elif score >= 2.0:
            return "‚ùå BASIQUE - Niveau D√©butant"
        else:
            return "üí• INSUFFISANT - Formation requise"
    
    def _generate_technical_recommendations(self) -> List[str]:
        """üéØ G√©n√®re des recommandations techniques bas√©es sur les r√©sultats"""
        recommendations = []
        
        # Analyse des challenges de d√©veloppement
        dev_challenges = self.test_results.get("development_challenges", {})
        failed_complexities = []
        
        for challenge_name, result in dev_challenges.items():
            if not result.get("passed", False):
                complexity = result.get("complexity", "unknown")
                failed_complexities.append(f"{challenge_name} ({complexity})")
        
        if failed_complexities:
            recommendations.append(f"‚ùå √âchecs d√©tect√©s: {', '.join(failed_complexities)}")
            recommendations.append("üìö Recommandation: Formation approfondie sur les concepts techniques √©chou√©s")
        
        # Analyse des capacit√©s de debug
        debug_stats = self._calculate_debug_stats()
        if debug_stats["success_rate"] < 50:
            recommendations.append("üêõ Capacit√©s de debugging insuffisantes - Pratiquer l'analyse de code")
        
        # Analyse des optimisations
        perf_stats = self._calculate_performance_stats()
        if perf_stats["success_rate"] < 60:
            recommendations.append("‚ö° Connaissances en optimisation limit√©es - √âtudier les patterns de performance")
        
        # Recommandations par mod√®le
        model_analysis = self._analyze_model_capabilities()
        for model, capabilities in model_analysis.items():
            if capabilities.get("technical_depth", 0) < 5:
                recommendations.append(f"üìà Mod√®le {model}: Am√©liorer les prompts pour plus de profondeur technique")
        
        return recommendations
    
    def _analyze_model_capabilities(self) -> Dict[str, Any]:
        """üîç Analyse les capacit√©s de chaque mod√®le test√©"""
        model_analysis = {}
        
        # Analyse par cat√©gorie de test
        for category, tests in self.test_results.items():
            if isinstance(tests, dict):
                for test_name, result in tests.items():
                    if isinstance(result, dict) and "model" in result:
                        model = result["model"]
                        
                        if model not in model_analysis:
                            model_analysis[model] = {
                                "tests_count": 0,
                                "avg_response_time": 0,
                                "technical_depth": 0,
                                "success_rate": 0,
                                "strengths": [],
                                "weaknesses": []
                            }
                        
                        # Accumulation des m√©triques
                        model_analysis[model]["tests_count"] += 1
                        
                        if "response_time" in result:
                            model_analysis[model]["avg_response_time"] += result["response_time"]
                        
                        if "technical_depth" in result:
                            depth = result["technical_depth"].get("depth_score", 0)
                            model_analysis[model]["technical_depth"] += depth
                        
                        # Identification des forces/faiblesses
                        if result.get("passed", False) or result.get("evaluation_score", 0) >= 3:
                            model_analysis[model]["strengths"].append(test_name)
                        else:
                            model_analysis[model]["weaknesses"].append(test_name)
        
        # Calcul des moyennes
        for model, stats in model_analysis.items():
            if stats["tests_count"] > 0:
                stats["avg_response_time"] /= stats["tests_count"]
                stats["technical_depth"] /= stats["tests_count"]
                stats["success_rate"] = len(stats["strengths"]) / stats["tests_count"] * 100
        
        return model_analysis
    
    def _generate_recommendations(self) -> List[str]:
        """G√©n√®re des recommandations bas√©es sur les r√©sultats de tests"""
        
        recommendations = []
        
        # V√©rification Ollama
        ollama_status = self.test_results.get("ollama_status", {})
        if not ollama_status.get("available", False):
            recommendations.append("‚ùå Ollama non disponible - Installer et configurer Ollama pour mod√®les locaux RTX3090")
        elif ollama_status.get("models_count", 0) < 3:
            recommendations.append("‚ö†Ô∏è Peu de mod√®les Ollama - T√©l√©charger llama3.1:8b, qwen-coder-32b, mixtral-8x7b")
        else:
            recommendations.append("‚úÖ Ollama configur√© correctement avec mod√®les RTX3090")
        
        # V√©rification performance
        performance_data = self.test_results.get("performance_metrics", {})
        slow_responses = []
        for agent, metrics in performance_data.items():
            for test, data in metrics.items():
                if isinstance(data, dict) and data.get("response_time", 0) > 30:
                    slow_responses.append(f"{agent}:{test}")
        
        if slow_responses:
            recommendations.append(f"‚ö†Ô∏è R√©ponses lentes d√©tect√©es: {', '.join(slow_responses)}")
        
        # V√©rification co√ªts
        cost_analysis = self.test_results.get("cost_analysis", {})
        total_cost = cost_analysis.get("total_cost_usd", 0)
        if total_cost > 1.0:
            recommendations.append(f"üí∞ Co√ªt √©lev√© d√©tect√©: ${total_cost:.2f} - Consid√©rer plus d'usage local")
        elif total_cost == 0:
            recommendations.append("‚úÖ Co√ªt optimal - Utilisation mod√®les locaux efficace")
        
        # V√©rification fallback
        fallback_tests = self.test_results.get("fallback_tests", {})
        failed_fallbacks = [agent for agent, result in fallback_tests.items() if result.get("error")]
        if failed_fallbacks:
            recommendations.append(f"‚ùå Fallback √©chou√©s: {', '.join(failed_fallbacks)}")
        
        return recommendations
    
    async def health_check(self) -> Dict[str, Any]:
        """V√©rification de sant√© de l'agent"""
        try:
            # Test connexion Ollama
            ollama_status = await self._check_ollama_health()
            
            # Test gestionnaire mod√®les
            model_manager_status = self._check_model_manager_health()
            
            return {
                "status": "healthy" if ollama_status and model_manager_status else "unhealthy",
                "ollama": ollama_status,
                "model_manager": model_manager_status,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def _check_ollama_health(self) -> bool:
        """V√©rifie la sant√© d'Ollama"""
        try:
            if hasattr(self.model_manager, 'ollama_client'):
                models = await self.model_manager.ollama_client.list_models()
                return len(models) > 0
            return False
        except:
            return False

    def _check_model_manager_health(self) -> bool:
        """V√©rifie la sant√© du gestionnaire de mod√®les"""
        try:
            return hasattr(self, 'model_manager') and self.model_manager is not None
        except:
            return False

    async def _validate_configuration(self):
        """Valide la configuration"""
        self.logger.info("üîß Validation configuration...")
        # Configuration validation logic here
        self.logger.info("‚úÖ Configuration valid√©e")

    async def _run_integration_test(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """Lance un test d'int√©gration"""
        try:
            self.logger.info("üß™ Test d'int√©gration d√©marr√©")
            
            # Test de base avec un mod√®le disponible
            test_prompt = "Bonjour, test de fonctionnement"
            result = await self.model_manager.generate_response(
                agent_id="test_agent",
                prompt=test_prompt,
                task_type="general"
            )
            
            return {
                "success": result.get("success", False),
                "model_used": result.get("model", "unknown"),
                "provider": result.get("provider", "unknown"),
                "response_time": result.get("response_time", 0),
                "test_type": "integration"
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur test int√©gration: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    async def _run_performance_test(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """Lance un test de performance"""
        try:
            self.logger.info("‚ö° Test de performance d√©marr√©")
            
            # Tests de performance basiques
            start_time = time.time()
            
            result = await self.model_manager.generate_response(
                agent_id="test_agent",
                prompt="Test de performance avec prompt court",
                task_type="general"
            )
            
            end_time = time.time()
            response_time = end_time - start_time
            
            return {
                "success": result.get("success", False),
                "response_time": response_time,
                "performance_score": 100 if response_time < 30 else 50,
                "test_type": "performance"
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur test performance: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    async def _test_model_compatibility(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """Test de compatibilit√© des mod√®les"""
        try:
            self.logger.info("üîç Test compatibilit√© mod√®les")
            
            # Test avec diff√©rents agents
            test_agents = [
                "agent_02_architecte_code_expert",
                "agent_04_expert_securite_crypto"
            ]
            
            results = []
            for agent_id in test_agents:
                try:
                    result = await self.model_manager.generate_response(
                        agent_id=agent_id,
                        prompt="Test compatibilit√©",
                        task_type="general"
                    )
                    results.append({
                        "agent": agent_id,
                        "success": result.get("success", False),
                        "model": result.get("model", "unknown")
                    })
                except Exception as e:
                    results.append({
                        "agent": agent_id,
                        "success": False,
                        "error": str(e)
                    })
            
            return {
                "success": any(r["success"] for r in results),
                "results": results,
                "test_type": "compatibility"
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur test compatibilit√©: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def shutdown(self):
        """Arr√™t de l'agent"""
        logger.info("üõë Arr√™t agent test mod√®les")
        return {"status": "shutdown", "timestamp": datetime.now().isoformat()}

# Fonctions utilitaires
async def run_quick_test():
    """Test rapide de l'int√©gration"""
    agent = AgentTestModelsIntegration()
    await agent.startup()
    
    task = Task("test_quick", "Test rapide int√©gration mod√®les")
    result = await agent.execute_task(task)
    
    await agent.shutdown()
    return result

async def run_complete_test():
    """Test complet de l'architecture"""
    agent = AgentTestModelsIntegration()
    await agent.startup()
    
    task = Task("test_complete", "Suite compl√®te de tests")
    result = await agent.execute_task(task)
    
    await agent.shutdown()
    return result

def main():
    """Point d'entr√©e principal"""
    print("üß™ AGENT TEST MOD√àLES IA - PATTERN FACTORY")
    print("=" * 50)
    
    # Test complet
    result = asyncio.run(run_complete_test())
    
    if result.success:
        print("‚úÖ Tests termin√©s avec succ√®s")
        print(f"üìä Rapport: {len(result.data.get('detailed_results', {}))} cat√©gories test√©es")
    else:
        print(f"‚ùå Tests √©chou√©s: {result.error}")

if __name__ == "__main__":
    main() 