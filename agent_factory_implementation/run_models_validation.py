#!/usr/bin/env python3
"""
üöÄ VALIDATION COMPL√àTE ARCHITECTURE MOD√àLES IA
==============================================

Script de validation exhaustive de la nouvelle architecture de gestion
des mod√®les IA avec support Ollama RTX3090 et fallback cloud.

√âTAPES VALIDATION :
1. ‚úÖ V√©rification configuration
2. üè† Test Ollama RTX3090  
3. ‚òÅÔ∏è Test providers cloud
4. üîÑ Test m√©canismes fallback
5. üß™ Ex√©cution agent test
6. üìä G√©n√©ration rapport final

Usage:
    python run_models_validation.py
    python run_models_validation.py --quick
    python run_models_validation.py --ollama-only

Version: 1.0.0
Cr√©√©: 19 juin 2025 - 18h15
"""

import argparse
import asyncio
import json
from logging_manager_optimized import LoggingManager
import os
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any

# Configuration logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
# LoggingManager NextGeneration - Agent
        from logging_manager_optimized import LoggingManager
        self.logger = LoggingManager().get_agent_logger(
            agent_name="ModelsArchitectureValidator",
            role="ai_processor",
            domain="general",
            async_enabled=True
        )

# Ajout chemin modules
sys.path.insert(0, str(Path(__file__).parent))

class ModelsArchitectureValidator:
    """Validateur complet de l'architecture mod√®les IA"""
    
    def __init__(self, quick_mode: bool = False, ollama_only: bool = False):
        self.quick_mode = quick_mode
        self.ollama_only = ollama_only
        self.start_time = datetime.now()
        
        self.validation_results = {
            "start_time": self.start_time.isoformat(),
            "config_validation": {},
            "ollama_validation": {},
            "cloud_validation": {},
            "fallback_validation": {},
            "agent_test_results": {},
            "performance_metrics": {},
            "recommendations": [],
            "final_score": 0
        }
        
        logger.info(f"üöÄ Validation architecture mod√®les - Mode: {'Quick' if quick_mode else 'Complet'}")
        if ollama_only:
            logger.info("üè† Mode Ollama uniquement")
    
    async def run_complete_validation(self) -> Dict[str, Any]:
        """Ex√©cute la validation compl√®te"""
        
        try:
            # √âtape 1: Validation configuration
            logger.info("üìã √âTAPE 1: Validation configuration")
            await self._validate_configuration()
            
            # √âtape 2: Test Ollama RTX3090
            logger.info("üè† √âTAPE 2: Test Ollama RTX3090")
            await self._validate_ollama()
            
            # √âtape 3: Test providers cloud (si pas ollama-only)
            if not self.ollama_only:
                logger.info("‚òÅÔ∏è √âTAPE 3: Test providers cloud")
                await self._validate_cloud_providers()
                
                # √âtape 4: Test fallback
                logger.info("üîÑ √âTAPE 4: Test m√©canismes fallback")
                await self._validate_fallback_mechanisms()
            
            # √âtape 5: Test agent int√©gration
            logger.info("ü§ñ √âTAPE 5: Test agent int√©gration")
            await self._run_agent_integration_test()
            
            # √âtape 6: M√©triques performance
            if not self.quick_mode:
                logger.info("‚ö° √âTAPE 6: M√©triques performance")
                await self._measure_performance()
            
            # √âtape 7: G√©n√©ration rapport final
            logger.info("üìä √âTAPE 7: G√©n√©ration rapport final")
            self._generate_final_report()
            
            return self.validation_results
            
        except Exception as e:
            logger.error(f"‚ùå Erreur validation: {e}")
            self.validation_results["fatal_error"] = str(e)
            return self.validation_results
    
    async def _validate_configuration(self):
        """Validation configuration mod√®les"""
        
        try:
            config_path = Path(__file__).parent / "config" / "models_config.json"
            
            if not config_path.exists():
                self.validation_results["config_validation"] = {
                    "error": "Fichier models_config.json non trouv√©",
                    "file_exists": False
                }
                return
            
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            # V√©rification structure (nouvelle version)
            required_sections = ["default_models", "providers", "agent_configurations"]
            missing_sections = [s for s in required_sections if s not in config]
            
            # R√©cup√©ration des configurations agents
            agent_configs = config.get("agent_configurations", {})
            
            # V√©rification agents enterprise
            enterprise_agents = [
                "agent_21_enterprise_orchestrator",
                "agent_22_architecture_consultant_enterprise", 
                "agent_23_fastapi_enterprise_specialist",
                "agent_24_enterprise_storage_specialist",
                "agent_25_enterprise_monitoring_specialist"
            ]
            
            missing_agents = [agent for agent in enterprise_agents if agent not in agent_configs]
            
            # Configuration Ollama
            ollama_config = config.get("providers", {}).get("ollama", {})
            ollama_enabled = ollama_config.get("enabled", False)
            
            self.validation_results["config_validation"] = {
                "file_exists": True,
                "json_valid": True,
                "structure_complete": len(missing_sections) == 0,
                "missing_sections": missing_sections,
                "enterprise_agents_count": len(enterprise_agents) - len(missing_agents),
                "missing_enterprise_agents": missing_agents,
                "ollama_configured": bool(ollama_config),
                "ollama_enabled": ollama_enabled,
                "total_agents": len(agent_configs),
                "config_version": config.get("version", "unknown")
            }
            
            logger.info(f"‚úÖ Configuration: {len(agent_configs)} agents, Ollama {'‚úÖ' if ollama_enabled else '‚ùå'}")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur validation config: {e}")
            self.validation_results["config_validation"] = {
                "error": str(e),
                "file_exists": config_path.exists()
            }
    
    async def _validate_ollama(self):
        """Validation Ollama RTX3090"""
        
        try:
            from core.model_manager import OllamaClient
            
            ollama_client = OllamaClient()
            
            # Test connexion
            start_time = time.time()
            models = await ollama_client.list_models()
            connection_time = time.time() - start_time
            
            # Mod√®les r√©ellement disponibles selon ollama list
            available_models_list = [
                "mixtral:8x7b-instruct-v0.1-q3_k_m",
                "nous-hermes-2-mistral-7b-dpo:latest", 
                "llama3:8b-instruct-q6_k",
                "qwen2.5-coder:1.5b",
                "starcoder2:3b",
                "qwen-coder-32b:latest",
                "code-stral:latest",
                "deepseek-coder:33b",
                "lux_model:latest",
                "nomic-embed-text:latest"
            ]
            
            # V√©rification disponibilit√© des mod√®les configur√©s
            available_models = []
            missing_models = []
            
            for model in available_models_list:
                if model in models:
                    available_models.append(model)
                else:
                    missing_models.append(model)
            
            # Test g√©n√©ration avec premier mod√®le disponible
            generation_test = {"success": False, "error": "Aucun mod√®le disponible"}
            
            if models:
                test_model = models[0]
                try:
                    start_gen = time.time()
                    result = await ollama_client.generate(test_model, "Bonjour, r√©ponds en une phrase.")
                    end_gen = time.time()
                    
                    generation_test = {
                        "success": result.get("success", False),
                        "model": test_model,
                        "response_time": end_gen - start_gen,
                        "tokens_per_sec": result.get("tokens_per_sec", 0),
                        "response_preview": result.get("response", "")[:100]
                    }
                    
                except Exception as e:
                    generation_test = {"success": False, "error": str(e)}
            
            # Monitoring GPU
            gpu_status = await ollama_client.get_gpu_usage()
            
            self.validation_results["ollama_validation"] = {
                "connection_success": len(models) >= 0,  # M√™me 0 mod√®le = connexion OK
                "connection_time": connection_time,
                "total_models": len(models),
                "models_list": models,
                "available_models_count": len(available_models),
                "missing_models_count": len(missing_models),
                "missing_models": missing_models,
                "generation_test": generation_test,
                "gpu_monitoring": gpu_status
            }
            
            logger.info(f"‚úÖ Ollama: {len(models)} mod√®les, {len(available_models)} configur√©s disponibles")
            
        except ImportError:
            logger.error("‚ùå OllamaClient non disponible")
            self.validation_results["ollama_validation"] = {
                "error": "OllamaClient non importable"
            }
        except Exception as e:
            logger.error(f"‚ùå Erreur Ollama: {e}")
            self.validation_results["ollama_validation"] = {
                "error": str(e)
            }
    
    async def _validate_cloud_providers(self):
        """Validation providers cloud"""
        
        try:
            from core.model_manager import ModelManager
            
            model_manager = ModelManager()
            
            # Test s√©lection mod√®les cloud
            cloud_tests = []
            
            test_agents = [
                ("agent_22_architecture_consultant_enterprise", "general"),
                ("agent_02_architecte_code_expert", "code"), 
                ("agent_04_expert_securite_crypto", "privacy")
            ]
            
            for agent_id, task_type in test_agents:
                try:
                    model, provider = await model_manager.get_model_for_agent(agent_id, task_type)
                    
                    cloud_tests.append({
                        "agent": agent_id,
                        "task_type": task_type,
                        "model": model,
                        "provider": provider.value,
                        "is_cloud": provider.value in ["anthropic", "openai"]
                    })
                    
                except Exception as e:
                    cloud_tests.append({
                        "agent": agent_id,
                        "task_type": task_type,
                        "error": str(e)
                    })
            
            # V√©rification API keys (sans les exposer)
            api_keys_status = {}
            for provider in ["ANTHROPIC_API_KEY", "OPENAI_API_KEY", "PERPLEXITY_API_KEY"]:
                api_keys_status[provider] = bool(os.getenv(provider))
            
            self.validation_results["cloud_validation"] = {
                "model_selection_tests": cloud_tests,
                "api_keys_configured": api_keys_status,
                "total_cloud_providers": sum(1 for v in api_keys_status.values() if v)
            }
            
            logger.info(f"‚úÖ Cloud: {len([t for t in cloud_tests if 'error' not in t])} tests OK")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur validation cloud: {e}")
            self.validation_results["cloud_validation"] = {"error": str(e)}
    
    async def _validate_fallback_mechanisms(self):
        """Validation m√©canismes fallback"""
        
        try:
            from core.model_manager import ModelManager
            from unittest.mock import patch
            
            model_manager = ModelManager()
            
            fallback_tests = []
            
            # Test 1: Fallback local vers cloud (mock √©chec Ollama)
            try:
                with patch.object(model_manager.ollama_client, 'generate', side_effect=Exception("Mock Ollama failure")):
                    result = await model_manager.generate_response(
                        "agent_02_architecte_code_expert",
                        "Test fallback local->cloud",
                        "code"
                    )
                    
                    fallback_tests.append({
                        "test": "local_to_cloud",
                        "success": result.get("success", False),
                        "final_provider": result.get("provider", "unknown"),
                        "expected_cloud": True
                    })
                    
            except Exception as e:
                fallback_tests.append({
                    "test": "local_to_cloud",
                    "error": str(e)
                })
            
            # Test 2: Fallback cloud vers local (mock √©chec cloud)
            try:
                with patch.object(model_manager, '_generate_anthropic', side_effect=Exception("Mock Anthropic failure")):
                    with patch.object(model_manager, '_generate_openai', side_effect=Exception("Mock OpenAI failure")):
                        result = await model_manager.generate_response(
                            "agent_22_architecture_consultant_enterprise",
                            "Test fallback cloud->local",
                            "general"
                        )
                        
                        fallback_tests.append({
                            "test": "cloud_to_local",
                            "success": result.get("success", False),
                            "final_provider": result.get("provider", "unknown"),
                            "expected_local": True
                        })
                        
            except Exception as e:
                fallback_tests.append({
                    "test": "cloud_to_local",
                    "error": str(e)
                })
            
            self.validation_results["fallback_validation"] = {
                "tests": fallback_tests,
                "total_tests": len(fallback_tests),
                "successful_tests": len([t for t in fallback_tests if t.get("success", False)])
            }
            
            logger.info(f"‚úÖ Fallback: {len([t for t in fallback_tests if 'error' not in t])}/2 tests")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur validation fallback: {e}")
            self.validation_results["fallback_validation"] = {"error": str(e)}
    
    async def _run_agent_integration_test(self):
        """Test int√©gration avec agent de test"""
        
        try:
            from agents.agent_test_models_integration import AgentTestModelsIntegration
            from core.agent_factory_architecture import Task
            
            # Cr√©er et d√©marrer agent test
            test_agent = AgentTestModelsIntegration()
            await test_agent.startup()
            
            # Ex√©cuter test complet
            if self.quick_mode:
                task = Task("test_quick", "Test rapide int√©gration")
            else:
                task = Task("test_complete", "Test complet int√©gration")
            
            start_time = time.time()
            result = await test_agent.execute_task(task)
            end_time = time.time()
            
            await test_agent.shutdown()
            
            self.validation_results["agent_test_results"] = {
                "success": result.success,
                "execution_time": end_time - start_time,
                "data": result.data if result.success else None,
                "error": result.error if not result.success else None
            }
            
            logger.info(f"‚úÖ Agent test: {'Succ√®s' if result.success else '√âchec'}")
            
        except ImportError as e:
            logger.error(f"‚ùå Agent test non disponible: {e}")
            self.validation_results["agent_test_results"] = {
                "error": f"Import failed: {str(e)}"
            }
        except Exception as e:
            logger.error(f"‚ùå Erreur agent test: {e}")
            self.validation_results["agent_test_results"] = {
                "error": str(e)
            }
    
    async def _measure_performance(self):
        """Mesure des performances"""
        
        try:
            from core.model_manager import ModelManager
            
            model_manager = ModelManager()
            
            # Benchmark diff√©rents types de requ√™tes
            benchmark_tests = [
                {"prompt": "Bonjour", "type": "fast", "expected_time": 10},
                {"prompt": "Explique l'IA en 2 phrases", "type": "general", "expected_time": 20},
                {"prompt": "√âcris une fonction Python pour trier une liste", "type": "code", "expected_time": 30}
            ]
            
            performance_results = []
            
            for test in benchmark_tests:
                try:
                    start_time = time.time()
                    
                    result = await model_manager.generate_response(
                        "agent_02_architecte_code_expert",
                        test["prompt"],
                        test["type"]
                    )
                    
                    end_time = time.time()
                    response_time = end_time - start_time
                    
                    performance_results.append({
                        "test_type": test["type"],
                        "prompt_length": len(test["prompt"]),
                        "response_time": response_time,
                        "expected_time": test["expected_time"],
                        "within_expected": response_time <= test["expected_time"],
                        "model_used": result.get("model", "unknown"),
                        "provider": result.get("provider", "unknown"),
                        "tokens": result.get("tokens", 0),
                        "tokens_per_sec": result.get("tokens_per_sec", 0)
                    })
                    
                except Exception as e:
                    performance_results.append({
                        "test_type": test["type"],
                        "error": str(e)
                    })
            
            # Calcul m√©triques globales
            successful_tests = [r for r in performance_results if "error" not in r]
            if successful_tests:
                avg_response_time = sum(r["response_time"] for r in successful_tests) / len(successful_tests)
                within_expected_count = sum(1 for r in successful_tests if r.get("within_expected", False))
            else:
                avg_response_time = 0
                within_expected_count = 0
            
            self.validation_results["performance_metrics"] = {
                "benchmark_tests": performance_results,
                "total_tests": len(benchmark_tests),
                "successful_tests": len(successful_tests),
                "avg_response_time": avg_response_time,
                "within_expected_count": within_expected_count,
                "performance_score": (within_expected_count / len(benchmark_tests)) * 100
            }
            
            logger.info(f"‚ö° Performance: {avg_response_time:.2f}s moyen, {within_expected_count}/{len(benchmark_tests)} dans temps attendu")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur mesure performance: {e}")
            self.validation_results["performance_metrics"] = {"error": str(e)}
    
    def _generate_final_report(self):
        """G√©n√®re le rapport final et calcule le score"""
        
        # Calcul score final
        scores = []
        
        # Score configuration (20%)
        config_val = self.validation_results.get("config_validation", {})
        if config_val.get("structure_complete") and config_val.get("ollama_enabled"):
            scores.append(20)
        elif config_val.get("structure_complete"):
            scores.append(15)
        else:
            scores.append(0)
        
        # Score Ollama (30%)
        ollama_val = self.validation_results.get("ollama_validation", {})
        if ollama_val.get("connection_success") and ollama_val.get("generation_test", {}).get("success"):
            scores.append(30)
        elif ollama_val.get("connection_success"):
            scores.append(20)
        else:
            scores.append(0)
        
        # Score cloud (20% si test√©)
        if not self.ollama_only:
            cloud_val = self.validation_results.get("cloud_validation", {})
            cloud_tests = cloud_val.get("model_selection_tests", [])
            successful_cloud = len([t for t in cloud_tests if "error" not in t])
            if successful_cloud >= len(cloud_tests) * 0.8:
                scores.append(20)
            elif successful_cloud >= len(cloud_tests) * 0.5:
                scores.append(10)
            else:
                scores.append(0)
        else:
            scores.append(20)  # Score plein si Ollama only
        
        # Score fallback (15% si test√©)
        if not self.ollama_only:
            fallback_val = self.validation_results.get("fallback_validation", {})
            if fallback_val.get("successful_tests", 0) >= 1:
                scores.append(15)
            else:
                scores.append(0)
        else:
            scores.append(15)  # Score plein si Ollama only
        
        # Score agent test (15%)
        agent_test = self.validation_results.get("agent_test_results", {})
        if agent_test.get("success"):
            scores.append(15)
        else:
            scores.append(0)
        
        final_score = sum(scores)
        self.validation_results["final_score"] = final_score
        
        # G√©n√©ration recommandations
        recommendations = []
        
        if final_score >= 90:
            recommendations.append("üéâ EXCELLENT - Architecture mod√®les pr√™te pour production")
        elif final_score >= 75:
            recommendations.append("‚úÖ BON - Architecture fonctionnelle avec am√©liorations mineures")
        elif final_score >= 60:
            recommendations.append("‚ö†Ô∏è MOYEN - Probl√®mes √† corriger avant production")
        else:
            recommendations.append("‚ùå CRITIQUE - Architecture n√©cessite refonte majeure")
        
        # Recommandations sp√©cifiques
        if not ollama_val.get("connection_success"):
            recommendations.append("üîß Installer et configurer Ollama pour mod√®les locaux")
        
        if ollama_val.get("missing_models"):
            missing = ollama_val["missing_models"]
            recommendations.append(f"üì¶ T√©l√©charger mod√®les manquants: {', '.join(missing)}")
        
        if not self.ollama_only:
            cloud_val = self.validation_results.get("cloud_validation", {})
            api_keys = cloud_val.get("api_keys_configured", {})
            missing_keys = [k for k, v in api_keys.items() if not v]
            if missing_keys:
                recommendations.append(f"üîë Configurer API keys: {', '.join(missing_keys)}")
        
        self.validation_results["recommendations"] = recommendations
        
        # Temps total
        end_time = datetime.now()
        total_duration = (end_time - self.start_time).total_seconds()
        self.validation_results["end_time"] = end_time.isoformat()
        self.validation_results["total_duration"] = total_duration
        
        # Sauvegarde rapport
        report_file = Path(__file__).parent / "reports" / f"models_validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        report_file.parent.mkdir(exist_ok=True)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.validation_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üìä Score final: {final_score}/100")
        logger.info(f"üìÑ Rapport sauvegard√©: {report_file}")

async def main():
    """Point d'entr√©e principal"""
    
    parser = argparse.ArgumentParser(description="Validation architecture mod√®les IA")
    parser.add_argument("--quick", action="store_true", help="Mode validation rapide")
    parser.add_argument("--ollama-only", action="store_true", help="Test Ollama uniquement")
    
    args = parser.parse_args()
    
    print("üöÄ VALIDATION ARCHITECTURE MOD√àLES IA - PATTERN FACTORY")
    print("=" * 60)
    
    validator = ModelsArchitectureValidator(
        quick_mode=args.quick,
        ollama_only=args.ollama_only
    )
    
    # Ex√©cution validation
    start_time = time.time()
    results = await validator.run_complete_validation()
    end_time = time.time()
    
    # Affichage r√©sultats
    print("\nüìä R√âSULTATS VALIDATION")
    print("-" * 30)
    
    final_score = results.get("final_score", 0)
    print(f"Score final: {final_score}/100")
    
    if final_score >= 90:
        print("üéâ EXCELLENT - Pr√™t pour production")
    elif final_score >= 75:
        print("‚úÖ BON - Fonctionnel")
    elif final_score >= 60:
        print("‚ö†Ô∏è MOYEN - Am√©liorations n√©cessaires")
    else:
        print("‚ùå CRITIQUE - Refonte requise")
    
    print(f"\nDur√©e: {end_time - start_time:.1f}s")
    
    # Recommandations
    recommendations = results.get("recommendations", [])
    if recommendations:
        print("\nüéØ RECOMMANDATIONS:")
        for rec in recommendations:
            print(f"  {rec}")
    
    return 0 if final_score >= 75 else 1

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code) 