#!/usr/bin/env python3
"""
VALIDATION MAINTENANCE AGENTS 108 & 109
=======================================

Script de validation pour tester la qualitÃ© de l'implÃ©mentation 
du nouvel orchestrateur et des modifications de l'agent adaptateur v4.3.0
sur les agents spÃ©cifiÃ©s.

Agents cibles:
- agent_108_performance_optimizer.py
- agent_109_pattern_factory_version.py

Author: Ã‰quipe NextGeneration
Version: Validation v1.0.0
"""

import asyncio
import sys
import time
from pathlib import Path
from datetime import datetime

# Configuration du chemin
PROJECT_ROOT = Path(__file__).resolve().parent
sys.path.insert(0, str(PROJECT_ROOT))

try:
    from agents.agent_MAINTENANCE_03_adaptateur_code import AgentMAINTENANCE03AdaptateurCode
    from core.agent_factory_architecture import Task, Result
except ImportError as e:
    print(f"âŒ Erreur import: {e}")
    sys.exit(1)

class ValidationMaintenanceAgents:
    """Validation de la maintenance des agents 108 et 109"""
    
    def __init__(self):
        self.adaptateur = AgentMAINTENANCE03AdaptateurCode(
            id="validation_108_109",
            pattern_confidence_threshold=0.75,
            enable_pattern_learning=True
        )
        
        # Chemins des agents cibles
        self.agent_108_path = PROJECT_ROOT / "agents" / "agent_108_performance_optimizer.py"
        self.agent_109_path = PROJECT_ROOT / "agents" / "agent_109_pattern_factory_version.py"
    
    async def analyze_agent_quality(self, agent_path: Path) -> Dict[str, Any]:
        """Analyse la qualitÃ© d'un agent avec l'adaptateur v4.3.0"""
        print(f"\nğŸ“Š ANALYSE QUALITÃ‰: {agent_path.name}")
        print("="*60)
        
        if not agent_path.exists():
            return {"error": f"Agent non trouvÃ©: {agent_path}"}
        
        try:
            # Lecture du code de l'agent
            code_content = agent_path.read_text(encoding='utf-8')
            print(f"âœ… Agent chargÃ©: {len(code_content):,} caractÃ¨res")
            print(f"âœ… Lignes de code: {len(code_content.split(''))}")
            
            # Test de la validation multi-niveaux
            validation_result = self.adaptateur.validate_multi_level(code_content)
            
            print(f"\nğŸ” RÃ‰SULTATS VALIDATION MULTI-NIVEAUX:")
            print(f"   Syntaxe valide: {'âœ…' if validation_result.syntax_valid else 'âŒ'} {validation_result.syntax_valid}")
            print(f"   SÃ©mantique valide: {'âœ…' if validation_result.semantic_valid else 'âŒ'} {validation_result.semantic_valid}")
            print(f"   Compilation rÃ©ussie: {'âœ…' if validation_result.compilation_successful else 'âŒ'} {validation_result.compilation_successful}")
            print(f"   RÃ©solution imports: {'âœ…' if validation_result.import_resolution else 'âŒ'} {validation_result.import_resolution}")
            print(f"   Score confiance: {validation_result.confidence_score:.2f}/1.0")
            
            if validation_result.issues:
                print(f"\nâš ï¸  Issues dÃ©tectÃ©es ({len(validation_result.issues)}):")
                for i, issue in enumerate(validation_result.issues[:5], 1):
                    print(f"   {i}. {issue}")
                if len(validation_result.issues) > 5:
                    print(f"   ... et {len(validation_result.issues) - 5} autres")
            
            if validation_result.warnings:
                print(f"\nğŸ’¡ Warnings ({len(validation_result.warnings)}):")
                for i, warning in enumerate(validation_result.warnings[:3], 1):
                    print(f"   {i}. {warning}")
                if len(validation_result.warnings) > 3:
                    print(f"   ... et {len(validation_result.warnings) - 3} autres")
            
            # Analyse des patterns d'indentation (si applicable)
            if not validation_result.syntax_valid:
                print(f"\nğŸ”§ ANALYSE PATTERNS D'INDENTATION:")
                pattern_analysis = self.adaptateur.analyze_indentation_pattern(
                    code_content, 
                    "analyse prÃ©ventive"
                )
                print(f"   Pattern type: {pattern_analysis['pattern_type']}")
                print(f"   Style indentation: {pattern_analysis['indentation_style']}")
                print(f"   Niveau indentation: {pattern_analysis['indentation_level']}")
                print(f"   Score complexitÃ©: {pattern_analysis['complexity_score']:.2f}")
            
            return {
                "agent_path": str(agent_path),
                "agent_name": agent_path.name,
                "code_length": len(code_content),
                "lines_count": len(code_content.split('\n')),
                "validation_result": validation_result.__dict__,
                "overall_score": validation_result.confidence_score,
                "needs_maintenance": validation_result.confidence_score < 0.8 or len(validation_result.issues) > 0,
                "chromadb_enabled": self.adaptateur.chroma_patterns.enabled,
                "postgresql_enabled": self.adaptateur.pg_analytics.enabled
            }
            
        except Exception as e:
            print(f"âŒ Erreur analyse agent: {e}")
            return {"error": str(e), "agent_path": str(agent_path)}
    
    async def test_adaptateur_improvements(self, agent_path: Path) -> Dict[str, Any]:
        """Test des amÃ©liorations de l'adaptateur v4.3.0 sur un agent"""
        print(f"\nğŸš€ TEST AMÃ‰LIORATIONS v4.3.0: {agent_path.name}")
        print("="*60)
        
        if not agent_path.exists():
            return {"error": f"Agent non trouvÃ©: {agent_path}"}
        
        try:
            code_content = agent_path.read_text(encoding='utf-8')
            
            # Simulation d'une tÃ¢che d'adaptation v4.3.0
            task = Task(
                id=f"test_v43_{agent_path.stem}",
                params={
                    "code": code_content,
                    "feedback": "Analyse qualitÃ© code agent",
                    "error_type": "generic",
                    "use_import_discovery": True,
                    "validate_result": True,
                    # Nouvelles options v4.3.0
                    "use_pattern_learning": True,
                    "enable_analytics": True
                }
            )
            
            # ExÃ©cution avec mesure du temps
            start_time = time.time()
            await self.adaptateur.startup()
            result = await self.adaptateur.execute_task(task)
            processing_time = time.time() - start_time
            
            if result.success:
                data = result.data
                
                print(f"âœ… Traitement rÃ©ussi en {processing_time:.3f}s")
                print(f"âœ… Adaptations: {len(data.get('adaptations', []))}")
                
                # Nouvelles mÃ©triques v4.3.0
                advanced_confidence = data.get("advanced_confidence_score", 0)
                patterns_found = data.get("patterns_found", 0)
                patterns_used = data.get("patterns_used", 0)
                chromadb_enabled = data.get("chromadb_enabled", False)
                postgresql_enabled = data.get("postgresql_enabled", False)
                pattern_learning_used = data.get("pattern_learning_used", False)
                
                print(f"\nğŸ“Š MÃ‰TRIQUES v4.3.0:")
                print(f"   Score confiance avancÃ©: {advanced_confidence:.2f}")
                print(f"   Patterns trouvÃ©s: {patterns_found}")
                print(f"   Patterns utilisÃ©s: {patterns_used}")
                print(f"   ChromaDB actif: {'âœ…' if chromadb_enabled else 'âŒ'}")
                print(f"   PostgreSQL actif: {'âœ…' if postgresql_enabled else 'âŒ'}")
                print(f"   Apprentissage patterns: {'âœ…' if pattern_learning_used else 'âŒ'}")
                
                validation_pre = data.get("validation_pre", {})
                validation_post = data.get("validation_post", {})
                
                if validation_pre and validation_post:
                    pre_score = validation_pre.get("confidence_score", 0)
                    post_score = validation_post.get("confidence_score", 0)
                    improvement = post_score - pre_score
                    
                    print(f"\nğŸ“ˆ AMÃ‰LIORATION QUALITÃ‰:")
                    print(f"   Score prÃ©: {pre_score:.2f}")
                    print(f"   Score post: {post_score:.2f}")
                    print(f"   AmÃ©lioration: {improvement:+.2f} points")
                
                return {
                    "success": True,
                    "processing_time": processing_time,
                    "adaptations_count": len(data.get('adaptations', [])),
                    "advanced_confidence": advanced_confidence,
                    "patterns_metrics": {
                        "found": patterns_found,
                        "used": patterns_used,
                        "learning_used": pattern_learning_used
                    },
                    "databases_status": {
                        "chromadb": chromadb_enabled,
                        "postgresql": postgresql_enabled
                    },
                    "quality_improvement": improvement if 'improvement' in locals() else 0
                }
            else:
                print(f"âŒ Ã‰chec traitement: {result.error}")
                return {"success": False, "error": result.error}
                
        except Exception as e:
            print(f"âŒ Erreur test amÃ©liorations: {e}")
            return {"error": str(e)}
        finally:
            await self.adaptateur.shutdown()
    
    async def validate_agent_architecture_compliance(self, agent_path: Path) -> Dict[str, Any]:
        """Valide la conformitÃ© architecturale d'un agent"""
        print(f"\nğŸ—ï¸  VALIDATION CONFORMITÃ‰ ARCHITECTURALE: {agent_path.name}")
        print("="*60)
        
        if not agent_path.exists():
            return {"error": f"Agent non trouvÃ©: {agent_path}"}
        
        try:
            code_content = agent_path.read_text(encoding='utf-8')
            
            # VÃ©rifications architecturales
            compliance_checks = {
                "has_agent_import": "from core.agent_factory_architecture import Agent" in code_content,
                "has_task_import": "Task" in code_content,
                "has_result_import": "Result" in code_content,
                "has_async_methods": "async def" in code_content,
                "has_startup_method": "def startup(" in code_content or "async def startup(" in code_content,
                "has_shutdown_method": "def shutdown(" in code_content or "async def shutdown(" in code_content,
                "has_execute_task": "def execute_task(" in code_content or "async def execute_task(" in code_content,
                "has_health_check": "def health_check(" in code_content or "async def health_check(" in code_content,
                "has_get_capabilities": "def get_capabilities(" in code_content,
                "has_docstring": '"""' in code_content[:500] or "'''" in code_content[:500],
                "has_logging": "import logging" in code_content or "self.logger" in code_content,
                "has_error_handling": "try:" in code_content and "except" in code_content
            }
            
            passed_checks = sum(1 for check in compliance_checks.values() if check)
            total_checks = len(compliance_checks)
            compliance_score = passed_checks / total_checks
            
            print(f"ğŸ“‹ RÃ‰SULTATS CONFORMITÃ‰ ({passed_checks}/{total_checks}):")
            for check_name, passed in compliance_checks.items():
                status = "âœ…" if passed else "âŒ"
                print(f"   {status} {check_name.replace('_', ' ').title()}")
            
            print(f"\nğŸ¯ Score conformitÃ©: {compliance_score:.2f} ({compliance_score*100:.1f}%)")
            
            # Recommandations
            if compliance_score < 1.0:
                print(f"\nğŸ’¡ RECOMMANDATIONS:")
                for check_name, passed in compliance_checks.items():
                    if not passed:
                        recommendations = {
                            "has_agent_import": "Ajouter: from core.agent_factory_architecture import Agent",
                            "has_async_methods": "Utiliser des mÃ©thodes async pour startup, shutdown, execute_task",
                            "has_startup_method": "ImplÃ©menter la mÃ©thode startup()",
                            "has_shutdown_method": "ImplÃ©menter la mÃ©thode shutdown()",
                            "has_execute_task": "ImplÃ©menter la mÃ©thode execute_task()",
                            "has_health_check": "ImplÃ©menter la mÃ©thode health_check()",
                            "has_get_capabilities": "ImplÃ©menter la mÃ©thode get_capabilities()",
                            "has_docstring": "Ajouter une docstring descriptive en dÃ©but de classe",
                            "has_logging": "Ajouter systÃ¨me de logging avec self.logger",
                            "has_error_handling": "Ajouter gestion d'erreurs avec try/except"
                        }
                        print(f"   â€¢ {recommendations.get(check_name, check_name)}")
            
            return {
                "agent_path": str(agent_path),
                "compliance_checks": compliance_checks,
                "compliance_score": compliance_score,
                "passed_checks": passed_checks,
                "total_checks": total_checks,
                "needs_architecture_update": compliance_score < 0.8
            }
            
        except Exception as e:
            print(f"âŒ Erreur validation conformitÃ©: {e}")
            return {"error": str(e)}
    
    async def run_complete_validation(self) -> Dict[str, Any]:
        """ExÃ©cute la validation complÃ¨te des deux agents"""
        print("ğŸš€ VALIDATION COMPLÃˆTE MAINTENANCE AGENTS 108 & 109")
        print("="*70)
        print(f"ğŸ¯ Validation de l'implÃ©mentation Orchestrateur + Adaptateur v4.3.0")
        print(f"ğŸ“… Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        results = {
            "validation_timestamp": datetime.now().isoformat(),
            "adaptateur_version": "4.3.0",
            "agents_tested": [],
            "overall_assessment": {}
        }
        
        agents_to_test = [
            ("agent_108_performance_optimizer.py", self.agent_108_path),
            ("agent_109_pattern_factory_version.py", self.agent_109_path)
        ]
        
        for agent_name, agent_path in agents_to_test:
            print(f"\n{'='*70}")
            print(f"ğŸ§ª VALIDATION AGENT: {agent_name}")
            print(f"{'='*70}")
            
            agent_results = {
                "agent_name": agent_name,
                "agent_path": str(agent_path),
                "tests": {}
            }
            
            # Test 1: Analyse qualitÃ©
            print(f"\n1ï¸âƒ£  TEST ANALYSE QUALITÃ‰")
            quality_analysis = await self.analyze_agent_quality(agent_path)
            agent_results["tests"]["quality_analysis"] = quality_analysis
            
            # Test 2: AmÃ©liorations adaptateur v4.3.0
            print(f"\n2ï¸âƒ£  TEST AMÃ‰LIORATIONS v4.3.0")
            improvements_test = await self.test_adaptateur_improvements(agent_path)
            agent_results["tests"]["adaptateur_improvements"] = improvements_test
            
            # Test 3: ConformitÃ© architecturale
            print(f"\n3ï¸âƒ£  TEST CONFORMITÃ‰ ARCHITECTURALE")
            compliance_check = await self.validate_agent_architecture_compliance(agent_path)
            agent_results["tests"]["architecture_compliance"] = compliance_check
            
            results["agents_tested"].append(agent_results)
        
        # Ã‰valuation globale
        print(f"\n{'='*70}")
        print(f"ğŸ“Š Ã‰VALUATION GLOBALE")
        print(f"{'='*70}")
        
        total_agents = len(results["agents_tested"])
        successful_tests = 0
        total_quality_score = 0
        total_compliance_score = 0
        
        for agent_result in results["agents_tested"]:
            tests = agent_result["tests"]
            
            # Score qualitÃ©
            quality_test = tests.get("quality_analysis", {})
            if "overall_score" in quality_test:
                total_quality_score += quality_test["overall_score"]
                
            # Score conformitÃ©
            compliance_test = tests.get("architecture_compliance", {})
            if "compliance_score" in compliance_test:
                total_compliance_score += compliance_test["compliance_score"]
            
            # Test amÃ©liorations rÃ©ussi
            improvements_test = tests.get("adaptateur_improvements", {})
            if improvements_test.get("success", False):
                successful_tests += 1
        
        avg_quality_score = total_quality_score / total_agents if total_agents > 0 else 0
        avg_compliance_score = total_compliance_score / total_agents if total_agents > 0 else 0
        success_rate = successful_tests / total_agents if total_agents > 0 else 0
        
        results["overall_assessment"] = {
            "total_agents_tested": total_agents,
            "successful_adaptateur_tests": successful_tests,
            "success_rate": success_rate,
            "average_quality_score": avg_quality_score,
            "average_compliance_score": avg_compliance_score,
            "adaptateur_v43_functional": success_rate > 0.5,
            "overall_validation_passed": avg_quality_score > 0.7 and avg_compliance_score > 0.7
        }
        
        print(f"âœ… Agents testÃ©s: {total_agents}")
        print(f"âœ… Tests adaptateur rÃ©ussis: {successful_tests}/{total_agents} ({success_rate*100:.1f}%)")
        print(f"âœ… Score qualitÃ© moyen: {avg_quality_score:.2f}")
        print(f"âœ… Score conformitÃ© moyen: {avg_compliance_score:.2f}")
        print(f"âœ… Adaptateur v4.3.0 fonctionnel: {'âœ…' if results['overall_assessment']['adaptateur_v43_functional'] else 'âŒ'}")
        print(f"âœ… Validation globale: {'âœ… SUCCÃˆS' if results['overall_assessment']['overall_validation_passed'] else 'âŒ Ã‰CHEC'}")
        
        return results

async def main():
    """Point d'entrÃ©e principal"""
    try:
        validator = ValidationMaintenanceAgents()
        results = await validator.run_complete_validation()
        
        # Sauvegarde des rÃ©sultats
        report_file = PROJECT_ROOT / f"validation_report_agents_108_109_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        try:
            import json
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ“„ Rapport sauvegardÃ©: {report_file}")
        except Exception as e:
            print(f"âš ï¸  Erreur sauvegarde rapport: {e}")
        
        # Retour du code de sortie
        overall_passed = results["overall_assessment"]["overall_validation_passed"]
        return 0 if overall_passed else 1
        
    except KeyboardInterrupt:
        print("\nâš ï¸ Validation interrompue par l'utilisateur")
        return 130
    except Exception as e:
        print(f"\nâŒ Erreur fatale: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(asyncio.run(main()))