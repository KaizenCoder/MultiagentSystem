#!/usr/bin/env python3
"""
ğŸ§ª TEST CLI - Agent 05 Moderne
Test rÃ©el d'analyse d'un autre agent du rÃ©pertoire agents/
"""

import asyncio
import sys
from pathlib import Path
import json

# Add paths
sys.path.insert(0, str(Path(__file__).parent))

async def test_cli_agent_05():
    """Test CLI de l'Agent 05 pour analyser un autre agent"""
    
    print("ğŸ§ª TEST CLI - Agent 05 MaÃ®tre Tests & Validation")
    print("=" * 60)
    print("Mission: Analyser l'agent_111_auditeur_qualite.py")
    print("")
    
    try:
        # Import agent
        from agents.modern.agent_05_maitre_tests_validation_modern_fixed import ModernAgent05MaitreTestsValidation
        from core.nextgen_architecture import Task
        
        # CrÃ©er agent
        agent = ModernAgent05MaitreTestsValidation()
        await agent.startup()
        
        print(f"âœ… Agent 05 dÃ©marrÃ© - Version: {agent.version}")
        print(f"ğŸ“‹ CapacitÃ©s: {len(agent.get_capabilities())}")
        
        # Cible d'analyse : Agent 111
        target_agent = Path(__file__).parent / "agents" / "agent_111_auditeur_qualite.py"
        if not target_agent.exists():
            print(f"âŒ Agent cible non trouvÃ©: {target_agent}")
            return False
        
        print(f"ğŸ¯ Cible d'analyse: {target_agent.name}")
        print("")
        
        # === TEST 1: ANALYSE TESTS DE L'AGENT 111 ===
        print("ğŸ“Š ANALYSE TESTS - Agent 111")
        print("-" * 40)
        
        context_analysis = {
            "target_agent": str(target_agent),
            "analysis_type": "comprehensive",
            "focus": ["quality", "testing", "validation"]
        }
        
        task_tests = Task(
            type="generer_rapport_strategique",
            params={
                "context": context_analysis,
                "type_rapport": "tests"
            }
        )
        
        result_tests = await agent.execute_task(task_tests)
        if result_tests.success:
            rapport = result_tests.data
            print(f"  ğŸ¯ Score Global: {rapport.get('score_global')}/100")
            print(f"  ğŸ† Niveau QualitÃ©: {rapport.get('niveau_qualite')}")
            print(f"  âœ… ConformitÃ©: {rapport.get('conformite')}")
            print(f"  ğŸ§ª Smoke Tests: {rapport.get('smoke_tests', {}).get('passed', 0)}/{rapport.get('smoke_tests', {}).get('total_tests', 0)} rÃ©ussis")
            
            # Afficher recommandations
            recommandations = rapport.get('recommandations', [])
            if recommandations:
                print(f"  ğŸ’¡ Recommandations ({len(recommandations)}):")
                for i, reco in enumerate(recommandations[:3], 1):
                    print(f"     {i}. {reco}")
        else:
            print(f"  âŒ Erreur: {result_tests.error}")
        
        print("")
        
        # === TEST 2: VALIDATION DE L'AGENT 111 ===
        print("âœ… VALIDATION - Agent 111")
        print("-" * 40)
        
        task_validation = Task(
            type="generer_rapport_strategique",
            params={
                "context": context_analysis,
                "type_rapport": "validation"
            }
        )
        
        result_validation = await agent.execute_task(task_validation)
        if result_validation.success:
            rapport = result_validation.data
            print(f"  ğŸ¯ Score Validation: {rapport.get('score_validation')}/100")
            
            criteres = rapport.get('criteres_validation', {})
            print(f"  ğŸ“‹ CritÃ¨res de Validation:")
            for critere, statut in criteres.items():
                icon = "âœ…" if statut == "OK" else "âš ï¸"
                print(f"     {icon} {critere}: {statut}")
        else:
            print(f"  âŒ Erreur: {result_validation.error}")
        
        print("")
        
        # === TEST 3: PERFORMANCE DE L'AGENT 111 ===
        print("âš¡ PERFORMANCE - Agent 111")
        print("-" * 40)
        
        task_performance = Task(
            type="generer_rapport_strategique",
            params={
                "context": context_analysis,
                "type_rapport": "performance"
            }
        )
        
        result_performance = await agent.execute_task(task_performance)
        if result_performance.success:
            rapport = result_performance.data
            print(f"  ğŸ¯ Score Performance: {rapport.get('score_performance')}/100")
            
            metrics = rapport.get('metriques_performance', {})
            print(f"  ğŸ’¾ MÃ©moire: {metrics.get('memory_usage_mb', 0):.1f} MB")
            print(f"  â±ï¸  Uptime: {metrics.get('uptime_seconds', 0):.1f}s")
            print(f"  ğŸ“„ Templates: {'âœ…' if metrics.get('templates_loaded') else 'âŒ'}")
        else:
            print(f"  âŒ Erreur: {result_performance.error}")
        
        print("")
        
        # === TEST 4: QUALITÃ‰ DE L'AGENT 111 ===
        print("ğŸ† QUALITÃ‰ - Agent 111") 
        print("-" * 40)
        
        task_qualite = Task(
            type="generer_rapport_strategique",
            params={
                "context": context_analysis,
                "type_rapport": "qualite"
            }
        )
        
        result_qualite = await agent.execute_task(task_qualite)
        if result_qualite.success:
            rapport = result_qualite.data
            print(f"  ğŸ¯ Score QualitÃ©: {rapport.get('score_qualite')}/100")
            
            standards = rapport.get('standards_respectes', [])
            print(f"  ğŸ“œ Standards RespectÃ©s ({len(standards)}):")
            for standard in standards[:4]:
                print(f"     âœ… {standard}")
                
            indicateurs = rapport.get('indicateurs_qualite', {})
            print(f"  ğŸ“Š Indicateurs:")
            print(f"     ğŸ§ª Tests: {indicateurs.get('tests_passes', 'N/A')}")
            print(f"     ğŸ’¯ Taux RÃ©ussite: {indicateurs.get('taux_reussite', 'N/A')}")
            print(f"     ğŸ¥ SantÃ©: {indicateurs.get('sante_systeme', 'N/A')}")
        else:
            print(f"  âŒ Erreur: {result_qualite.error}")
        
        print("")
        
        # === TEST 5: GÃ‰NÃ‰RATION RAPPORT MARKDOWN ===
        print("ğŸ“ GÃ‰NÃ‰RATION MARKDOWN")
        print("-" * 40)
        
        if result_tests.success:
            task_markdown = Task(
                type="generer_rapport_markdown",
                params={
                    "rapport_json": result_tests.data,
                    "type_rapport": "tests",
                    "context": context_analysis
                }
            )
            
            result_markdown = await agent.execute_task(task_markdown)
            if result_markdown.success:
                markdown = result_markdown.data.get("markdown", "")
                lines = len(markdown.split('\n'))
                chars = len(markdown)
                print(f"  ğŸ“„ Rapport Markdown gÃ©nÃ©rÃ©:")
                print(f"     ğŸ“ {lines} lignes, {chars} caractÃ¨res")
                print(f"     ğŸ¯ Format: {'âœ… Valide' if '# ğŸ§ª RAPPORT TESTS' in markdown else 'âŒ Invalide'}")
                
                # Sauvegarder le rapport
                report_file = Path(__file__).parent / "reports" / f"analyse_agent_111_par_agent_05.md"
                report_file.parent.mkdir(exist_ok=True)
                report_file.write_text(markdown, encoding='utf-8')
                print(f"     ğŸ’¾ SauvegardÃ©: {report_file}")
            else:
                print(f"  âŒ Erreur Markdown: {result_markdown.error}")
        
        print("")
        
        # === RÃ‰SUMÃ‰ FINAL ===
        print("=" * 60)
        print("ğŸ‰ RÃ‰SUMÃ‰ - Test CLI Agent 05")
        print("=" * 60)
        
        # Calculer score global
        scores = []
        if result_tests.success:
            scores.append(result_tests.data.get('score_global', 0))
        if result_validation.success:
            scores.append(result_validation.data.get('score_validation', 0))
        if result_performance.success:
            scores.append(result_performance.data.get('score_performance', 0))
        if result_qualite.success:
            scores.append(result_qualite.data.get('score_qualite', 0))
        
        if scores:
            score_moyen = sum(scores) / len(scores)
            print(f"ğŸ“Š Score Moyen d'Analyse: {score_moyen:.1f}/100")
            
            if score_moyen >= 90:
                print("ğŸ† EXCELLENT - Agent 111 de trÃ¨s haute qualitÃ©")
            elif score_moyen >= 70:
                print("âœ… BIEN - Agent 111 de bonne qualitÃ©")
            elif score_moyen >= 50:
                print("âš ï¸ MOYEN - Agent 111 nÃ©cessite amÃ©liorations")
            else:
                print("âŒ CRITIQUE - Agent 111 nÃ©cessite refactoring")
        
        print(f"ğŸ¯ Agent AnalysÃ©: agent_111_auditeur_qualite.py")
        print(f"ğŸ”§ Agent Analyseur: Agent 05 Moderne v{agent.version}")
        print(f"âœ… Toutes les fonctionnalitÃ©s legacy validÃ©es")
        print(f"ğŸš€ CapacitÃ©s modernes opÃ©rationnelles")
        
        await agent.shutdown()
        return True
        
    except Exception as e:
        print(f"âŒ ERREUR CRITIQUE: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = asyncio.run(test_cli_agent_05())
    print("\n" + "=" * 60)
    if success:
        print("ğŸ‰ TEST CLI AGENT 05 RÃ‰USSI")
        print("L'agent moderne fonctionne parfaitement en conditions rÃ©elles")
    else:
        print("âŒ TEST CLI AGENT 05 Ã‰CHOUÃ‰")
    print("=" * 60)
    sys.exit(0 if success else 1)