#!/usr/bin/env python3
"""
ðŸ§ª Test IntÃ©gration LLM - Agents Modernes RÃ©els
===============================================

Test fonctionnel des agents modernes avec leurs dÃ©pendances rÃ©elles
pour vÃ©rifier l'intÃ©gration LLM en conditions rÃ©elles.

Auteur: Claude Code
Date: 2025-06-28
"""

import asyncio
import sys
import os
import json
from pathlib import Path
from typing import Dict, Any, Optional

# Ajouter les chemins nÃ©cessaires
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'agents' / 'modern'))
sys.path.insert(0, str(project_root / 'core'))

async def check_environment():
    """VÃ©rifier l'environnement de test"""
    print("ðŸ” === VÃ‰RIFICATION ENVIRONNEMENT ===")
    
    # VÃ©rifier Python version
    python_version = f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"
    print(f"ðŸ Python: {python_version}")
    
    # VÃ©rifier les dÃ©pendances critiques
    dependencies = {
        'requests': False,
        'asyncio': True,  # Built-in
        'pathlib': True,  # Built-in
        'json': True      # Built-in
    }
    
    for dep_name in ['requests']:
        try:
            __import__(dep_name)
            dependencies[dep_name] = True
            print(f"âœ… {dep_name}: Disponible")
        except ImportError:
            dependencies[dep_name] = False
            print(f"âŒ {dep_name}: Manquant")
    
    # VÃ©rifier structure projet
    paths_to_check = [
        project_root / 'core',
        project_root / 'agents' / 'modern',
        project_root / 'agents' / 'modern' / 'agent_05_maitre_tests_validation_modern_fixed.py',
        project_root / 'agents' / 'modern' / 'agent_FASTAPI_23_orchestration_enterprise_modern.py'
    ]
    
    all_paths_ok = True
    for path in paths_to_check:
        if path.exists():
            print(f"âœ… {path.name}: TrouvÃ©")
        else:
            print(f"âŒ {path.name}: Manquant")
            all_paths_ok = False
    
    return all_paths_ok and all(dependencies.values())

async def test_ollama_status():
    """Test statut Ollama"""
    print("\nðŸ” === TEST OLLAMA ===")
    
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=3)
        if response.status_code == 200:
            models = response.json().get('models', [])
            print(f"âœ… Ollama actif - {len(models)} modÃ¨les")
            return True, models
        else:
            print(f"âš ï¸ Ollama rÃ©pond mais erreur: {response.status_code}")
            return False, []
    except Exception as e:
        print(f"âŒ Ollama non accessible: {str(e)[:60]}...")
        print("ðŸ’¡ Pour dÃ©marrer: ollama serve")
        return False, []

async def create_mock_llm_gateway():
    """CrÃ©er un mock LLM Gateway pour test sans Ollama"""
    
    class MockLLMGateway:
        def __init__(self):
            self.is_available = True
            self.model_name = "mock-model"
        
        async def query(self, prompt: str, **kwargs) -> Dict[str, Any]:
            """Simuler une rÃ©ponse LLM"""
            return {
                "response": f"Mock LLM Analysis: {prompt[:50]}... [Analyse simulÃ©e pour test]",
                "model": self.model_name,
                "success": True,
                "tokens": 50
            }
        
        async def process_request(self, request: Dict[str, Any]) -> Dict[str, Any]:
            """Traiter une requÃªte structurÃ©e"""
            prompt = request.get('prompt', 'No prompt')
            return await self.query(prompt)
        
        def is_healthy(self) -> bool:
            return True
    
    return MockLLMGateway()

async def test_agent_05_functionality(use_mock_llm: bool = False):
    """Test fonctionnalitÃ© Agent 05 avec/sans LLM rÃ©el"""
    print(f"\nðŸ§ª === TEST AGENT 05 {'(Mock LLM)' if use_mock_llm else ''} ===")
    
    try:
        # Essayer d'importer l'agent avec gestion des erreurs
        try:
            from agent_05_maitre_tests_validation_modern_fixed import ModernAgent05MaitreTestsValidation
        except ImportError as e:
            print(f"âŒ Import Agent 05 Ã©chouÃ©: {e}")
            return False
        
        # CrÃ©er l'agent
        agent = ModernAgent05MaitreTestsValidation()
        print("âœ… Agent 05 crÃ©Ã©")
        
        # Injecter mock LLM si nÃ©cessaire
        if use_mock_llm:
            mock_llm = await create_mock_llm_gateway()
            agent.llm_gateway = mock_llm
            print("ðŸ”§ Mock LLM injectÃ©")
        
        # Test donnÃ©es rÃ©alistes
        test_context = {
            "project": "NextGeneration LLM Test",
            "branch": "feature/llm-integration",
            "environment": "development",
            "timestamp": "2025-06-28T21:45:00Z"
        }
        
        test_metriques = {
            "tests_executes": 245,
            "tests_reussis": 238,
            "tests_echoues": 7,
            "couverture_code": 0.89,
            "duree_execution": 156.8,
            "memoire_utilisee": 384,
            "erreurs_critiques": 1,
            "warnings": 12,
            "performance_score": 0.94
        }
        
        print("ðŸ“Š GÃ©nÃ©ration rapport de tests...")
        
        # Test de la mÃ©thode principale
        try:
            rapport = await agent.generer_rapport_strategique(
                context=test_context,
                type_rapport="tests", 
                metriques=test_metriques
            )
            
            if rapport and isinstance(rapport, dict):
                print("âœ… Rapport gÃ©nÃ©rÃ© avec succÃ¨s")
                
                # Analyser le contenu
                if 'contenu' in rapport:
                    contenu = rapport['contenu']
                    if isinstance(contenu, dict):
                        sections = list(contenu.keys())
                        print(f"   ðŸ“‹ Sections: {', '.join(sections[:3])}...")
                        
                        # Chercher des indicateurs LLM
                        content_str = json.dumps(contenu).lower()
                        llm_indicators = [
                            'analyse' in content_str,
                            'recommandation' in content_str,
                            'insight' in content_str,
                            'optimisation' in content_str
                        ]
                        
                        if any(llm_indicators):
                            print("ðŸ¤– Contenu enrichi par LLM dÃ©tectÃ©")
                        else:
                            print("ðŸ“ Contenu standard (fallback mode)")
                    
                    print(f"   ðŸ“ Taille rapport: {len(str(contenu))} caractÃ¨res")
                
                # VÃ©rifier la qualitÃ©
                if 'metadata' in rapport:
                    metadata = rapport['metadata']
                    if 'llm_enhanced' in metadata:
                        print(f"   ðŸ”¬ LLM utilisÃ©: {metadata['llm_enhanced']}")
                
                return True
            else:
                print("âŒ Rapport invalide ou vide")
                return False
                
        except Exception as e:
            print(f"ðŸ’¥ Erreur gÃ©nÃ©ration rapport: {e}")
            return False
        
    except Exception as e:
        print(f"ðŸ’¥ Erreur test Agent 05: {e}")
        return False

async def test_agent_fastapi_functionality(use_mock_llm: bool = False):
    """Test fonctionnalitÃ© Agent FASTAPI avec/sans LLM rÃ©el"""
    print(f"\nðŸš€ === TEST AGENT FASTAPI {'(Mock LLM)' if use_mock_llm else ''} ===")
    
    try:
        # Import de l'agent
        try:
            from agent_FASTAPI_23_orchestration_enterprise_modern import ModernAgent23FastAPIOrchestrationEnterprise
        except ImportError as e:
            print(f"âŒ Import Agent FASTAPI Ã©chouÃ©: {e}")
            return False
        
        # CrÃ©er l'agent
        agent = ModernAgent23FastAPIOrchestrationEnterprise()
        print("âœ… Agent FASTAPI crÃ©Ã©")
        
        # Injecter mock LLM si nÃ©cessaire
        if use_mock_llm:
            mock_llm = await create_mock_llm_gateway()
            agent.llm_gateway = mock_llm
            print("ðŸ”§ Mock LLM injectÃ©")
        
        # Test donnÃ©es API rÃ©alistes
        test_input = {
            "api_specification": {
                "name": "User Management API",
                "version": "2.0",
                "base_path": "/api/v2"
            },
            "endpoints": [
                {
                    "path": "/users",
                    "method": "GET",
                    "description": "List all users with pagination",
                    "parameters": ["page", "limit", "sort"]
                },
                {
                    "path": "/users/{id}",
                    "method": "GET", 
                    "description": "Get user by ID",
                    "parameters": ["id"]
                }
            ],
            "models": {
                "User": {
                    "id": "integer",
                    "username": "string", 
                    "email": "string",
                    "created_at": "datetime",
                    "is_active": "boolean"
                }
            },
            "security": {
                "authentication": "Bearer Token",
                "rate_limiting": "100 req/min",
                "cors_enabled": True
            }
        }
        
        print("ðŸ”§ Orchestration API...")
        
        try:
            result = await agent.execute_task(test_input)
            
            if result and isinstance(result, dict):
                print("âœ… Orchestration rÃ©ussie")
                
                # Analyser le rÃ©sultat
                if result.get('success', False):
                    print("   âœ… SuccÃ¨s confirmÃ©")
                else:
                    print("   âš ï¸ SuccÃ¨s partiel")
                
                if 'outputs' in result:
                    outputs = result['outputs']
                    output_keys = list(outputs.keys())
                    print(f"   ðŸ“¦ Outputs: {', '.join(output_keys[:3])}...")
                    
                    # VÃ©rifier gÃ©nÃ©ration de code
                    if 'generated_code' in outputs:
                        code = outputs['generated_code']
                        print(f"   ðŸ’» Code gÃ©nÃ©rÃ©: {len(str(code))} caractÃ¨res")
                    
                    # VÃ©rifier documentation
                    if 'documentation' in outputs:
                        docs = outputs['documentation']
                        print(f"   ðŸ“š Documentation: {len(str(docs))} caractÃ¨res")
                
                return True
            else:
                print("âŒ RÃ©sultat invalide")
                return False
                
        except Exception as e:
            print(f"ðŸ’¥ Erreur orchestration: {e}")
            return False
        
    except Exception as e:
        print(f"ðŸ’¥ Erreur test Agent FASTAPI: {e}")
        return False

async def main():
    """Point d'entrÃ©e principal"""
    print("ðŸ§ª TEST INTÃ‰GRATION LLM - AGENTS MODERNES RÃ‰ELS")
    print("=" * 55)
    print()
    
    # RÃ©sultats
    results = {
        'environment_ok': False,
        'ollama_available': False,
        'agent_05_real': False,
        'agent_05_mock': False,
        'agent_fastapi_real': False,
        'agent_fastapi_mock': False
    }
    
    try:
        # 1. VÃ©rifier environnement
        results['environment_ok'] = await check_environment()
        if not results['environment_ok']:
            print("âŒ Environnement incomplet - arrÃªt des tests")
            return False
        
        # 2. Test Ollama
        ollama_ok, models = await test_ollama_status()
        results['ollama_available'] = ollama_ok
        
        # 3. Tests Agent 05
        if ollama_ok:
            print("\nðŸ”„ Test avec LLM rÃ©el...")
            results['agent_05_real'] = await test_agent_05_functionality(use_mock_llm=False)
        
        print("\nðŸ”„ Test avec Mock LLM...")
        results['agent_05_mock'] = await test_agent_05_functionality(use_mock_llm=True)
        
        # 4. Tests Agent FASTAPI
        if ollama_ok:
            print("\nðŸ”„ Test FASTAPI avec LLM rÃ©el...")
            results['agent_fastapi_real'] = await test_agent_fastapi_functionality(use_mock_llm=False)
        
        print("\nðŸ”„ Test FASTAPI avec Mock LLM...")
        results['agent_fastapi_mock'] = await test_agent_fastapi_functionality(use_mock_llm=True)
        
        # 5. Rapport final
        print("\nðŸ“Š === RAPPORT FINAL ===")
        
        successes = sum(results.values())
        total_tests = len(results)
        success_rate = (successes / total_tests) * 100
        
        print(f"ðŸŽ¯ Tests rÃ©ussis: {successes}/{total_tests} ({success_rate:.1f}%)")
        
        # DÃ©tail par catÃ©gorie
        print("\nðŸ” DÃ©tail des rÃ©sultats:")
        for test_name, success in results.items():
            status = "âœ…" if success else "âŒ"
            print(f"   {status} {test_name.replace('_', ' ').title()}")
        
        # Analyse et recommandations
        print("\nðŸ’¡ === ANALYSE ===")
        
        if results['environment_ok']:
            print("âœ… Environnement: OK")
        else:
            print("âŒ Environnement: ProblÃ¨me dÃ©pendances")
        
        if results['ollama_available']:
            print("âœ… Infrastructure LLM: Ollama opÃ©rationnel")
        else:
            print("âš ï¸ Infrastructure LLM: Ollama non disponible")
            print("   ðŸ’¡ DÃ©marrer avec: ollama serve")
        
        # Tests agents
        agent_05_ok = results['agent_05_real'] or results['agent_05_mock']
        agent_fastapi_ok = results['agent_fastapi_real'] or results['agent_fastapi_mock']
        
        if agent_05_ok:
            print("âœ… Agent 05: IntÃ©gration LLM fonctionnelle")
        else:
            print("âŒ Agent 05: ProblÃ¨me intÃ©gration LLM")
        
        if agent_fastapi_ok:
            print("âœ… Agent FASTAPI: IntÃ©gration LLM fonctionnelle")
        else:
            print("âŒ Agent FASTAPI: ProblÃ¨me intÃ©gration LLM")
        
        # Conclusion
        if agent_05_ok and agent_fastapi_ok:
            print("\nðŸŽ‰ SUCCÃˆS: IntÃ©gration LLM validÃ©e dans les agents modernes!")
            print("   Les agents peuvent fonctionner avec et sans LLM")
        elif results['agent_05_mock'] or results['agent_fastapi_mock']:
            print("\nâš ï¸ PARTIEL: Agents fonctionnent en mode fallback")
            print("   IntÃ©gration LLM prÃ©sente mais nÃ©cessite Ollama pour le mode complet")
        else:
            print("\nâŒ Ã‰CHEC: ProblÃ¨mes dans l'intÃ©gration LLM")
        
        return success_rate >= 50  # Au moins 50% de succÃ¨s
        
    except Exception as e:
        print(f"\nðŸ’¥ Erreur durant les tests: {e}")
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)