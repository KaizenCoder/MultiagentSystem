#!/usr/bin/env python3
"""
üß™ Test Int√©gration LLM - Agents Modernes R√©els
===============================================

Test fonctionnel des agents modernes avec leurs d√©pendances r√©elles
pour v√©rifier l'int√©gration LLM en conditions r√©elles.

Auteur: Claude Code
Date: 2025-06-28
"""

import asyncio
import sys
import os
import json
from pathlib import Path
from typing import Dict, Any, Optional

# Ajouter les chemins n√©cessaires
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'agents' / 'modern'))
sys.path.insert(0, str(project_root / 'core'))

async def check_environment():
    """V√©rifier l'environnement de test"""
    print("üîç === V√âRIFICATION ENVIRONNEMENT ===")
    
    # V√©rifier Python version
    python_version = f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"
    print(f"üêç Python: {python_version}")
    
    # V√©rifier les d√©pendances critiques
    dependencies = {
        'requests': False,
        'asyncio': True,  # Built-in
        'pathlib': True,  # Built-in
        'json': True      # Built-in
    }
    
    for dep_name in ['requests']:
        try:
            __import__(dep_name)
            dependencies[dep_name] = True
            print(f"‚úÖ {dep_name}: Disponible")
        except ImportError:
            dependencies[dep_name] = False
            print(f"‚ùå {dep_name}: Manquant")
    
    # V√©rifier structure projet
    paths_to_check = [
        project_root / 'core',
        project_root / 'agents' / 'modern',
        project_root / 'agents' / 'modern' / 'agent_05_maitre_tests_validation_modern_fixed.py',
        project_root / 'agents' / 'modern' / 'agent_FASTAPI_23_orchestration_enterprise_modern.py'
    ]
    
    all_paths_ok = True
    for path in paths_to_check:
        if path.exists():
            print(f"‚úÖ {path.name}: Trouv√©")
        else:
            print(f"‚ùå {path.name}: Manquant")
            all_paths_ok = False
    
    return all_paths_ok and all(dependencies.values())

async def test_ollama_status():
    """Test statut Ollama"""
    print("\nüîç === TEST OLLAMA ===")
    
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=3)
        if response.status_code == 200:
            models = response.json().get('models', [])
            print(f"‚úÖ Ollama actif - {len(models)} mod√®les")
            return True, models
        else:
            print(f"‚ö†Ô∏è Ollama r√©pond mais erreur: {response.status_code}")
            return False, []
    except Exception as e:
        print(f"‚ùå Ollama non accessible: {str(e)[:60]}...")
        print("üí° Pour d√©marrer: ollama serve")
        return False, []

async def create_mock_llm_gateway():
    """Cr√©er un mock LLM Gateway pour test sans Ollama"""
    
    class MockLLMGateway:
        def __init__(self):
            self.is_available = True
            self.model_name = "mock-model"
        
        async def query(self, prompt: str, **kwargs) -> Dict[str, Any]:
            """Simuler une r√©ponse LLM"""
            return {
                "response": f"Mock LLM Analysis: {prompt[:50]}... [Analyse simul√©e pour test]",
                "model": self.model_name,
                "success": True,
                "tokens": 50
            }
        
        async def process_request(self, request: Dict[str, Any]) -> Dict[str, Any]:
            """Traiter une requ√™te structur√©e"""
            prompt = request.get('prompt', 'No prompt')
            return await self.query(prompt)
        
        def is_healthy(self) -> bool:
            return True
    
    return MockLLMGateway()

async def test_agent_05_functionality(use_mock_llm: bool = False):
    """Test fonctionnalit√© Agent 05 avec/sans LLM r√©el"""
    print(f"\nüß™ === TEST AGENT 05 {'(Mock LLM)' if use_mock_llm else ''} ===")
    
    try:
        # Essayer d'importer l'agent avec gestion des erreurs
        try:
            from agent_05_maitre_tests_validation_modern_fixed import ModernAgent05MaitreTestsValidation
        except ImportError as e:
            print(f"‚ùå Import Agent 05 √©chou√©: {e}")
            return False
        
        # Cr√©er l'agent
        agent = ModernAgent05MaitreTestsValidation()
        print("‚úÖ Agent 05 cr√©√©")
        
        # Injecter mock LLM si n√©cessaire
        if use_mock_llm:
            mock_llm = await create_mock_llm_gateway()
            agent.llm_gateway = mock_llm
            print("üîß Mock LLM inject√©")
        
        # Test donn√©es r√©alistes
        test_context = {
            "project": "NextGeneration LLM Test",
            "branch": "feature/llm-integration",
            "environment": "development",
            "timestamp": "2025-06-28T21:45:00Z"
        }
        
        test_metriques = {
            "tests_executes": 245,
            "tests_reussis": 238,
            "tests_echoues": 7,
            "couverture_code": 0.89,
            "duree_execution": 156.8,
            "memoire_utilisee": 384,
            "erreurs_critiques": 1,
            "warnings": 12,
            "performance_score": 0.94
        }
        
        print("üìä G√©n√©ration rapport de tests...")
        
        # Test de la m√©thode principale
        try:
            rapport = await agent.generer_rapport_strategique(
                context=test_context,
                type_rapport="tests", 
                metriques=test_metriques
            )
            
            if rapport and isinstance(rapport, dict):
                print("‚úÖ Rapport g√©n√©r√© avec succ√®s")
                
                # Analyser le contenu
                if 'contenu' in rapport:
                    contenu = rapport['contenu']
                    if isinstance(contenu, dict):
                        sections = list(contenu.keys())
                        print(f"   üìã Sections: {', '.join(sections[:3])}...")
                        
                        # Chercher des indicateurs LLM
                        content_str = json.dumps(contenu).lower()
                        llm_indicators = [
                            'analyse' in content_str,
                            'recommandation' in content_str,
                            'insight' in content_str,
                            'optimisation' in content_str
                        ]
                        
                        if any(llm_indicators):
                            print("ü§ñ Contenu enrichi par LLM d√©tect√©")
                        else:
                            print("üìù Contenu standard (fallback mode)")
                    
                    print(f"   üìè Taille rapport: {len(str(contenu))} caract√®res")
                
                # V√©rifier la qualit√©
                if 'metadata' in rapport:
                    metadata = rapport['metadata']
                    if 'llm_enhanced' in metadata:
                        print(f"   üî¨ LLM utilis√©: {metadata['llm_enhanced']}")
                
                return True
            else:
                print("‚ùå Rapport invalide ou vide")
                return False
                
        except Exception as e:
            print(f"üí• Erreur g√©n√©ration rapport: {e}")
            return False
        
    except Exception as e:
        print(f"üí• Erreur test Agent 05: {e}")
        return False

async def test_agent_fastapi_functionality(use_mock_llm: bool = False):
    """Test fonctionnalit√© Agent FASTAPI avec/sans LLM r√©el"""
    print(f"\nüöÄ === TEST AGENT FASTAPI {'(Mock LLM)' if use_mock_llm else ''} ===")
    
    try:
        # Import de l'agent
        try:
            from agent_FASTAPI_23_orchestration_enterprise_modern import ModernAgent23FastAPIOrchestrationEnterprise
        except ImportError as e:
            print(f"‚ùå Import Agent FASTAPI √©chou√©: {e}")
            return False
        
        # Cr√©er l'agent
        agent = ModernAgent23FastAPIOrchestrationEnterprise()
        print("‚úÖ Agent FASTAPI cr√©√©")
        
        # Injecter mock LLM si n√©cessaire
        if use_mock_llm:
            mock_llm = await create_mock_llm_gateway()
            agent.llm_gateway = mock_llm
            print("üîß Mock LLM inject√©")
        
        # Test donn√©es API r√©alistes
        test_input = {
            "api_specification": {
                "name": "User Management API",
                "version": "2.0",
                "base_path": "/api/v2"
            },
            "endpoints": [
                {
                    "path": "/users",
                    "method": "GET",
                    "description": "List all users with pagination",
                    "parameters": ["page", "limit", "sort"]
                },
                {
                    "path": "/users/{id}",
                    "method": "GET", 
                    "description": "Get user by ID",
                    "parameters": ["id"]
                }
            ],
            "models": {
                "User": {
                    "id": "integer",
                    "username": "string", 
                    "email": "string",
                    "created_at": "datetime",
                    "is_active": "boolean"
                }
            },
            "security": {
                "authentication": "Bearer Token",
                "rate_limiting": "100 req/min",
                "cors_enabled": True
            }
        }
        
        print("üîß Orchestration API...")
        
        try:
            result = await agent.execute_task(test_input)
            
            if result and isinstance(result, dict):
                print("‚úÖ Orchestration r√©ussie")
                
                # Analyser le r√©sultat
                if result.get('success', False):
                    print("   ‚úÖ Succ√®s confirm√©")
                else:
                    print("   ‚ö†Ô∏è Succ√®s partiel")
                
                if 'outputs' in result:
                    outputs = result['outputs']
                    output_keys = list(outputs.keys())
                    print(f"   üì¶ Outputs: {', '.join(output_keys[:3])}...")
                    
                    # V√©rifier g√©n√©ration de code
                    if 'generated_code' in outputs:
                        code = outputs['generated_code']
                        print(f"   üíª Code g√©n√©r√©: {len(str(code))} caract√®res")
                    
                    # V√©rifier documentation
                    if 'documentation' in outputs:
                        docs = outputs['documentation']
                        print(f"   üìö Documentation: {len(str(docs))} caract√®res")
                
                return True
            else:
                print("‚ùå R√©sultat invalide")
                return False
                
        except Exception as e:
            print(f"üí• Erreur orchestration: {e}")
            return False
        
    except Exception as e:
        print(f"üí• Erreur test Agent FASTAPI: {e}")
        return False

async def main():
    """Point d'entr√©e principal"""
    print("üß™ TEST INT√âGRATION LLM - AGENTS MODERNES R√âELS")
    print("=" * 55)
    print()
    
    # R√©sultats
    results = {
        'environment_ok': False,
        'ollama_available': False,
        'agent_05_real': False,
        'agent_05_mock': False,
        'agent_fastapi_real': False,
        'agent_fastapi_mock': False
    }
    
    try:
        # 1. V√©rifier environnement
        results['environment_ok'] = await check_environment()
        if not results['environment_ok']:
            print("‚ùå Environnement incomplet - arr√™t des tests")
            return False
        
        # 2. Test Ollama
        ollama_ok, models = await test_ollama_status()
        results['ollama_available'] = ollama_ok
        
        # 3. Tests Agent 05
        if ollama_ok:
            print("\nüîÑ Test avec LLM r√©el...")
            results['agent_05_real'] = await test_agent_05_functionality(use_mock_llm=False)
        
        print("\nüîÑ Test avec Mock LLM...")
        results['agent_05_mock'] = await test_agent_05_functionality(use_mock_llm=True)
        
        # 4. Tests Agent FASTAPI
        if ollama_ok:
            print("\nüîÑ Test FASTAPI avec LLM r√©el...")
            results['agent_fastapi_real'] = await test_agent_fastapi_functionality(use_mock_llm=False)
        
        print("\nüîÑ Test FASTAPI avec Mock LLM...")
        results['agent_fastapi_mock'] = await test_agent_fastapi_functionality(use_mock_llm=True)
        
        # 5. Rapport final
        print("\nüìä === RAPPORT FINAL ===")
        
        successes = sum(results.values())
        total_tests = len(results)
        success_rate = (successes / total_tests) * 100
        
        print(f"üéØ Tests r√©ussis: {successes}/{total_tests} ({success_rate:.1f}%)")
        
        # D√©tail par cat√©gorie
        print("\nüîç D√©tail des r√©sultats:")
        for test_name, success in results.items():
            status = "‚úÖ" if success else "‚ùå"
            print(f"   {status} {test_name.replace('_', ' ').title()}")
        
        # Analyse et recommandations
        print("\nüí° === ANALYSE ===")
        
        if results['environment_ok']:
            print("‚úÖ Environnement: OK")
        else:
            print("‚ùå Environnement: Probl√®me d√©pendances")
        
        if results['ollama_available']:
            print("‚úÖ Infrastructure LLM: Ollama op√©rationnel")
        else:
            print("‚ö†Ô∏è Infrastructure LLM: Ollama non disponible")
            print("   üí° D√©marrer avec: ollama serve")
        
        # Tests agents
        agent_05_ok = results['agent_05_real'] or results['agent_05_mock']
        agent_fastapi_ok = results['agent_fastapi_real'] or results['agent_fastapi_mock']
        
        if agent_05_ok:
            print("‚úÖ Agent 05: Int√©gration LLM fonctionnelle")
        else:
            print("‚ùå Agent 05: Probl√®me int√©gration LLM")
        
        if agent_fastapi_ok:
            print("‚úÖ Agent FASTAPI: Int√©gration LLM fonctionnelle")
        else:
            print("‚ùå Agent FASTAPI: Probl√®me int√©gration LLM")
        
        # Conclusion
        if agent_05_ok and agent_fastapi_ok:
            print("\nüéâ SUCC√àS: Int√©gration LLM valid√©e dans les agents modernes!")
            print("   Les agents peuvent fonctionner avec et sans LLM")
        elif results['agent_05_mock'] or results['agent_fastapi_mock']:
            print("\n‚ö†Ô∏è PARTIEL: Agents fonctionnent en mode fallback")
            print("   Int√©gration LLM pr√©sente mais n√©cessite Ollama pour le mode complet")
        else:
            print("\n‚ùå √âCHEC: Probl√®mes dans l'int√©gration LLM")
        
        return success_rate >= 50  # Au moins 50% de succ√®s
        
    except Exception as e:
        print(f"\nüí• Erreur durant les tests: {e}")
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)