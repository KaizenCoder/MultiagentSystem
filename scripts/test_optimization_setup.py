#!/usr/bin/env python3
"""
Script de Test - Setup Optimisations √âquipe Maintenance
NextGeneration - Validation Infrastructure

Ce script teste et valide tous les composants d'optimisation :
- Configuration centralis√©e
- Collecteur de m√©triques
- Circuit breaker
- Cache intelligent
- Int√©gration compl√®te
"""

import asyncio
import time
import random
from pathlib import Path

# Configuration du logging
import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

def test_configuration():
    """Test de la configuration centralis√©e"""
    print("üîß Test Configuration Centralis√©e")
    print("-" * 50)
    
    try:
        from core.monitoring.config_manager import MaintenanceConfig
        
        # Chargement de la configuration
        config = MaintenanceConfig.from_yaml('config/maintenance_optimization_config.yaml')
        print(f"‚úÖ Configuration charg√©e: {len(config.team_config)} agents")
        
        # Validation
        validation = config.validate_configuration()
        print(f"‚úÖ Validation: {len(validation['errors'])} erreurs, {len(validation['warnings'])} avertissements")
        
        # Test d'acc√®s aux configurations
        chef_config = config.get_agent_config("chef_equipe")
        print(f"‚úÖ Config Chef d'√âquipe: parall√©lisme={chef_config.parallel_processing}, s√©maphore={chef_config.semaphore_limit}")
        
        # Configuration globale
        global_config = config.global_settings
        print(f"‚úÖ Config Globale: cache_ttl={global_config.cache_ttl}, monitoring={global_config.performance_monitoring}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur configuration: {e}")
        return False

async def test_metrics_collector():
    """Test du collecteur de m√©triques"""
    print("\nüìä Test Collecteur de M√©triques")
    print("-" * 50)
    
    try:
        from core.monitoring.metrics_collector import AdvancedMetricsCollector, monitor_performance
        
        # Cr√©ation du collecteur
        collector = AdvancedMetricsCollector()
        print("‚úÖ Collecteur cr√©√©")
        
        # Simulation d'ex√©cutions
        for i in range(10):
            agent_id = f"agent_test_{random.randint(1, 3)}"
            duration = random.uniform(0.5, 3.0)
            success = random.choice([True, True, True, False])  # 75% succ√®s
            memory = random.randint(50, 150) * 1024 * 1024  # 50-150MB
            
            collector.record_execution(agent_id, duration, success, memory, f"task_{i}")
        
        print("‚úÖ 10 ex√©cutions simul√©es enregistr√©es")
        
        # Dashboard
        dashboard = collector.get_dashboard_data()
        print(f"‚úÖ Dashboard: {dashboard['global_metrics']['total_executions']} ex√©cutions")
        print(f"   Taux de succ√®s: {dashboard['global_metrics']['global_success_rate']}")
        print(f"   Agents actifs: {dashboard['global_metrics']['active_agents']}")
        
        # Test du d√©corateur
        @monitor_performance(collector)
        async def test_function(self):
            await asyncio.sleep(0.1)
            return {"result": "success"}
        
        # Simulation d'une classe agent
        class TestAgent:
            def __init__(self):
                self.agent_id = "test_agent_decorator"
        
        test_agent = TestAgent()
        result = await test_function(test_agent)
        print("‚úÖ D√©corateur @monitor_performance test√©")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur m√©triques: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_circuit_breaker():
    """Test du circuit breaker"""
    print("\nüîÑ Test Circuit Breaker")
    print("-" * 50)
    
    try:
        from core.monitoring.circuit_breaker import CircuitBreaker, CircuitBreakerOpenException
        
        # Cr√©ation du circuit breaker
        cb = CircuitBreaker(failure_threshold=3, timeout_seconds=2)
        print("‚úÖ Circuit breaker cr√©√©")
        
        # Fonction de test qui √©choue parfois
        async def test_function():
            if random.random() < 0.6:  # 60% d'√©chec
                raise Exception("Erreur simul√©e")
            return "Succ√®s"
        
        # Test avec √©checs
        success_count = 0
        failure_count = 0
        circuit_open_count = 0
        
        for i in range(15):
            try:
                result = await cb.call(test_function)
                success_count += 1
                print(f"   ‚úÖ Appel {i+1}: {result}")
            except CircuitBreakerOpenException:
                circuit_open_count += 1
                print(f"   üö® Appel {i+1}: Circuit ouvert")
            except Exception:
                failure_count += 1
                print(f"   ‚ùå Appel {i+1}: √âchec")
            
            await asyncio.sleep(0.1)
        
        # M√©triques
        metrics = cb.get_metrics()
        print(f"‚úÖ M√©triques: {success_count} succ√®s, {failure_count} √©checs, {circuit_open_count} circuit ouvert")
        print(f"   Disponibilit√©: {metrics['availability']:.1f}%")
        print(f"   Score fiabilit√©: {metrics['reliability_score']}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur circuit breaker: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_cache():
    """Test du cache intelligent"""
    print("\nüíæ Test Cache Intelligent")
    print("-" * 50)
    
    try:
        from core.monitoring.cache_manager import IntelligentCache, cached
        
        # Cr√©ation du cache (mode m√©moire pour les tests)
        cache = IntelligentCache(enable_redis=False, max_memory_items=100)
        print("‚úÖ Cache cr√©√© (mode m√©moire)")
        
        # Test basique
        await cache.set("test_key", {"data": "test_value", "timestamp": time.time()})
        result = await cache.get("test_key")
        print(f"‚úÖ Test basique: {result is not None}")
        
        # Test avec namespace
        await cache.set("user:123", {"name": "John", "role": "tester"}, namespace="users")
        user = await cache.get("user:123", namespace="users")
        print(f"‚úÖ Test namespace: {user['name'] if user else 'None'}")
        
        # Simulation de charge pour tester le hit rate
        for i in range(50):
            await cache.set(f"key_{i}", f"value_{i}")
            
            # Quelques r√©cup√©rations pour g√©n√©rer des hits
            if i % 5 == 0 and i > 0:
                await cache.get(f"key_{i-2}")
                await cache.get(f"key_{i-1}")
        
        print("‚úÖ 50 op√©rations de cache simul√©es")
        
        # Statistiques
        stats = cache.get_stats()
        print(f"   Hit rate: {stats['hit_rate']}")
        print(f"   Items en m√©moire: {stats['memory_items']}")
        print(f"   Temps d'acc√®s moyen: {stats['avg_access_time_ms']:.2f}ms")
        
        # Test du d√©corateur @cached
        @cached(ttl=60, namespace="functions")
        async def expensive_function(param):
            await asyncio.sleep(0.1)  # Simulation calcul co√ªteux
            return f"result_for_{param}"
        
        # Premier appel (miss)
        start = time.time()
        result1 = await expensive_function("test")
        time1 = time.time() - start
        
        # Deuxi√®me appel (hit)
        start = time.time()
        result2 = await expensive_function("test")
        time2 = time.time() - start
        
        speedup = time1/time2 if time2 > 0 else float('inf')
        print(f"‚úÖ D√©corateur @cached: {time1:.3f}s vs {time2:.3f}s (speedup: {speedup:.1f}x)")
        
        # √âtat de sant√©
        health = cache.get_health_status()
        print(f"‚úÖ Sant√© du cache: {health['health_status']} (Score: {health['health_score']})")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur cache: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_integration():
    """Test d'int√©gration compl√®te"""
    print("\nüöÄ Test d'Int√©gration Compl√®te")
    print("-" * 50)
    
    try:
        from core.monitoring.config_manager import MaintenanceConfig
        from core.monitoring.metrics_collector import AdvancedMetricsCollector
        from core.monitoring.circuit_breaker import CircuitBreaker
        from core.monitoring.cache_manager import IntelligentCache
        
        # Chargement de la configuration
        config = MaintenanceConfig.from_yaml('config/maintenance_optimization_config.yaml')
        
        # Initialisation des composants
        metrics = AdvancedMetricsCollector()
        cache = IntelligentCache(enable_redis=False)
        
        # Configuration des circuit breakers par agent
        circuit_breakers = {}
        for agent_name, agent_config in config.team_config.items():
            circuit_breakers[agent_name] = CircuitBreaker(
                failure_threshold=agent_config.circuit_breaker_threshold,
                timeout_seconds=agent_config.timeout_seconds
            )
        
        print(f"‚úÖ Int√©gration: {len(circuit_breakers)} circuit breakers configur√©s")
        
        # Simulation d'un workflow d'agent
        async def simulate_agent_workflow(agent_name: str):
            """Simule le workflow d'un agent avec tous les composants"""
            
            # R√©cup√©ration de la configuration
            agent_config = config.get_agent_config(agent_name)
            cb = circuit_breakers[agent_name]
            
            # Simulation de t√¢che avec cache
            cache_key = f"agent_task_{agent_name}_{time.time()}"
            
            # V√©rification cache
            cached_result = await cache.get(cache_key, namespace="agent_tasks")
            if cached_result:
                metrics.record_execution(agent_name, 0.01, True, 1024*1024, "cached_task")
                return cached_result
            
            # Ex√©cution avec circuit breaker
            async def agent_task():
                # Simulation de travail
                await asyncio.sleep(random.uniform(0.1, 0.5))
                
                # Simulation d'√©chec parfois
                if random.random() < 0.2:  # 20% d'√©chec
                    raise Exception(f"Erreur simul√©e dans {agent_name}")
                
                return {"status": "success", "agent": agent_name, "timestamp": time.time()}
            
            try:
                start_time = time.time()
                result = await cb.call(agent_task)
                duration = time.time() - start_time
                
                # Mise en cache du r√©sultat
                await cache.set(cache_key, result, ttl=300, namespace="agent_tasks")
                
                # Enregistrement des m√©triques
                metrics.record_execution(agent_name, duration, True, random.randint(1, 5)*1024*1024, "workflow_task")
                
                return result
                
            except Exception as e:
                duration = time.time() - start_time
                metrics.record_execution(agent_name, duration, False, random.randint(1, 3)*1024*1024, "workflow_task")
                raise
        
        # Simulation de plusieurs agents en parall√®le
        agent_names = list(config.team_config.keys())[:5]  # Test avec 5 agents
        tasks = [simulate_agent_workflow(name) for name in agent_names]
        
        print(f"üöÄ Simulation de {len(tasks)} agents en parall√®le...")
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Analyse des r√©sultats
        successful = sum(1 for r in results if not isinstance(r, Exception))
        failed = len(results) - successful
        
        print(f"‚úÖ R√©sultats: {successful} succ√®s, {failed} √©checs")
        
        # Dashboard global
        dashboard = metrics.get_dashboard_data()
        print(f"‚úÖ M√©triques globales:")
        print(f"   Total ex√©cutions: {dashboard['global_metrics']['total_executions']}")
        print(f"   Taux de succ√®s: {dashboard['global_metrics']['global_success_rate']}")
        print(f"   Agents actifs: {dashboard['global_metrics']['active_agents']}")
        
        # Statistiques de cache
        cache_stats = cache.get_stats()
        print(f"‚úÖ Cache: {cache_stats['hit_rate']} hit rate, {cache_stats['memory_items']} items")
        
        # √âtats des circuit breakers
        healthy_circuits = sum(1 for cb in circuit_breakers.values() if cb.get_state()['state'] == 'closed')
        print(f"‚úÖ Circuit breakers: {healthy_circuits}/{len(circuit_breakers)} en √©tat sain")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur int√©gration: {e}")
        import traceback
        traceback.print_exc()
        return False

async def run_all_tests():
    """Ex√©cute tous les tests"""
    print("üß™ TESTS D'OPTIMISATION - √âQUIPE MAINTENANCE NEXTGENERATION")
    print("=" * 70)
    
    results = []
    
    # Tests individuels
    results.append(("Configuration", test_configuration()))
    results.append(("M√©triques", await test_metrics_collector()))
    results.append(("Circuit Breaker", await test_circuit_breaker()))
    results.append(("Cache", await test_cache()))
    results.append(("Int√©gration", await test_integration()))
    
    # R√©sum√©
    print("\n" + "=" * 70)
    print("üìã R√âSUM√â DES TESTS")
    print("=" * 70)
    
    passed = 0
    total = len(results)
    
    for test_name, result in results:
        status = "‚úÖ PASS√â" if result else "‚ùå √âCHEC"
        print(f"{status:10} | {test_name}")
        if result:
            passed += 1
    
    print("-" * 70)
    print(f"TOTAL: {passed}/{total} tests r√©ussis ({passed/total*100:.1f}%)")
    
    if passed == total:
        print("\nüéâ TOUS LES TESTS SONT PASS√âS!")
        print("üöÄ Infrastructure d'optimisation pr√™te pour l'impl√©mentation!")
        print("\nüìã Prochaines √©tapes:")
        print("  1. Impl√©mentation du Chef d'√âquipe parall√®le")
        print("  2. Optimisation de l'Adaptateur Code avec cache")
        print("  3. Tests de performance en conditions r√©elles")
        print("  4. Monitoring et alertes en production")
    else:
        print(f"\n‚ö†Ô∏è {total-passed} test(s) ont √©chou√© - Correction n√©cessaire avant impl√©mentation")
    
    return passed == total

if __name__ == "__main__":
    # Ex√©cution des tests
    success = asyncio.run(run_all_tests())
    exit(0 if success else 1) 