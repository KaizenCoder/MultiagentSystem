#!/usr/bin/env python3
"""
üöß DRAFT VERSION üöß
Agent M√©ta-Strat√©gique pour l'Auto-Am√©lioration - VERSION DRAFT/PROTOTYPE
Mission: Analyser les performances du syst√®me et proposer des strat√©gies d'optimisation

‚ö†Ô∏è ATTENTION: CETTE VERSION EST UN PROTOTYPE EN D√âVELOPPEMENT
- Ne pas utiliser en production
- Fonctionnalit√©s en cours de test et validation
- Rapports g√©n√©r√©s √† titre exp√©rimental uniquement

Responsabilit√©s:
- Analyser les rapports de performance et m√©triques
- Surveiller les logs et rapports CI/CD
- Identifier les probl√®mes de fond et tendances
- Proposer des missions strat√©giques d'am√©lioration
- G√©n√©rer des rapports de revue strat√©gique p√©riodiques
"""

import json
import logging
import os
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from collections import defaultdict
import statistics

@dataclass
class PerformanceMetric:
    """M√©trique de performance avec contexte temporel"""
    timestamp: datetime
    agent_id: str
    metric_name: str
    value: float
    unit: str
    context: Dict[str, Any]

@dataclass
class StrategicInsight:
    """Insight strat√©gique identifi√© par l'analyse"""
    type: str  # "performance_degradation", "efficiency_opportunity", "quality_issue"
    severity: str  # "LOW", "MEDIUM", "HIGH", "CRITICAL"
    title: str
    description: str
    impact: str
    recommended_actions: List[str]
    data_sources: List[str]
    confidence_score: float

@dataclass
class StrategicMission:
    """Mission strat√©gique propos√©e"""
    mission_id: str
    title: str
    objective: str
    priority: str  # "LOW", "MEDIUM", "HIGH", "CRITICAL"
    estimated_impact: str
    target_agents: List[str]
    success_criteria: List[str]
    prompt_template: str
    deadline: datetime

class AgentMetaStrategique:
    """Agent M√©ta-Strat√©gique pour l'auto-am√©lioration continue"""
    
    def __init__(self, workspace_path: str = "agent_factory_implementation"):
        self.workspace_path = Path(workspace_path)
        self.logger = logging.getLogger("AgentMetaStrategique_DRAFT")
        
        # üöß DRAFT VERSION WARNING
        self.version_info = {
            "status": "DRAFT/PROTOTYPE",
            "version": "0.1.0-draft",
            "warning": "‚ö†Ô∏è VERSION EXP√âRIMENTALE - NE PAS UTILISER EN PRODUCTION",
            "development_stage": "Prototype en test"
        }
        
        # Chemins des sources de donn√©es
        self.metrics_path = self.workspace_path / "metrics"
        self.logs_path = self.workspace_path / "logs"
        self.reports_path = self.workspace_path / "reports"
        self.syntheses_path = self.workspace_path
        
        # Configuration des seuils d'alerte
        self.performance_thresholds = {
            "response_time_ms": 100,
            "error_rate_percent": 5,
            "cpu_usage_percent": 80,
            "memory_usage_percent": 85,
            "success_rate_percent": 95
        }
        
        # Historique des m√©triques pour analyse de tendances
        self.metrics_history: List[PerformanceMetric] = []
        self.insights_history: List[StrategicInsight] = []
        
        # Patterns de d√©tection d'anomalies
        self.anomaly_patterns = {
            "performance_degradation": r"(lent|slow|timeout|√©chec|failed|error)",
            "resource_exhaustion": r"(memory|cpu|disk|resource|limite|limit)",
            "quality_issues": r"(bug|d√©faut|probl√®me|issue|erreur|exception)",
            "ci_cd_failures": r"(build|deploy|test|failed|√©chec|ci|cd)"
        }
        
    def analyser_performance_globale(self) -> Dict[str, Any]:
        """Analyse compl√®te des performances du syst√®me"""
        self.logger.info("[M√âTA-STRAT√âGIQUE-DRAFT] üöß D√©marrage analyse performance globale (VERSION PROTOTYPE)")
        
        # Collecte des donn√©es de toutes les sources
        metrics_data = self._collecter_metriques()
        logs_analysis = self._analyser_logs()
        reports_analysis = self._analyser_rapports()
        ci_cd_analysis = self._analyser_ci_cd()
        
        # Analyse des tendances
        trends_analysis = self._analyser_tendances(metrics_data)
        
        # D√©tection d'anomalies
        anomalies = self._detecter_anomalies(metrics_data, logs_analysis)
        
        # G√©n√©ration d'insights strat√©giques
        insights = self._generer_insights_strategiques(
            metrics_data, logs_analysis, reports_analysis, trends_analysis, anomalies
        )
        
        # Proposition de missions strat√©giques
        missions = self._proposer_missions_strategiques(insights)
        
        return {
            "draft_warning": "üöß RAPPORT G√âN√âR√â PAR VERSION DRAFT/PROTOTYPE - NE PAS UTILISER EN PRODUCTION",
            "version_info": self.version_info,
            "timestamp": datetime.now().isoformat(),
            "analysis_scope": {
                "metrics_analyzed": len(metrics_data),
                "logs_analyzed": len(logs_analysis),
                "reports_analyzed": len(reports_analysis),
                "period_days": 7
            },
            "performance_summary": self._generer_resume_performance(metrics_data),
            "trends_analysis": trends_analysis,
            "anomalies_detected": anomalies,
            "strategic_insights": [self._insight_to_dict(i) for i in insights],
            "proposed_missions": [self._mission_to_dict(m) for m in missions],
            "recommendations": self._generer_recommandations_executives(insights)
        }
    
    def _collecter_metriques(self) -> List[PerformanceMetric]:
        """Collecte des m√©triques de performance depuis les fichiers JSON"""
        metrics = []
        
        if not self.metrics_path.exists():
            self.logger.warning(f"R√©pertoire m√©triques non trouv√©: {self.metrics_path}")
            return metrics
        
        for metric_file in self.metrics_path.glob("*.json"):
            try:
                with open(metric_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                # Extraction des m√©triques selon le format
                timestamp = self._extraire_timestamp_fichier(metric_file.name)
                
                if isinstance(data, dict):
                    for key, value in data.items():
                        if isinstance(value, (int, float)):
                            metric = PerformanceMetric(
                                timestamp=timestamp,
                                agent_id=data.get("agent_id", "system"),
                                metric_name=key,
                                value=float(value),
                                unit=self._determiner_unite_metrique(key),
                                context={"source_file": metric_file.name}
                            )
                            metrics.append(metric)
                            
            except Exception as e:
                self.logger.error(f"Erreur lecture m√©trique {metric_file}: {e}")
        
        return metrics
    
    def _analyser_logs(self) -> List[Dict[str, Any]]:
        """Analyse des logs pour d√©tecter les probl√®mes"""
        log_analysis = []
        
        if not self.logs_path.exists():
            return log_analysis
        
        for log_file in self.logs_path.glob("*.log"):
            try:
                with open(log_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                analysis = {
                    "file": log_file.name,
                    "size_bytes": log_file.stat().st_size,
                    "last_modified": datetime.fromtimestamp(log_file.stat().st_mtime),
                    "error_count": len(re.findall(r'ERROR|ERREUR', content, re.IGNORECASE)),
                    "warning_count": len(re.findall(r'WARNING|WARN|ATTENTION', content, re.IGNORECASE)),
                    "exception_count": len(re.findall(r'Exception|Traceback', content, re.IGNORECASE)),
                    "anomaly_patterns": self._detecter_patterns_logs(content)
                }
                
                log_analysis.append(analysis)
                
            except Exception as e:
                self.logger.error(f"Erreur analyse log {log_file}: {e}")
        
        return log_analysis
    
    def _analyser_rapports(self) -> List[Dict[str, Any]]:
        """Analyse des rapports de performance et de qualit√©"""
        reports_analysis = []
        
        if not self.reports_path.exists():
            return reports_analysis
        
        for report_file in self.reports_path.glob("*.json"):
            try:
                with open(report_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                analysis = {
                    "file": report_file.name,
                    "agent_id": data.get("agent_id", "unknown"),
                    "mission_status": data.get("mission_status", "unknown"),
                    "quality_score": data.get("qualite_moyenne", 0),
                    "progression": data.get("progression_globale", 0),
                    "performance_indicators": self._extraire_indicateurs_performance(data),
                    "risks_identified": data.get("risques_identifies", []),
                    "timestamp": data.get("timestamp_rapport", datetime.now().isoformat())
                }
                
                reports_analysis.append(analysis)
                
            except Exception as e:
                self.logger.error(f"Erreur analyse rapport {report_file}: {e}")
        
        return reports_analysis
    
    def _analyser_ci_cd(self) -> Dict[str, Any]:
        """Analyse des m√©triques CI/CD (simul√©e pour l'exemple)"""
        # Dans un vrai syst√®me, ceci interrogerait l'API GitHub
        return {
            "build_success_rate": 85.0,
            "deployment_frequency": 2.5,  # par semaine
            "lead_time_hours": 4.2,
            "mttr_hours": 1.8,
            "failed_deployments": 3,
            "test_coverage": 78.5,
            "recent_failures": [
                {
                    "type": "build_failure",
                    "timestamp": "2025-06-19T02:15:00",
                    "reason": "Test timeout",
                    "impact": "MEDIUM"
                }
            ]
        }
    
    def _analyser_tendances(self, metrics: List[PerformanceMetric]) -> Dict[str, Any]:
        """Analyse des tendances de performance"""
        if not metrics:
            return {}
        
        # Grouper par m√©trique
        metrics_by_name = defaultdict(list)
        for metric in metrics:
            metrics_by_name[metric.metric_name].append(metric)
        
        trends = {}
        for metric_name, metric_list in metrics_by_name.items():
            if len(metric_list) < 2:
                continue
                
            # Trier par timestamp
            metric_list.sort(key=lambda x: x.timestamp)
            values = [m.value for m in metric_list]
            
            # Calcul de tendance
            if len(values) >= 3:
                recent_avg = statistics.mean(values[-3:])
                older_avg = statistics.mean(values[:-3]) if len(values) > 3 else values[0]
                
                trend_direction = "stable"
                if recent_avg > older_avg * 1.1:
                    trend_direction = "increasing"
                elif recent_avg < older_avg * 0.9:
                    trend_direction = "decreasing"
                
                trends[metric_name] = {
                    "direction": trend_direction,
                    "recent_average": recent_avg,
                    "older_average": older_avg,
                    "change_percent": ((recent_avg - older_avg) / older_avg * 100) if older_avg > 0 else 0,
                    "volatility": statistics.stdev(values) if len(values) > 1 else 0
                }
        
        return trends
    
    def _detecter_anomalies(self, metrics: List[PerformanceMetric], 
                           logs_analysis: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """D√©tection d'anomalies dans les m√©triques et logs"""
        anomalies = []
        
        # Anomalies de performance
        for metric in metrics:
            if metric.metric_name in self.performance_thresholds:
                threshold = self.performance_thresholds[metric.metric_name]
                
                if metric.value > threshold:
                    anomalies.append({
                        "type": "performance_threshold_exceeded",
                        "metric": metric.metric_name,
                        "value": metric.value,
                        "threshold": threshold,
                        "agent_id": metric.agent_id,
                        "timestamp": metric.timestamp.isoformat(),
                        "severity": "HIGH" if metric.value > threshold * 1.5 else "MEDIUM"
                    })
        
        # Anomalies dans les logs
        for log_analysis in logs_analysis:
            if log_analysis["error_count"] > 10:
                anomalies.append({
                    "type": "high_error_rate",
                    "source": log_analysis["file"],
                    "error_count": log_analysis["error_count"],
                    "timestamp": log_analysis["last_modified"].isoformat(),
                    "severity": "HIGH" if log_analysis["error_count"] > 50 else "MEDIUM"
                })
        
        return anomalies
    
    def _generer_insights_strategiques(self, metrics: List[PerformanceMetric],
                                     logs_analysis: List[Dict[str, Any]],
                                     reports_analysis: List[Dict[str, Any]],
                                     trends: Dict[str, Any],
                                     anomalies: List[Dict[str, Any]]) -> List[StrategicInsight]:
        """G√©n√©ration d'insights strat√©giques bas√©s sur l'analyse"""
        insights = []
        
        # Insight 1: D√©gradation de performance
        performance_issues = [a for a in anomalies if a["type"] == "performance_threshold_exceeded"]
        if performance_issues:
            insight = StrategicInsight(
                type="performance_degradation",
                severity="HIGH",
                title="D√©gradation de performance d√©tect√©e",
                description=f"{len(performance_issues)} m√©triques d√©passent les seuils de performance",
                impact="Risque de d√©gradation de l'exp√©rience utilisateur et de la productivit√©",
                recommended_actions=[
                    "Optimiser les agents avec les temps de r√©ponse les plus √©lev√©s",
                    "Analyser les goulots d'√©tranglement dans le pipeline",
                    "Impl√©menter des optimisations de cache et de parall√©lisation"
                ],
                data_sources=["metrics", "performance_reports"],
                confidence_score=0.85
            )
            insights.append(insight)
        
        # Insight 2: Opportunit√©s d'efficacit√©
        if trends:
            improving_metrics = [name for name, trend in trends.items() 
                               if trend["direction"] == "decreasing" and "time" in name.lower()]
            if improving_metrics:
                insight = StrategicInsight(
                    type="efficiency_opportunity",
                    severity="MEDIUM",
                    title="Opportunit√©s d'am√©lioration identifi√©es",
                    description=f"Certaines m√©triques montrent des am√©liorations: {', '.join(improving_metrics)}",
                    impact="Potentiel d'optimisation suppl√©mentaire √† exploiter",
                    recommended_actions=[
                        "Analyser les facteurs de ces am√©liorations",
                        "Appliquer les m√™mes optimisations √† d'autres composants",
                        "Documenter les bonnes pratiques identifi√©es"
                    ],
                    data_sources=["trends_analysis"],
                    confidence_score=0.70
                )
                insights.append(insight)
        
        # Insight 3: Probl√®mes de qualit√©
        quality_issues = [r for r in reports_analysis if r["quality_score"] < 8.0]
        if quality_issues:
            insight = StrategicInsight(
                type="quality_issue",
                severity="HIGH",
                title="Probl√®mes de qualit√© d√©tect√©s",
                description=f"{len(quality_issues)} agents ont un score de qualit√© < 8.0",
                impact="Risque de bugs et de maintenance accrue",
                recommended_actions=[
                    "Renforcer les processus de peer review",
                    "Am√©liorer la couverture de tests",
                    "Impl√©menter des m√©triques de qualit√© automatis√©es"
                ],
                data_sources=["quality_reports"],
                confidence_score=0.90
            )
            insights.append(insight)
        
        return insights
    
    def _proposer_missions_strategiques(self, insights: List[StrategicInsight]) -> List[StrategicMission]:
        """Proposition de missions strat√©giques bas√©es sur les insights"""
        missions = []
        
        for i, insight in enumerate(insights):
            if insight.severity in ["HIGH", "CRITICAL"]:
                mission = StrategicMission(
                    mission_id=f"MISSION_STRATEGIQUE_{datetime.now().strftime('%Y%m%d')}_{i+1:02d}",
                    title=f"R√©solution: {insight.title}",
                    objective=insight.description,
                    priority=insight.severity,
                    estimated_impact=insight.impact,
                    target_agents=self._identifier_agents_cibles(insight),
                    success_criteria=self._generer_criteres_succes(insight),
                    prompt_template=self._generer_prompt_mission(insight),
                    deadline=datetime.now() + timedelta(days=7 if insight.severity == "HIGH" else 3)
                )
                missions.append(mission)
        
        return missions
    
    def generer_rapport_revue_strategique(self) -> str:
        """G√©n√©ration du rapport de revue strat√©gique complet"""
        analysis = self.analyser_performance_globale()
        
        rapport = f"""# üöß RAPPORT DE REVUE STRAT√âGIQUE - VERSION DRAFT üöß
## Agent M√©ta-Strat√©gique PROTOTYPE - {datetime.now().strftime('%Y-%m-%d %H:%M')}

‚ö†Ô∏è **ATTENTION: RAPPORT G√âN√âR√â PAR VERSION EXP√âRIMENTALE**
- **Statut**: DRAFT/PROTOTYPE v0.1.0-draft
- **Usage**: Tests et validation uniquement
- **Production**: NE PAS UTILISER EN PRODUCTION

---

## üìä R√âSUM√â EX√âCUTIF

### P√©rim√®tre d'Analyse
- **M√©triques analys√©es**: {analysis['analysis_scope']['metrics_analyzed']}
- **Logs analys√©s**: {analysis['analysis_scope']['logs_analyzed']}
- **Rapports analys√©s**: {analysis['analysis_scope']['reports_analyzed']}
- **P√©riode**: {analysis['analysis_scope']['period_days']} jours

### Performance Globale
{self._formater_resume_performance(analysis['performance_summary'])}

---

## üîç INSIGHTS STRAT√âGIQUES

"""
        
        for insight in analysis['strategic_insights']:
            rapport += f"""### {insight['severity']} - {insight['title']}
**Impact**: {insight['impact']}
**Confiance**: {insight['confidence_score']:.0%}

**Actions Recommand√©es**:
{chr(10).join(f"- {action}" for action in insight['recommended_actions'])}

---

"""
        
        rapport += f"""## üéØ MISSIONS STRAT√âGIQUES PROPOS√âES

"""
        
        for mission in analysis['proposed_missions']:
            rapport += f"""### {mission['priority']} - {mission['title']}
**Objectif**: {mission['objective']}
**Impact Estim√©**: {mission['estimated_impact']}
**√âch√©ance**: {mission['deadline']}

**Agents Cibles**: {', '.join(mission['target_agents'])}

**Crit√®res de Succ√®s**:
{chr(10).join(f"- {critere}" for critere in mission['success_criteria'])}

---

"""
        
        rapport += f"""## üí° RECOMMANDATIONS EX√âCUTIVES

{chr(10).join(f"- {rec}" for rec in analysis['recommendations'])}

---

## üìà M√âTRIQUES CL√âS

### Tendances Identifi√©es
"""
        
        for metric_name, trend in analysis['trends_analysis'].items():
            direction_emoji = "üìà" if trend['direction'] == "increasing" else "üìâ" if trend['direction'] == "decreasing" else "‚û°Ô∏è"
            rapport += f"- **{metric_name}**: {direction_emoji} {trend['direction']} ({trend['change_percent']:+.1f}%)\n"
        
        rapport += f"""
### Anomalies D√©tect√©es
"""
        
        for anomaly in analysis['anomalies_detected']:
            severity_emoji = "üî¥" if anomaly['severity'] == "HIGH" else "üü°"
            rapport += f"- {severity_emoji} **{anomaly['type']}**: {anomaly.get('metric', anomaly.get('source', 'N/A'))}\n"
        
        rapport += f"""
---

üöß **AVERTISSEMENT VERSION DRAFT** üöß
*Rapport g√©n√©r√© automatiquement par l'Agent M√©ta-Strat√©gique PROTOTYPE v0.1.0-draft*
*‚ö†Ô∏è Version exp√©rimentale - Ne pas utiliser pour des d√©cisions de production*
*Prochaine revue programm√©e: {(datetime.now() + timedelta(days=1)).strftime('%Y-%m-%d %H:%M')}*
"""
        
        return rapport
    
    def sauvegarder_rapport_strategique(self) -> str:
        """Sauvegarde du rapport strat√©gique"""
        rapport = self.generer_rapport_revue_strategique()
        
        # Nom du fichier avec timestamp et marquage DRAFT
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"REVUE_STRATEGIQUE_DRAFT_{timestamp}.md"
        filepath = self.reports_path / filename
        
        # Cr√©er le r√©pertoire si n√©cessaire
        self.reports_path.mkdir(exist_ok=True)
        
        # Sauvegarder le rapport
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(rapport)
        
        self.logger.info(f"[M√âTA-STRAT√âGIQUE-DRAFT] üöß Rapport PROTOTYPE sauvegard√©: {filepath}")
        return str(filepath)
    
    # M√©thodes utilitaires
    def _extraire_timestamp_fichier(self, filename: str) -> datetime:
        """Extraction du timestamp depuis le nom de fichier"""
        match = re.search(r'(\d{8}_\d{6})', filename)
        if match:
            return datetime.strptime(match.group(1), '%Y%m%d_%H%M%S')
        return datetime.now()
    
    def _determiner_unite_metrique(self, metric_name: str) -> str:
        """D√©termination de l'unit√© d'une m√©trique"""
        if 'time' in metric_name.lower() or 'ms' in metric_name.lower():
            return 'ms'
        elif 'percent' in metric_name.lower() or 'rate' in metric_name.lower():
            return '%'
        elif 'count' in metric_name.lower():
            return 'count'
        else:
            return 'unit'
    
    def _detecter_patterns_logs(self, content: str) -> Dict[str, int]:
        """D√©tection de patterns dans les logs"""
        patterns = {}
        for pattern_name, pattern_regex in self.anomaly_patterns.items():
            matches = re.findall(pattern_regex, content, re.IGNORECASE)
            patterns[pattern_name] = len(matches)
        return patterns
    
    def _extraire_indicateurs_performance(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Extraction des indicateurs de performance depuis les donn√©es"""
        indicators = {}
        
        # Indicateurs standard
        for key in ['progression_globale', 'qualite_moyenne', 'performance']:
            if key in data:
                indicators[key] = data[key]
        
        # Indicateurs sp√©cialis√©s
        if 'agents_operationnels' in data and 'agents_total' in data:
            indicators['operational_ratio'] = data['agents_operationnels'] / data['agents_total'] * 100
        
        return indicators
    
    def _generer_resume_performance(self, metrics: List[PerformanceMetric]) -> Dict[str, Any]:
        """G√©n√©ration du r√©sum√© de performance"""
        if not metrics:
            return {"status": "no_data"}
        
        # M√©triques r√©centes (derni√®res 24h)
        recent_metrics = [m for m in metrics 
                         if m.timestamp > datetime.now() - timedelta(days=1)]
        
        return {
            "total_metrics": len(metrics),
            "recent_metrics": len(recent_metrics),
            "agents_monitored": len(set(m.agent_id for m in metrics)),
            "avg_response_time": statistics.mean([m.value for m in metrics 
                                                if 'time' in m.metric_name.lower()]) if any('time' in m.metric_name.lower() for m in metrics) else None,
            "status": "healthy" if len(recent_metrics) > 0 else "concerning"
        }
    
    def _formater_resume_performance(self, summary: Dict[str, Any]) -> str:
        """Formatage du r√©sum√© de performance pour le rapport"""
        if summary.get("status") == "no_data":
            return "‚ö†Ô∏è **Aucune donn√©e de performance disponible**"
        
        status_emoji = "‚úÖ" if summary["status"] == "healthy" else "‚ö†Ô∏è"
        
        return f"""{status_emoji} **Statut**: {summary["status"]}
- **Agents monitor√©s**: {summary["agents_monitored"]}
- **M√©triques totales**: {summary["total_metrics"]}
- **M√©triques r√©centes**: {summary["recent_metrics"]}
- **Temps de r√©ponse moyen**: {summary.get("avg_response_time", "N/A")} ms"""
    
    def _identifier_agents_cibles(self, insight: StrategicInsight) -> List[str]:
        """Identification des agents cibles pour une mission"""
        # Logique simplifi√©e - dans un vrai syst√®me, ceci serait plus sophistiqu√©
        if insight.type == "performance_degradation":
            return ["agent_08_performance_optimizer", "agent_06_specialiste_monitoring"]
        elif insight.type == "quality_issue":
            return ["agent_11_auditeur_qualite", "agent_05_specialiste_tests"]
        else:
            return ["agent_01_coordinateur_principal"]
    
    def _generer_criteres_succes(self, insight: StrategicInsight) -> List[str]:
        """G√©n√©ration des crit√®res de succ√®s pour une mission"""
        if insight.type == "performance_degradation":
            return [
                "Temps de r√©ponse < 100ms pour tous les agents",
                "R√©duction de 50% des anomalies de performance",
                "Am√©lioration de 20% des m√©triques de performance"
            ]
        elif insight.type == "quality_issue":
            return [
                "Score de qualit√© > 8.5 pour tous les agents",
                "Couverture de tests > 90%",
                "Z√©ro d√©faut critique identifi√©"
            ]
        else:
            return [
                "R√©solution compl√®te du probl√®me identifi√©",
                "Validation par les m√©triques de performance",
                "Documentation des am√©liorations apport√©es"
            ]
    
    def _generer_prompt_mission(self, insight: StrategicInsight) -> str:
        """G√©n√©ration du prompt pour une mission strat√©gique"""
        return f"""# MISSION STRAT√âGIQUE: {insight.title}

## CONTEXTE
{insight.description}

## OBJECTIF
R√©soudre le probl√®me identifi√© par l'Agent M√©ta-Strat√©gique avec un impact estim√©: {insight.impact}

## ACTIONS RECOMMAND√âES
{chr(10).join(f"- {action}" for action in insight.recommended_actions)}

## CRIT√àRES DE SUCC√àS
{chr(10).join(f"- {critere}" for critere in self._generer_criteres_succes(insight))}

## PRIORIT√â
{insight.severity} - Confiance: {insight.confidence_score:.0%}

## INSTRUCTIONS
1. Analyser en d√©tail le probl√®me identifi√©
2. Impl√©menter les solutions recommand√©es
3. Valider l'am√©lioration avec des m√©triques
4. Documenter les changements apport√©s
5. Rapporter les r√©sultats pour validation

üöß *Mission g√©n√©r√©e automatiquement par l'Agent M√©ta-Strat√©gique PROTOTYPE v0.1.0-draft*
‚ö†Ô∏è *Version exp√©rimentale - Valider avant impl√©mentation*
"""
    
    def _generer_recommandations_executives(self, insights: List[StrategicInsight]) -> List[str]:
        """G√©n√©ration des recommandations pour les dirigeants"""
        recommendations = []
        
        high_severity_count = len([i for i in insights if i.severity == "HIGH"])
        if high_severity_count > 0:
            recommendations.append(f"Traiter prioritairement les {high_severity_count} probl√®mes de haute s√©v√©rit√©")
        
        performance_issues = len([i for i in insights if i.type == "performance_degradation"])
        if performance_issues > 0:
            recommendations.append("Investir dans l'optimisation des performances syst√®me")
        
        quality_issues = len([i for i in insights if i.type == "quality_issue"])
        if quality_issues > 0:
            recommendations.append("Renforcer les processus de qualit√© et de validation")
        
        if not recommendations:
            recommendations.append("Maintenir les bonnes performances actuelles")
            recommendations.append("Continuer le monitoring proactif")
        
        return recommendations
    
    def _insight_to_dict(self, insight: StrategicInsight) -> Dict[str, Any]:
        """Conversion d'un insight en dictionnaire"""
        return {
            "type": insight.type,
            "severity": insight.severity,
            "title": insight.title,
            "description": insight.description,
            "impact": insight.impact,
            "recommended_actions": insight.recommended_actions,
            "data_sources": insight.data_sources,
            "confidence_score": insight.confidence_score
        }
    
    def _mission_to_dict(self, mission: StrategicMission) -> Dict[str, Any]:
        """Conversion d'une mission en dictionnaire"""
        return {
            "mission_id": mission.mission_id,
            "title": mission.title,
            "objective": mission.objective,
            "priority": mission.priority,
            "estimated_impact": mission.estimated_impact,
            "target_agents": mission.target_agents,
            "success_criteria": mission.success_criteria,
            "deadline": mission.deadline.isoformat(),
            "prompt_template": mission.prompt_template
        }


# Fonction principale pour test et ex√©cution
def main():
    """Fonction principale pour ex√©cution de l'agent"""
    # Configuration du logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Cr√©ation de l'agent
    agent = AgentMetaStrategique()
    
    # G√©n√©ration et sauvegarde du rapport
    rapport_path = agent.sauvegarder_rapport_strategique()
    print(f"‚úÖ Rapport de revue strat√©gique g√©n√©r√©: {rapport_path}")
    
    # Affichage du r√©sum√©
    analysis = agent.analyser_performance_globale()
    print(f"\nüìä R√©sum√© de l'analyse:")
    print(f"- Insights identifi√©s: {len(analysis['strategic_insights'])}")
    print(f"- Missions propos√©es: {len(analysis['proposed_missions'])}")
    print(f"- Anomalies d√©tect√©es: {len(analysis['anomalies_detected'])}")


if __name__ == "__main__":
    main() 