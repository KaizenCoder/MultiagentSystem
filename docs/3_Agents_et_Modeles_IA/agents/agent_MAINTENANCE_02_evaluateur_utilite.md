# Agent MAINTENANCE 02 â€“ Ã‰valuateur d'UtilitÃ©

## 1. Identification

- **Nom :** Ã‰valuateur d'UtilitÃ© NextGeneration
- **Identifiant :** `agent_MAINTENANCE_02_evaluateur_utilite`
- **Version :** 3.1.0 (Logging Uniforme + Audit Universel)
- **Responsable Principal :** Ã‰quipe de Maintenance NextGeneration
- **Contact Technique :** `#canal-maintenance-ia`

## 2. Description GÃ©nÃ©rale

âš–ï¸ Agent spÃ©cialisÃ© dans l'Ã©valuation quantitative de la pertinence et de la qualitÃ© fonctionnelle du code Python avec capacitÃ©s d'audit universel. Utilise l'analyse AST avancÃ©e avec systÃ¨me de scoring heuristique pour Ã©valuer des fichiers individuels ou des projets complets.

**ğŸš€ NOUVEAUTÃ‰ V3.1 (Travaux claudecode) :** IntÃ©gration complÃ¨te du systÃ¨me de logging uniforme + capacitÃ© d'audit universel d'Ã©valuation Ã©tendue pour analyser et scorer des **projets Python complets** (rÃ©pertoires entiers) en plus des fichiers individuels.

### ğŸ”§ SystÃ¨me de Logging Uniforme V3.1
```python
# âœ… MIGRATION SYSTÃˆME LOGGING UNIFIÃ‰ (claudecode)
try:
    from core.manager import LoggingManager
    logging_manager = LoggingManager()
    self.logger = logging_manager.get_logger(
        config_name="maintenance",
        custom_config={
            "logger_name": f"nextgen.maintenance.evaluateur_utilite.{self.id}",
            "log_dir": "logs/maintenance/evaluateur",
            "metadata": {
                "agent_type": "MAINTENANCE_02_evaluateur_utilite",
                "agent_role": "evaluateur_utilite",
                "system": "nextgeneration"
            }
        }
    )
except ImportError:
    # Fallback en cas d'indisponibilitÃ© du LoggingManager
    self.logger = logging.getLogger(self.__class__.__name__)
```

## 3. Objectifs et Missions

### 3.1 Missions Principales
- **Ã‰valuation Quantitative** : Scoring automatique basÃ© sur l'analyse AST des structures Python
- **Classification d'UtilitÃ©** : DÃ©termination binaire utilitÃ©/inutilitÃ© selon seuil configurable
- **DÃ©tection d'Obsolescence** : Identification d'Ã©lÃ©ments vides ou non fonctionnels
- **Analyse de ComplexitÃ©** : Mesure de la richesse structurelle et fonctionnelle
- **Support Maintenance** : Aide Ã  la dÃ©cision pour conservation/suppression de code

### 3.2 CapacitÃ©s d'Audit Universel (V3.0)
- **Audit de fichiers individuels** : Ã‰valuation et scoring d'un fichier Python spÃ©cifique
- **ğŸ†• Audit de rÃ©pertoires complets** : Analyse et scoring rÃ©cursif de structures de projets entiÃ¨res
- **Filtrage intelligent** : Exclusion automatique des rÃ©pertoires non pertinents (.venv, __pycache__, .git, etc.)
- **Rapports consolidÃ©s** : MÃ©triques d'utilitÃ© globales + dÃ©tails par fichier
- **Scoring unifiÃ©** : SystÃ¨me de notation d'utilitÃ© cohÃ©rent (0-100)

## 4. Architecture V3.0 (Mission Claudecode)

### 4.1 Architecture Technique
- **Orchestrateur Central** : `audit_universal_evaluation` coordonne tous les types d'audit
- **Audit Fichier Unique** : `_audit_single_python_file` pour l'analyse et scoring dÃ©taillÃ©
- **Filtrage Intelligent** : `_should_skip_path` pour ignorer les rÃ©pertoires non pertinents
- **Mapping QualitÃ©** : `_map_score_to_utility_health` pour la notation uniforme
- **Gestion ConsolidÃ©e** : Centralisation des mÃ©triques et scoring dans l'orchestrateur

### 4.2 MÃ©triques d'UtilitÃ©
```python
utility_metrics = {
    'code_complexity': {'weight': 0.3, 'threshold': {'warning': 70, 'critical': 50}},
    'functional_richness': {'weight': 0.3, 'threshold': {'warning': 75, 'critical': 60}},
    'code_reuse': {'weight': 0.2, 'threshold': {'warning': 80, 'critical': 65}},
    'maintainability': {'weight': 0.2, 'threshold': {'warning': 85, 'critical': 70}}
}
```

## 5. Guide d'Utilisation

### 5.1 Initialisation
```python
from agents.agent_MAINTENANCE_02_evaluateur_utilite import AgentMAINTENANCE02EvaluateurUtilite
agent = AgentMAINTENANCE02EvaluateurUtilite()
await agent.startup()
```

### 5.2 Audit d'un Fichier Individuel
```python
# Audit d'utilitÃ© d'un fichier Python spÃ©cifique
task_details = {
    "action": "audit_universal_evaluation",
    "params": {
        "target_path": "chemin/vers/votre/fichier.py"
    }
}
result = await agent.execute_task(task_details)
print(f"Score utilitÃ© : {result['data']['score_global']}/100")
print(f"Ã‰tat de l'utilitÃ© : {result['data']['etat_utilite']}")
```

### 5.3 ğŸ†• Audit d'un Projet Complet (V3.0)
```python
# Audit d'utilitÃ© d'un rÃ©pertoire complet
task_details = {
    "action": "audit_universal_evaluation",
    "params": {
        "target_path": "chemin/vers/votre/projet/"
    }
}
result = await agent.execute_task(task_details)

# RÃ©sultats consolidÃ©s
print(f"Nombre de fichiers analysÃ©s : {result['data']['nb_fichiers_analyses']}")
print(f"Score global d'utilitÃ© : {result['data']['score_global']}/100")
print(f"Ã‰tat global de l'utilitÃ© : {result['data']['etat_utilite']}")

# DÃ©tails par fichier
for fichier_result in result['data']['resultats_fichiers']:
    print(f"- {fichier_result['fichier']} :")
    print(f"  Score : {fichier_result['score']}/100")
    print(f"  ProblÃ¨mes utilitÃ© : {len(fichier_result['utility_issues'])}")
```

## 6. SpÃ©cifications Techniques V3.0

### 6.1 MÃ©thodes Principales
- **`audit_universal_evaluation(target_path)`** : Orchestrateur principal (fichier ou rÃ©pertoire)
- **`_audit_single_python_file(file_path)`** : Audit dÃ©taillÃ© d'utilitÃ© d'un fichier
- **`_should_skip_path(path)`** : Filtrage intelligent des chemins Ã  ignorer
- **`_map_score_to_utility_health(score)`** : Mapping score â†’ Ã©tat de l'utilitÃ©

### 6.2 Filtrage Intelligent
RÃ©pertoires automatiquement ignorÃ©s :
- `.venv/`, `venv/`, `env/` (environnements virtuels)
- `__pycache__/`, `.pyc` (cache Python)
- `.git/`, `.svn/` (contrÃ´le de version)
- `node_modules/`, `.npm/` (dÃ©pendances JS)
- `build/`, `dist/`, `.egg-info/` (artefacts de build)

### 6.3 MÃ©triques d'UtilitÃ©
- **Score global** : Note consolidÃ©e 0-100
- **Nombre de fichiers** : Fichiers Python analysÃ©s
- **ProblÃ¨mes utilitÃ©** : Issues dÃ©tectÃ©es par type
- **ComplexitÃ© du code** : Richesse structurelle
- **Richesse fonctionnelle** : DiversitÃ© des fonctionnalitÃ©s
- **RÃ©utilisation du code** : ModularitÃ© et rÃ©utilisation

## 7. FonctionnalitÃ©s ClÃ©s (ConformitÃ© Pattern Factory)

L'agent respecte le Pattern Factory NextGeneration et expose les mÃ©thodes suivantes :

- **`startup()`** : Initialise l'agent Ã©valuateur d'utilitÃ©
- **`health_check()`** : VÃ©rifie l'Ã©tat de santÃ©. Retourne `{"status": "ok"}` en fonctionnement normal
- **`execute_task(task: Task)`** : Point d'entrÃ©e principal pour les Ã©valuations d'utilitÃ©
  - **Action attendue** : Ã‰valuation de fichier Python
    - **`task.params` attendus** :
      - `file_path` (str) : Chemin vers le fichier Python Ã  Ã©valuer
    - **RÃ©sultat** : Score d'utilitÃ©, classification binaire et dÃ©tails
- **`shutdown()`** : ArrÃªte l'agent proprement

### CapacitÃ©s SpÃ©cialisÃ©es

```python
get_capabilities() -> [
    "evaluate_utility",
    "ast_evaluation", 
    "scoring_heuristique_code",
    "detection_elements_obsoletes",
    "classification_binaire_utilite",
    "analyse_complexite_structurelle",
    "gestion_erreurs_syntaxiques"
]
```

## 8. SystÃ¨me de Scoring Heuristique

### MÃ©triques de Base

| Ã‰lÃ©ment AST | Points AttribuÃ©s | Description |
|-------------|------------------|-------------|
| **Import** | +1 par import | Chaque module importÃ© |
| **Classe** | +10 + len(body) | Points base + complexitÃ© interne |
| **Fonction** | +5 + len(body) | Points base + complexitÃ© interne |
| **Appel de fonction** | +1 | Chaque ast.Call dÃ©tectÃ© |
| **Structure contrÃ´le** | +2 | if/for/while/try |

### Bonus et Malus

- **Bonus Classe + Fonction** : +5 points si prÃ©sence des deux
- **Malus Ã‰lÃ©ments Vides** : -5 points par fonction/classe avec seulement `pass`

### Classification

```python
score >= seuil_min (dÃ©faut: 15) â†’ "Utile" (is_useful: True)
score < seuil_min â†’ "Peu utile" (is_useful: False)
```

## 9. Workflow d'Ã‰valuation

```
1. ğŸ“‹ RÃ©ception tÃ¢che avec file_path
2. ğŸ“– Lecture fichier Python (UTF-8)
3. ğŸ—ï¸ Parsing AST du code source
4. ğŸ§® Calcul score via mÃ©triques heuristiques
5. âš–ï¸ Classification binaire selon seuil
6. ğŸ“Š Retour rÃ©sultat structurÃ©
```

## 10. Format de RÃ©sultat

### SuccÃ¨s d'Ã‰valuation

```json
{
  "success": true,
  "data": {
    "score": 42,
    "is_useful": true,
    "details": "Ã‰valuation rÃ©ussie"
  }
}
```

### Erreur Syntaxique

```json
{
  "success": true,
  "data": {
    "score": 0,
    "is_useful": false,
    "details": "Erreur de syntaxe: invalid syntax (line 15)"
  }
}
```

### Erreur de Lecture

```json
{
  "success": false,
  "error": "Erreur de lecture du fichier: [Errno 2] No such file or directory"
}
```

## 11. Exemples d'Utilisation

### Ã‰valuation d'un Agent

```python
from core.agent_factory_architecture import Task
from agents.agent_MAINTENANCE_02_evaluateur_utilite import create_agent_MAINTENANCE_02_evaluateur_utilite

# CrÃ©ation de l'agent
evaluateur = create_agent_MAINTENANCE_02_evaluateur_utilite()
await evaluateur.startup()

# Ã‰valuation d'un fichier
task = Task(
    type="evaluate_utility",
    params={"file_path": "./agents/agent_exemple.py"}
)

result = await evaluateur.execute_task(task)
if result.success:
    score = result.data["score"]
    is_useful = result.data["is_useful"]
    print(f"Score: {score}, Utile: {is_useful}")
else:
    print(f"Erreur: {result.error}")

await evaluateur.shutdown()
```

### Ã‰valuation de Plusieurs Fichiers

```python
files_to_evaluate = [
    "./agents/agent_01.py",
    "./agents/agent_02.py", 
    "./agents/agent_obsolete.py"
]

results = []
for file_path in files_to_evaluate:
    task = Task(type="evaluate_utility", params={"file_path": file_path})
    result = await evaluateur.execute_task(task)
    results.append({
        "file": file_path,
        "score": result.data.get("score", 0) if result.success else 0,
        "useful": result.data.get("is_useful", False) if result.success else False
    })

# Filtrer les agents utiles
useful_agents = [r for r in results if r["useful"]]
print(f"Agents utiles: {len(useful_agents)}/{len(results)}")
```

## 12. Configuration

### Seuil d'UtilitÃ© PersonnalisÃ©

```python
# Via configuration d'agent (si disponible)
evaluateur.config = {"min_score_for_usefulness": 20}  # Seuil plus Ã©levÃ©

# Seuil par dÃ©faut : 15 points
```

## 13. DÃ©pendances

- **Python 3.8+**
- **Modules standard** : ast, pathlib, logging
- **core.agent_factory_architecture** (Agent, Task, Result)
- **Aucune dÃ©pendance externe** pour l'analyse AST

## 14. Journal des Modifications (Changelog)

- **v2.2.0 (2025-06-26)** :
  - Harmonisation avec standards Pattern Factory NextGeneration
  - Enrichissement docstrings classe avec description dÃ©taillÃ©e du scoring
  - Extension `get_capabilities()` : 2 â†’ 7 capacitÃ©s spÃ©cialisÃ©es
  - Documentation .md complÃ¨tement refaite avec exemples concrets
- **v2.1.0** :
  - AmÃ©lioration systÃ¨me de scoring avec bonus/malus
  - Gestion robuste des erreurs syntaxiques
- **v1.0** :
  - Version initiale avec Ã©valuation AST de base

## 15. ProcÃ©dure de Test CLI

```python
# test_agent_maintenance_02_evaluation.py
import asyncio
from agents.agent_MAINTENANCE_02_evaluateur_utilite import create_agent_MAINTENANCE_02_evaluateur_utilite
from core.agent_factory_architecture import Task

async def test_evaluateur_utilite():
    # 1. CrÃ©ation et startup
    evaluateur = create_agent_MAINTENANCE_02_evaluateur_utilite()
    await evaluateur.startup()
    
    # 2. Test health check
    health = await evaluateur.health_check()
    print(f"SantÃ©: {health}")
    
    # 3. Test capacitÃ©s
    caps = evaluateur.get_capabilities()
    print(f"CapacitÃ©s: {len(caps)} trouvÃ©es")
    
    # 4. Test auto-Ã©valuation
    task_self = Task(
        type="evaluate_utility",
        params={"file_path": "./agents/agent_MAINTENANCE_02_evaluateur_utilite.py"}
    )
    
    result = await evaluateur.execute_task(task_self)
    print(f"Auto-Ã©valuation rÃ©ussie: {result.success}")
    if result.success:
        print(f"  Score: {result.data['score']}")
        print(f"  Utile: {result.data['is_useful']}")
        print(f"  DÃ©tails: {result.data['details']}")
    
    # 5. Test fichier inexistant
    task_error = Task(
        type="evaluate_utility", 
        params={"file_path": "./fichier_inexistant.py"}
    )
    
    result_error = await evaluateur.execute_task(task_error)
    print(f"Gestion erreur: {not result_error.success}")
    
    # 6. Shutdown
    await evaluateur.shutdown()

# ExÃ©cution
# python -c "import asyncio; asyncio.run(test_evaluateur_utilite())"
```

## 16. Cas d'Usage RecommandÃ©s

- **Audit de codebase** : Identification des fichiers peu utiles
- **Nettoyage de maintenance** : Support Ã  la dÃ©cision de suppression
- **Ã‰valuation qualitÃ©** : Mesure quantitative de la richesse du code
- **Triage automatique** : Classification rapide de grands volumes de fichiers
- **MÃ©triques de projet** : Calcul de scores globaux de qualitÃ©

## 17. Statut et Validation

- âœ… **Pattern Factory** : Conforme (Agent, Task, Result)
- âœ… **MÃ©thodes async** : startup, shutdown, execute_task, health_check  
- âœ… **Capabilities** : 7 capacitÃ©s spÃ©cialisÃ©es dÃ©finies
- âœ… **Documentation** : Docstrings enrichies et .md synchronisÃ©
- âœ… **Tests CLI** : ProcÃ©dure de validation dÃ©finie
- âœ… **Scoring System** : MÃ©triques heuristiques documentÃ©es

**Agent MAINTENANCE 02 - Ã‰tat : PRÃŠT POUR VALIDATION**