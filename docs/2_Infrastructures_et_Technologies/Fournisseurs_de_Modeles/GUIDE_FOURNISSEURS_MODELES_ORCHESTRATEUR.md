# Guide Complet : Fournisseurs de ModÃ¨les - Orchestrateur NextGeneration

## ğŸ¯ Vue d'ensemble

L'orchestrateur NextGeneration supporte plusieurs fournisseurs de modÃ¨les LLM pour offrir une flexibilitÃ© maximale et des performances optimales selon les tÃ¢ches.

## ğŸ”‘ Configuration des ClÃ©s API

### Variables d'environnement requises dans `.env`

```env
# ğŸ”´ OBLIGATOIRES
OPENAI_API_KEY=sk-proj-your-openai-key-here
ANTHROPIC_API_KEY=sk-ant-api03-your-anthropic-key-here
ORCHESTRATOR_API_KEY=your-orchestrator-secret-key

# ğŸŸ¡ OPTIONNELLES
GOOGLE_API_KEY=AIzaSy-your-google-key-here      # Pour Gemini
GEMINI_API_KEY=AIzaSy-your-gemini-key-here      # Alias pour Gemini

# ğŸ”µ MODÃˆLES LOCAUX (RTX 3090)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_GPU_DEVICE=1
LOCAL_MODELS_ENABLED=true
LOCAL_MODELS_PATH=D:/modeles_llm
```

---

## ğŸ¤– Fournisseurs SupportÃ©s

### 1. ğŸŸ¢ OpenAI (OBLIGATOIRE)

#### Configuration
- **Variable d'environnement** : `OPENAI_API_KEY`
- **Format de clÃ©** : `sk-proj-...` ou `sk-...`
- **Endpoint** : `https://api.openai.com/v1/`
- **Statut** : âœ… TestÃ© et fonctionnel

#### ModÃ¨les disponibles
| ModÃ¨le | Usage dans l'orchestrateur | SpÃ©cialitÃ© | CoÃ»t |
|--------|----------------------------|------------|------|
| **gpt-4o** | Agent `code_generation`, `diag_postgresql`, `testing` | Code, analyse, diagnostic | ğŸ’°ğŸ’°ğŸ’° |
| **gpt-4-turbo** | Alternative haute performance | Raisonnement complexe | ğŸ’°ğŸ’°ğŸ’° |
| **gpt-3.5-turbo** | TÃ¢ches gÃ©nÃ©rales rapides | RapiditÃ©, Ã©conomie | ğŸ’° |

#### Utilisation dans l'orchestrateur
```python
# Dans workers.py
if agent_type == "code_generation":
    llm = ChatOpenAI(model="gpt-4o", temperature=0.1, api_key=settings.OPENAI_API_KEY)
elif agent_type == "testing":
    llm = ChatOpenAI(model="gpt-4o", temperature=0.2, api_key=settings.OPENAI_API_KEY)
```

#### Comment obtenir la clÃ©
1. Aller sur [OpenAI Platform](https://platform.openai.com/api-keys)
2. CrÃ©er un compte et ajouter des crÃ©dits
3. GÃ©nÃ©rer une nouvelle clÃ© API
4. Ajouter dans `.env` : `OPENAI_API_KEY=sk-proj-...`

---

### 2. ğŸ”µ Anthropic Claude (OBLIGATOIRE)

#### Configuration
- **Variable d'environnement** : `ANTHROPIC_API_KEY`
- **Format de clÃ©** : `sk-ant-api03-...`
- **Endpoint** : `https://api.anthropic.com/v1/`
- **Statut** : âœ… TestÃ© et fonctionnel

#### ModÃ¨les disponibles
| ModÃ¨le | Usage dans l'orchestrateur | SpÃ©cialitÃ© | CoÃ»t |
|--------|----------------------------|------------|------|
| **claude-3-5-sonnet-20240620** | Agent `documentation` | RÃ©daction, analyse longue | ğŸ’°ğŸ’° |
| **claude-3-haiku-20240307** | Tests et validations rapides | RapiditÃ©, Ã©conomie | ğŸ’° |
| **claude-3-opus-20240229** | TÃ¢ches complexes | Raisonnement avancÃ© | ğŸ’°ğŸ’°ğŸ’° |

#### Utilisation dans l'orchestrateur
```python
# Dans workers.py
if agent_type == "documentation":
    llm = ChatAnthropic(model="claude-3-5-sonnet-20240620", temperature=0.2, api_key=settings.ANTHROPIC_API_KEY)
```

#### Comment obtenir la clÃ©
1. Aller sur [Anthropic Console](https://console.anthropic.com/)
2. CrÃ©er un compte et ajouter des crÃ©dits
3. GÃ©nÃ©rer une clÃ© API
4. Ajouter dans `.env` : `ANTHROPIC_API_KEY=sk-ant-api03-...`

---

### 3. ğŸŸ¡ Google Gemini (OPTIONNEL)

#### Configuration
- **Variables d'environnement** : `GOOGLE_API_KEY` ou `GEMINI_API_KEY`
- **Format de clÃ©** : `AIzaSy...`
- **Endpoint** : `https://generativelanguage.googleapis.com/v1beta/`
- **Statut** : âœ… TestÃ© et fonctionnel

#### ModÃ¨les disponibles
| ModÃ¨le | CapacitÃ©s | SpÃ©cialitÃ© | CoÃ»t |
|--------|-----------|------------|------|
| **gemini-1.5-pro** | Multimodal, 1M tokens contexte | Analyse approfondie, code | ğŸ’°ğŸ’° |
| **gemini-1.5-flash** | Rapide, multimodal | RÃ©ponses rapides, Ã©conomie | ğŸ’° |
| **gemini-ultra** | Performance maximale | TÃ¢ches complexes | ğŸ’°ğŸ’°ğŸ’° |

#### Utilisation via API directe
```python
# Exemple d'utilisation directe
url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={api_key}"
payload = {
    "contents": [{"parts": [{"text": "Votre prompt ici"}]}],
    "generationConfig": {
        "temperature": 0.7,
        "maxOutputTokens": 1000
    }
}
```

#### Comment obtenir la clÃ©
1. Aller sur [Google AI Studio](https://makersuite.google.com/app/apikey)
2. CrÃ©er un compte Google
3. GÃ©nÃ©rer une clÃ© API
4. Ajouter dans `.env` : `GOOGLE_API_KEY=AIzaSy...`

#### IntÃ©gration dans l'orchestrateur
```python
# Ã€ ajouter dans workers.py pour support complet
elif agent_type == "gemini_analysis":
    # Configuration Gemini (Ã  implÃ©menter)
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", google_api_key=settings.GOOGLE_API_KEY)
```

---

### 4. ğŸ”´ Ollama Local (RTX 3090)

#### Configuration
- **Variables d'environnement** :
  ```env
  OLLAMA_BASE_URL=http://localhost:11434
  OLLAMA_GPU_DEVICE=1
  LOCAL_MODELS_ENABLED=true
  LOCAL_MODELS_PATH=D:/modeles_llm
  ```
- **Endpoint** : `http://localhost:11434/api/`
- **Statut** : âœ… ConfigurÃ© pour RTX 3090

#### ModÃ¨les locaux supportÃ©s
| ModÃ¨le | Taille | SpÃ©cialitÃ© | Performance RTX 3090 |
|--------|--------|------------|----------------------|
| **llama2:70b** | 70B | Raisonnement gÃ©nÃ©ral | ğŸš€ğŸš€ |
| **codellama:34b** | 34B | GÃ©nÃ©ration de code | ğŸš€ğŸš€ğŸš€ |
| **mistral:7b** | 7B | RapiditÃ©, Ã©conomie | ğŸš€ğŸš€ğŸš€ğŸš€ |
| **neural-chat:7b** | 7B | Conversation | ğŸš€ğŸš€ğŸš€ğŸš€ |
| **vicuna:13b** | 13B | Polyvalent | ğŸš€ğŸš€ğŸš€ |

#### Utilisation dans l'orchestrateur
```python
# Dans ollama_worker.py
class OllamaLocalWorker:
    def __init__(self, config):
        self.ollama_url = getattr(config, 'OLLAMA_BASE_URL', 'http://localhost:11434')
        self.gpu_device = getattr(config, 'OLLAMA_GPU_DEVICE', '1')
        
    async def _call_ollama(self, model_name: str, prompt: str):
        # Appel API Ollama local
        response = await client.post(f"{self.ollama_url}/api/generate", json=payload)
```

#### Installation et configuration
```bash
# Installation Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# DÃ©marrage avec GPU
CUDA_VISIBLE_DEVICES=1 ollama serve

# Installation des modÃ¨les
ollama pull llama2:70b
ollama pull codellama:34b
ollama pull mistral:7b
```

---

## ğŸ¯ Agents et Attribution des ModÃ¨les

### Configuration actuelle dans `workers.py`

| Agent | ModÃ¨le AssignÃ© | Fournisseur | Justification |
|-------|----------------|-------------|---------------|
| **code_generation** | gpt-4o | OpenAI | Excellence en gÃ©nÃ©ration de code |
| **documentation** | claude-3-5-sonnet | Anthropic | QualitÃ© rÃ©dactionnelle supÃ©rieure |
| **testing** | gpt-4o | OpenAI | PrÃ©cision pour les tests |
| **diag_postgresql** | gpt-4o | OpenAI | Analyse technique approfondie |

### Agents potentiels avec Gemini
```python
# Configurations suggÃ©rÃ©es
"gemini_analysis": {
    "model": "gemini-1.5-pro",
    "provider": "Google",
    "use_case": "Analyse multimodale, contexte long"
}

"gemini_rapid": {
    "model": "gemini-1.5-flash", 
    "provider": "Google",
    "use_case": "RÃ©ponses rapides, prototypage"
}
```

---

## ğŸ“Š Comparaison des Performances

### Temps de rÃ©ponse moyens (tests rÃ©els)
| Fournisseur | ModÃ¨le | Temps moyen | QualitÃ© | CoÃ»t relatif |
|-------------|--------|-------------|---------|--------------|
| OpenAI | gpt-4o | 3-5s | â­â­â­â­â­ | ğŸ’°ğŸ’°ğŸ’° |
| Anthropic | claude-3-5-sonnet | 4-7s | â­â­â­â­â­ | ğŸ’°ğŸ’° |
| Google | gemini-1.5-flash | 0.6s | â­â­â­â­ | ğŸ’° |
| Google | gemini-1.5-pro | 2-3s | â­â­â­â­â­ | ğŸ’°ğŸ’° |
| Ollama | llama2:70b | 1-2s | â­â­â­â­ | ğŸ†“ |

### Recommandations d'usage
- **DÃ©veloppement/Prototypage** : Gemini Flash (rapide, Ã©conomique)
- **Production/QualitÃ©** : GPT-4o ou Claude-3.5-Sonnet
- **Analyse de code** : GPT-4o (meilleure comprÃ©hension)
- **RÃ©daction longue** : Claude-3.5-Sonnet (contexte Ã©tendu)
- **Ã‰conomie/ConfidentialitÃ©** : Ollama local (gratuit, privÃ©)

---

## ğŸ”§ Scripts de Test

### Test de toutes les clÃ©s API
```bash
# Test automatique de toutes les clÃ©s
python test_api_keys.py

# Test spÃ©cifique Gemini
python test_gemini_rapide.py

# Test complet avec benchmarks
python test_gemini_complet.py
```

### Test de l'orchestrateur avec diffÃ©rents modÃ¨les
```bash
# Test avec modÃ¨le spÃ©cifique
curl -X POST "http://localhost:8003/orchestrator/process" \
  -H "X-API-Key: demo-key-for-testing" \
  -H "Content-Type: application/json" \
  -d '{
    "task": "Analyse ce code Python",
    "preferred_model": "gemini-1.5-flash",
    "requirements": ["analysis"]
  }'
```

---

## ğŸš€ Extension et Personnalisation

### Ajouter un nouveau fournisseur
1. **Ajouter la clÃ© API** dans `.env`
2. **Modifier `config.py`** pour inclure la nouvelle clÃ©
3. **Ã‰tendre `workers.py`** avec le nouveau modÃ¨le
4. **CrÃ©er un agent spÃ©cialisÃ©** si nÃ©cessaire
5. **Tester** avec les scripts fournis

### Exemple d'ajout d'un agent Gemini
```python
# Dans workers.py
elif agent_type == "gemini_multimodal":
    # Configuration pour Gemini (nÃ©cessite langchain-google-genai)
    from langchain_google_genai import ChatGoogleGenerativeAI
    llm = ChatGoogleGenerativeAI(
        model="gemini-1.5-pro",
        google_api_key=settings.GOOGLE_API_KEY,
        temperature=0.3
    )
    tools = real_analysis_tools
```

---

## ğŸ“‹ Checklist de Configuration

### âœ… Configuration Minimale (Fonctionnelle)
- [ ] `OPENAI_API_KEY` configurÃ©e et testÃ©e
- [ ] `ANTHROPIC_API_KEY` configurÃ©e et testÃ©e  
- [ ] `ORCHESTRATOR_API_KEY` dÃ©finie
- [ ] Orchestrateur dÃ©marrable : `python start_orchestrator.py`
- [ ] Tests API rÃ©ussis : `python test_api_keys.py`

### âœ… Configuration ComplÃ¨te (RecommandÃ©e)
- [ ] Toutes les clÃ©s de la configuration minimale
- [ ] `GOOGLE_API_KEY` ou `GEMINI_API_KEY` ajoutÃ©e
- [ ] Ollama installÃ© et configurÃ© (optionnel)
- [ ] Tests Gemini rÃ©ussis : `python test_gemini_rapide.py`
- [ ] ModÃ¨les locaux tÃ©lÃ©chargÃ©s (optionnel)

### âœ… Configuration AvancÃ©e (Production)
- [ ] Toutes les configurations prÃ©cÃ©dentes
- [ ] Monitoring et mÃ©triques activÃ©s
- [ ] Load balancing configurÃ©
- [ ] Fallback entre fournisseurs implÃ©mentÃ©
- [ ] CoÃ»ts et quotas surveillÃ©s

---

## ğŸ‰ RÃ©sumÃ©

L'orchestrateur NextGeneration supporte actuellement **4 fournisseurs principaux** :

1. **ğŸŸ¢ OpenAI** - Obligatoire, excellent pour le code
2. **ğŸ”µ Anthropic** - Obligatoire, parfait pour la documentation  
3. **ğŸŸ¡ Google Gemini** - Optionnel, rapide et Ã©conomique
4. **ğŸ”´ Ollama Local** - Optionnel, gratuit et privÃ© (RTX 3090)

**Configuration actuelle testÃ©e et fonctionnelle** :
- âœ… 3 fournisseurs cloud configurÃ©s
- âœ… 1 solution locale (Ollama RTX 3090)
- âœ… 4 agents spÃ©cialisÃ©s opÃ©rationnels
- âœ… Scripts de test et monitoring complets

**Votre ajout de `GEMINI_API_KEY` permet d'utiliser Google Gemini comme alternative rapide et Ã©conomique aux modÃ¨les principaux.**

---

*Guide des Fournisseurs de ModÃ¨les - Orchestrateur NextGeneration v1.0* 