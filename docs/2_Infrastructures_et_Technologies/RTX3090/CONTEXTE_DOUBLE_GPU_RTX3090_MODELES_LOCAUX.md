# üéÆ CONTEXTE CONFIGURATION DOUBLE GPU RTX 3090 - MOD√àLES LLM LOCAUX
## Guide Contexte et Workflow pour Assistants IA

---

**Projet :** NextGeneration Multi-Agent System  
**Configuration :** Double GPU RTX 3090 (Port Secondaire)  
**Date :** 17 Juin 2025  
**Version :** 1.0 CONTEXTE COMPLET  
**Statut :** Document de R√©f√©rence Principal  

---

## üéØ OBJECTIF PRINCIPAL

**Utiliser des mod√®les LLM locaux pour alimenter les assistants du syst√®me multi-agent NextGeneration** en exploitant la configuration double GPU RTX 3090 optimis√©e pour les performances IA.

### üéÆ **Vision Syst√®me**
- **Ind√©pendance** : Mod√®les LLM h√©berg√©s localement, aucune d√©pendance cloud
- **Performance** : Exploitation optimale de la RTX 3090 (24GB VRAM)
- **Flexibilit√©** : S√©lection dynamique de mod√®les selon les besoins des assistants
- **√âvolutivit√©** : Capacit√© d'ajout de nouveaux mod√®les selon les besoins

---

## üèóÔ∏è ARCHITECTURE CONFIGURATION DOUBLE GPU

### üéÆ **Configuration Mat√©rielle Actuelle**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    CONFIGURATION SYST√àME                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üéÆ GPU Principal (Port Secondaire - Bus PCI 1)           ‚îÇ
‚îÇ      ‚îú‚îÄ NVIDIA GeForce RTX 3090                            ‚îÇ
‚îÇ      ‚îú‚îÄ VRAM: 24GB GDDR6X                                  ‚îÇ
‚îÇ      ‚îú‚îÄ Compute Capability: SM_86                          ‚îÇ
‚îÇ      ‚îî‚îÄ Usage: IA/ML EXCLUSIF (SuperWhisper V6 Standards)  ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  üéÆ GPU Secondaire (Port Principal - Bus PCI 0)           ‚îÇ
‚îÇ      ‚îú‚îÄ NVIDIA GeForce RTX 5060 Ti                         ‚îÇ
‚îÇ      ‚îú‚îÄ VRAM: 16GB GDDR6X                                  ‚îÇ
‚îÇ      ‚îú‚îÄ Compute Capability: SM_120                         ‚îÇ
‚îÇ      ‚îî‚îÄ Usage: INTERDIT pour IA (Incompatibilit√© CUDA)    ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  üíæ Stockage Mod√®les LLM                                   ‚îÇ
‚îÇ      ‚îî‚îÄ R√©pertoire: D:\modeles_llm\                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### üîß **Mapping GPU et Standards**

```python
# Configuration GPU OBLIGATOIRE pour NextGeneration
os.environ['CUDA_VISIBLE_DEVICES'] = '1'        # RTX 3090 (Bus PCI 1) UNIQUEMENT
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Force ordre bus physique
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:1024'

# Apr√®s mapping CUDA:
# CUDA:0 ‚Üí RTX 3090 (24GB) - GPU IA Principal
# RTX 5060 Ti ‚Üí INVISIBLE et INUTILISABLE (Standards Projet)
```

---

## üìö R√âF√âRENCES DOCUMENTAIRES OBLIGATOIRES

### ÔøΩ **Documentation Standards GPU (LECTURE OBLIGATOIRE)**

| üìñ **Document** | üéØ **Contenu** | üí° **Usage** |
|-----------------|----------------|--------------|
| **[standards_gpu_rtx3090_definitifs.md](./standards_gpu_rtx3090_definitifs.md)** | Standards GPU absolus du projet | Configuration code OBLIGATOIRE |
| **[guide_developpement_gpu_rtx3090.md](./guide_developpement_gpu_rtx3090.md)** | Guide d√©veloppement pratique | Workflow d√©veloppement |
| **[RTX_5060_CUDA_PYTORCH_INCOMPATIBILITE.md](./RTX_5060_CUDA_PYTORCH_INCOMPATIBILITE.md)** | Probl√©matique RTX 5060 | Justification exclusion RTX 5060 |

### ÔøΩ **Documentation Configuration Ollama (R√âF√âRENCE TECHNIQUE)**

| üìñ **Document** | üéØ **Contenu** | üí° **Usage** |
|-----------------|----------------|--------------|
| **[GUIDE_EXPLOITATION_OLLAMA_RTX3090_OPTIMISE.md](./GUIDE_EXPLOITATION_OLLAMA_RTX3090_OPTIMISE.md)** | Exploitation optimale Ollama | Configuration mod√®les locaux |
| **[CONFIGURATION_OLLAMA_RTX3090_NEXTGENERATION.md](./CONFIGURATION_OLLAMA_RTX3090_NEXTGENERATION.md)** | Configuration sp√©cifique projet | Param√©trage Ollama |

### üìã **Scripts et Outils de Validation**

| üîß **Script** | üéØ **Fonction** | üí° **Utilisation** |
|---------------|----------------|-------------------|
| **[monitor_rtx3090.py](./monitor_rtx3090.py)** | Monitoring RTX 3090 temps r√©el | Surveillance performances |
| **[selecteur_ollama_rtx3090.py](./selecteur_ollama_rtx3090.py)** | S√©lection automatique mod√®les | Optimisation usage mod√®les |
| **[test_configuration_multi_gpu.py](./test_configuration_multi_gpu.py)** | Validation configuration | Tests configuration syst√®me |

---

## üóÇÔ∏è MOD√àLES LLM LOCAUX - STOCKAGE ET GESTION

### **üìÅ Localisation des Mod√®les**

```
R√©pertoire Principal : D:\modeles_llm\
‚îÇ
‚îú‚îÄ‚îÄ ollama\                     # Mod√®les Ollama (format .gguf)
‚îÇ   ‚îú‚îÄ‚îÄ mixtral-8x7b\          # Mod√®le qualit√© maximum (26GB)
‚îÇ   ‚îú‚îÄ‚îÄ qwen-coder-32b\        # Sp√©cialiste code (19GB)
‚îÇ   ‚îú‚îÄ‚îÄ llama3-8b-instruct\    # Usage quotidien (6.6GB)
‚îÇ   ‚îú‚îÄ‚îÄ nous-hermes-7b\        # Ultra rapide (4.1GB)
‚îÇ   ‚îî‚îÄ‚îÄ qwen2.5-coder-1.5b\    # Tests express (1GB)
‚îÇ
‚îú‚îÄ‚îÄ transformers\              # Mod√®les HuggingFace
‚îÇ   ‚îú‚îÄ‚îÄ microsoft-dialoGPT\    # Mod√®les conversationnels
‚îÇ   ‚îú‚îÄ‚îÄ codellama\             # Mod√®les sp√©cialis√©s code
‚îÇ   ‚îî‚îÄ‚îÄ mistral-7b\            # Mod√®les Mistral
‚îÇ
‚îú‚îÄ‚îÄ custom\                    # Mod√®les personnalis√©s
‚îÇ   ‚îú‚îÄ‚îÄ fine-tuned\            # Mod√®les fine-tun√©s
‚îÇ   ‚îî‚îÄ‚îÄ specialized\           # Mod√®les sp√©cialis√©s m√©tier
‚îÇ
‚îî‚îÄ‚îÄ cache\                     # Cache mod√®les temporaires
    ‚îú‚îÄ‚îÄ downloads\             # T√©l√©chargements en cours
    ‚îî‚îÄ‚îÄ converted\             # Mod√®les convertis
```

### **üìä Mod√®les Actuellement Disponibles (Valid√©s RTX 3090)**

| Mod√®le | Taille | VRAM Requise | Usage Recommand√© | Performance |
|--------|--------|--------------|------------------|-------------|
| **mixtral-8x7b:latest** | 26GB | 26GB (100% RTX 3090) | Analyses complexes | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **qwen-coder-32b:latest** | 19GB | 19GB (80% RTX 3090) | D√©veloppement code | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **llama3:8b-instruct-q6_k** | 6.6GB | 6.6GB (28% RTX 3090) | Usage quotidien | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **nous-hermes-2-mistral-7b-dpo** | 4.1GB | 4.1GB (17% RTX 3090) | R√©ponses rapides | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **qwen2.5-coder:1.5b** | 1GB | 1GB (4% RTX 3090) | Tests express | ‚≠ê‚≠ê‚≠ê |

---

## ‚ö†Ô∏è PROC√âDURE OBLIGATOIRE POUR NOUVEAUX MOD√àLES

### **üîç √âtape 1 : Consultation Liste Mod√®les Disponibles**

**AVANT** tout t√©l√©chargement, **OBLIGATOIREMENT** consulter :

```bash
# Lister mod√®les Ollama disponibles
ollama list

# Rechercher mod√®les par crit√®re
ollama search [nom_modele]

# Afficher informations d√©taill√©es
ollama show [nom_modele]
```

### **üìã √âtape 2 : Validation Compatibilit√© RTX 3090**

**Crit√®res obligatoires** :
- **VRAM requise** ‚â§ 24GB (RTX 3090)
- **Format compatible** : GGUF, GGML, ou Transformers
- **Architecture support√©e** : Llama, Mistral, Qwen, etc.
- **Quantization** : Q4_K_M minimum (√©quilibre qualit√©/taille)

### **üì• √âtape 3 : T√©l√©chargement Autoris√©**

```bash
# T√©l√©chargement Ollama (recommand√©)
ollama pull [nom_modele]

# Exemple
ollama pull llama3.1:8b-instruct-q4_k_m
```

### **‚úÖ √âtape 4 : Test et Validation**

```python
# Test avec script de validation
python selecteur_ollama_rtx3090.py

# Test performance sp√©cifique
python test_nouveau_modele.py [nom_modele]
```

---

## üöÄ INT√âGRATION AVEC ASSISTANTS NEXTGENERATION

### **ü§ñ Architecture Assistants Locaux**

```python
# Configuration assistant avec mod√®le local
class AssistantLocal:
    def __init__(self, model_name="llama3:8b-instruct-q6_k"):
        # Configuration RTX 3090 obligatoire
        os.environ['CUDA_VISIBLE_DEVICES'] = '1'
        self.model_name = model_name
        self.ollama_url = "http://localhost:11434"
        
    async def process_query(self, query: str, context: dict = None):
        """Traite une requ√™te avec mod√®le local RTX 3090"""
        # S√©lection mod√®le optimal selon contexte
        optimal_model = self.select_optimal_model(query, context)
        
        # Traitement sur RTX 3090
        response = await self.call_local_model(optimal_model, query)
        
        return response
```

### **‚ö° S√©lection Automatique de Mod√®les**

```python
# Int√©gration s√©lecteur intelligent
from selecteur_ollama_rtx3090 import RTX3090OllamaSelector

class SmartAssistant:
    def __init__(self):
        self.selector = RTX3090OllamaSelector()
        
    async def intelligent_response(self, task: str, priority: str = "auto"):
        """R√©ponse avec s√©lection automatique de mod√®le"""
        # Analyse t√¢che et s√©lection mod√®le optimal
        analysis = self.selector.analyze_task(task, priority)
        
        # Traitement avec mod√®le s√©lectionn√©
        result = await self.selector.process_task(task, priority)
        
        return result
```

---

## üìä WORKFLOWS OPTIMIS√âS PAR USAGE

### **üîß D√©veloppement & Code**

```python
# Assistant sp√©cialis√© d√©veloppement
async def code_assistant(query: str):
    """Assistant d√©veloppement avec Qwen-Coder 32B"""
    model = "qwen-coder-32b:latest"  # 19GB VRAM
    # Sp√©cialis√© : Python, JavaScript, Debug, Architecture
    
    enhanced_prompt = f"""
    Tu es un expert d√©veloppeur sur RTX 3090.
    Contexte: Projet NextGeneration multi-agent.
    Standards: SuperWhisper V6 avec GPU RTX 3090.
    
    T√¢che: {query}
    
    Fournis du code propre, document√© et optimis√©.
    """
    
    return await call_ollama_model(model, enhanced_prompt)
```

### **üß† Analyses Complexes**

```python
# Assistant analyse avec Mixtral 8x7B
async def analysis_assistant(content: str):
    """Assistant analyse avec Mixtral (qualit√© maximum)"""
    model = "mixtral-8x7b:latest"  # 26GB VRAM (100% RTX 3090)
    
    enhanced_prompt = f"""
    Tu es un expert analyste utilisant toute la capacit√© RTX 3090.
    Contexte: Syst√®me multi-agent NextGeneration.
    
    Analyse approfondie: {content}
    
    Fournis une analyse d√©taill√©e, structur√©e et argument√©e.
    """
    
    return await call_ollama_model(model, enhanced_prompt)
```

### **‚ö° R√©ponses Rapides**

```python
# Assistant rapide avec Nous-Hermes
async def quick_assistant(question: str):
    """Assistant ultra-rapide pour interactions en temps r√©el"""
    model = "nous-hermes-2-mistral-7b-dpo:latest"  # 4.1GB VRAM
    
    return await call_ollama_model(model, question)
```

---

## üíæ GESTION MOD√àLES LLM LOCAUX

### üìÅ **R√©pertoire de Stockage Principal**

```
D:\modeles_llm\
‚îú‚îÄ‚îÄ README.md                           # Documentation r√©pertoire
‚îú‚îÄ‚îÄ modeles_ollama\                     # Mod√®les Ollama (gestion automatique)
‚îÇ   ‚îú‚îÄ‚îÄ mistral-7b-instruct\
‚îÇ   ‚îú‚îÄ‚îÄ llama3-8b-instruct\
‚îÇ   ‚îú‚îÄ‚îÄ qwen-coder-32b\
‚îÇ   ‚îú‚îÄ‚îÄ mixtral-8x7b\
‚îÇ   ‚îî‚îÄ‚îÄ nous-hermes-2-mistral-7b-dpo\
‚îú‚îÄ‚îÄ modeles_huggingface\               # Mod√®les HuggingFace manuels
‚îÇ   ‚îú‚îÄ‚îÄ microsoft--DialoGPT-medium\
‚îÇ   ‚îú‚îÄ‚îÄ sentence-transformers--all-MiniLM-L6-v2\
‚îÇ   ‚îî‚îÄ‚îÄ ... autres mod√®les HF
‚îú‚îÄ‚îÄ modeles_custom\                    # Mod√®les personnalis√©s/fine-tun√©s
‚îÇ   ‚îî‚îÄ‚îÄ (r√©serv√© pour d√©veloppements futurs)
‚îî‚îÄ‚îÄ cache\                             # Cache temporaire mod√®les
    ‚îú‚îÄ‚îÄ tokenizers\
    ‚îú‚îÄ‚îÄ transformers\
    ‚îî‚îÄ‚îÄ ollama_cache\
```

### üìã **Mod√®les Disponibles et Valid√©s**

#### ü¶ô **Mod√®les Ollama (Pr√™ts √† l'Usage)**
```yaml
modeles_ollama_valides:
  speed_model:
    nom: "nous-hermes-2-mistral-7b-dpo:latest"
    vram: "4.1GB"
    vitesse: "‚ö° Ultra-rapide (6 tokens/sec)"
    usage: "R√©ponses rapides, brainstorming, dev"
    
  quality_model:
    nom: "mixtral-8x7b:latest"
    vram: "26GB (100% RTX 3090)"
    vitesse: "üèÜ Qualit√© max (1 token/sec)"
    usage: "Analyses complexes, raisonnement avanc√©"
    
  code_model:
    nom: "qwen-coder-32b:latest"
    vram: "19GB (80% RTX 3090)"
    vitesse: "üîß Sp√©cialis√© (2 tokens/sec)"
    usage: "G√©n√©ration code, debug, architecture"
    
  daily_model:
    nom: "llama3:8b-instruct-q6_k"
    vram: "6.6GB (28% RTX 3090)"
    vitesse: "üì± √âquilibr√© (4 tokens/sec)"
    usage: "Usage quotidien, chat, r√©sum√©s"
    
  mini_model:
    nom: "qwen2.5-coder:1.5b"
    vram: "1GB (4% RTX 3090)"
    vitesse: "üß™ Express (10 tokens/sec)"
    usage: "Tests rapides, prototypage"
```

#### ü§ó **Mod√®les HuggingFace (Configuration Manuelle)**
```yaml
modeles_huggingface_disponibles:
  conversational:
    nom: "microsoft/DialoGPT-medium"
    taille: "345MB"
    usage: "Chat conversationnel"
    
  embeddings:
    nom: "sentence-transformers/all-MiniLM-L6-v2"
    taille: "80MB"
    usage: "Embeddings texte, similarity"
    
  multilingual:
    nom: "Helsinki-NLP/opus-mt-fr-en"
    taille: "300MB"
    usage: "Traduction FR-EN"
```

---

## üîÑ WORKFLOW D'UTILISATION POUR ASSISTANTS

### ü§ñ **Int√©gration avec Syst√®me Multi-Agent**

```python
# Exemple d'int√©gration dans un assistant NextGeneration
class AssistantAvecModelesLocaux:
    """Assistant utilisant mod√®les LLM locaux via RTX 3090"""
    
    def __init__(self):
        # Configuration GPU RTX 3090 OBLIGATOIRE
        os.environ['CUDA_VISIBLE_DEVICES'] = '1'
        os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
        
        self.model_selector = RTX3090OllamaSelector()
        self.models_path = "D:\\modeles_llm"
        
    async def process_request(self, task: str, priority: str = "auto"):
        """Traite une requ√™te avec s√©lection automatique de mod√®le"""
        
        # S√©lection automatique selon t√¢che
        analysis = self.model_selector.analyze_task(task, priority)
        selected_model = analysis["recommended_model"]
        
        # Traitement avec mod√®le optimal
        response = await self.model_selector.process_task(
            task=task,
            priority=priority,
            model=selected_model
        )
        
        return {
            "response": response["content"],
            "model_used": response["model"],
            "performance": response["performance"],
            "confidence": analysis["confidence"]
        }
```

### üéØ **S√©lection Automatique par Type d'Assistant**

```python
# Mapping assistants ‚Üí mod√®les optimaux
ASSISTANT_MODEL_MAPPING = {
    "peer_review_agent": "quality_model",      # Mixtral-8x7B pour analyses
    "code_agent": "code_model",                # Qwen-Coder pour d√©veloppement
    "documentation_agent": "daily_model",      # Llama3 pour r√©daction
    "security_agent": "quality_model",         # Mixtral pour analyses s√©curit√©
    "performance_agent": "code_model",         # Qwen-Coder pour optimisations
    "chat_agent": "speed_model",              # Nous-Hermes pour rapidit√©
    "test_agent": "mini_model"                # Qwen-Mini pour tests rapides
}
```

---

## üì• PROC√âDURE AJOUT NOUVEAUX MOD√àLES

### ‚ö†Ô∏è **R√àGLE OBLIGATOIRE - CONSULTATION PR√âALABLE**

> **üö® ATTENTION : Avant de t√©l√©charger un nouveau mod√®le, vous DEVEZ consulter la liste des mod√®les disponibles pour √©viter les doublons et valider la compatibilit√©.**

### üìã **√âtapes d'Ajout d'un Nouveau Mod√®le**

#### **1. Consultation Obligatoire**
```bash
# Lister mod√®les Ollama disponibles
ollama list

# Lister mod√®les locaux existants
ls -la D:\modeles_llm\modeles_ollama\

# V√©rifier espace disque disponible
df -h D:\
```

#### **2. Recherche et Validation**
```bash
# Rechercher mod√®les compatibles
ollama search <nom_modele>

# V√©rifier taille et compatibilit√© RTX 3090
ollama show <nom_modele>
```

#### **3. T√©l√©chargement S√©curis√©**
```bash
# T√©l√©charger avec validation GPU
CUDA_VISIBLE_DEVICES=1 ollama pull <nom_modele>

# Exemple
CUDA_VISIBLE_DEVICES=1 ollama pull codellama:7b-instruct
```

#### **4. Test et Validation**
```python
# Script de test nouveau mod√®le
async def test_nouveau_modele(model_name: str):
    """Test complet nouveau mod√®le sur RTX 3090"""
    
    # Test basique
    response = await process_with_model(
        model=model_name,
        prompt="Test RTX 3090 NextGeneration",
        max_tokens=100
    )
    
    # Validation performance
    benchmark = await benchmark_model_performance(model_name)
    
    return {
        "status": "validated" if benchmark["success"] else "failed",
        "vram_usage": benchmark["vram_gb"],
        "tokens_per_sec": benchmark["speed"],
        "compatibility": "rtx3090_compatible"
    }
```

#### **5. Documentation et Int√©gration**
```python
# Ajouter dans selecteur_ollama_rtx3090.py
nouveaux_modeles = {
    "custom_model": {
        "name": "nom-modele:version",
        "vram_gb": 8.5,
        "tokens_per_sec": 3,
        "quality_score": 8,
        "speciality": "Usage sp√©cialis√©",
        "use_cases": ["custom", "specific", "domain"],
        "temperature_optimal": 0.4,
        "max_tokens_optimal": 1024
    }
}
```

---

## üõ†Ô∏è OUTILS DE GESTION ET MONITORING

### üìä **Scripts de Monitoring Disponibles**

```python
# Monitoring utilisation mod√®les en temps r√©el
from monitor_rtx3090 import RTX3090Monitor

monitor = RTX3090Monitor()
stats = monitor.get_model_usage_stats()

print(f"Mod√®le actif: {stats['active_model']}")
print(f"VRAM utilis√©e: {stats['vram_used_gb']:.1f}GB / 24GB")
print(f"Temp√©rature: {stats['temperature']}¬∞C")
print(f"Utilisation: {stats['gpu_utilization']}%")
```

### üîß **Maintenance et Optimisation**

```bash
# Nettoyage cache mod√®les
ollama prune

# Lib√©ration m√©moire GPU
python -c "
import torch
torch.cuda.empty_cache()
print('‚úÖ Cache GPU nettoy√©')
"

# V√©rification sant√© mod√®les
python test_configuration_multi_gpu.py
```

---

## üéØ EXEMPLES D'USAGE PRATIQUES

### ü§ñ **Assistant Peer-Review (Qualit√© Maximale)**
```python
# Utilise Mixtral-8x7B pour analyses approfondies
peer_review_agent = AssistantAvecModelesLocaux()

review = await peer_review_agent.process_request(
    task="Analyser ce code Python et identifier les vuln√©rabilit√©s de s√©curit√©",
    priority="quality"  # Force Mixtral-8x7B
)

print(f"Review: {review['response']}")
print(f"Mod√®le utilis√©: {review['model_used']}")
```

### üíª **Assistant Code (Sp√©cialis√© D√©veloppement)**
```python
# Utilise Qwen-Coder-32B pour g√©n√©ration code
code_agent = AssistantAvecModelesLocaux()

code = await code_agent.process_request(
    task="G√©n√®re une classe Python pour g√©rer une base de donn√©es PostgreSQL",
    priority="code"  # Force Qwen-Coder-32B
)

print(f"Code g√©n√©r√©: {code['response']}")
```

### ‚ö° **Assistant Chat (Ultra-Rapide)**
```python
# Utilise Nous-Hermes pour r√©ponses instantan√©es
chat_agent = AssistantAvecModelesLocaux()

response = await chat_agent.process_request(
    task="Salut ! Comment √ßa va ?",
    priority="speed"  # Force Nous-Hermes
)

print(f"R√©ponse: {response['response']}")
```

---

## üìã CHECKLIST DE D√âPLOIEMENT

### ‚úÖ **Validation Configuration Syst√®me**
- [ ] RTX 3090 d√©tect√©e sur Bus PCI 1
- [ ] RTX 5060 Ti exclue (Bus PCI 0)
- [ ] Configuration GPU standards respect√©e
- [ ] R√©pertoire `D:\modeles_llm` accessible
- [ ] Ollama service d√©marr√© et fonctionnel

### ‚úÖ **Validation Mod√®les Locaux**
- [ ] Mod√®les Ollama list√©s et fonctionnels
- [ ] S√©lecteur automatique configur√©
- [ ] Tests performance RTX 3090 r√©ussis
- [ ] Monitoring actif et op√©rationnel

### ‚úÖ **Int√©gration Assistants**
- [ ] Mapping assistants ‚Üí mod√®les d√©fini
- [ ] Scripts d'int√©gration valid√©s
- [ ] Proc√©dures ajout nouveaux mod√®les document√©es
- [ ] Workflow complet test√© et fonctionnel

---

## üöÄ PROCHAINES √âTAPES RECOMMAND√âES

1. **Tester l'int√©gration** avec vos assistants existants
2. **Calibrer les param√®tres** selon vos besoins sp√©cifiques
3. **Monitorer les performances** en conditions r√©elles d'usage
4. **Documenter les retours** d'exp√©rience pour optimisations futures
5. **√âvaluer l'ajout** de mod√®les sp√©cialis√©s selon besoins √©mergents

---

**Ce document constitue la r√©f√©rence principale pour l'utilisation des mod√®les LLM locaux dans votre syst√®me multi-agent NextGeneration. Consultez r√©guli√®rement les documents de r√©f√©rence cit√©s pour maintenir la conformit√© avec les standards du projet.**

üéÆ‚ú® **Configuration Double GPU RTX 3090 + Mod√®les Locaux = Puissance IA Maximale !**
