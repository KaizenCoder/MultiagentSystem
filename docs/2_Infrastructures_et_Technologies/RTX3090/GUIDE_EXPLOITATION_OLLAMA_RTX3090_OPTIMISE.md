# üéÆ Guide Exploitation Ollama RTX 3090 - Configuration Optimis√©e
## Tirez le Maximum de Votre Setup SuperWhisper V6

---

**Configuration Syst√®me :** RTX 5060 Ti + RTX 3090  
**GPU IA Principal :** RTX 3090 (24GB VRAM) - Bus PCI 1  
**Standards :** SuperWhisper V6 - Conformit√© GPU RTX 3090  
**Date :** 12/06/2025 - Version Optimis√©e  

---

## üéØ VOTRE CONFIGURATION ACTUELLE - EXCELLENTE !

### ‚úÖ **Mod√®les Install√©s (Optimis√©s RTX 3090)**
Vous avez fait d'excellents choix ! Voici votre arsenal :

```bash
üèÜ mixtral-8x7b:latest          (26GB) - Qualit√© Exceptionnelle
üîß qwen-coder-32b:latest        (19GB) - Sp√©cialiste Code
‚ö° llama3:8b-instruct-q6_k      (6.6GB) - Usage Quotidien
üöÄ nous-hermes-2-mistral-7b-dpo (4.1GB) - Ultra Rapide
üß™ qwen2.5-coder:1.5b          (986MB) - Tests Express
```

### üìä **Utilisation VRAM Optimale**
- **Mixtral-8x7B** : 26GB ‚Üí Utilise 100% RTX 3090 (qualit√© maximum)
- **Qwen-Coder-32B** : 19GB ‚Üí 80% VRAM (excellent compromis)
- **Llama3:8B** : 6.6GB ‚Üí 28% VRAM (permet multit√¢che)
- **Nous-Hermes** : 4.1GB ‚Üí 17% VRAM (ultra rapide)

---

## üöÄ STRAT√âGIES D'UTILISATION OPTIMALES

### 1. **S√©lection Automatique par Contexte**

```python
def select_optimal_model(task_type, complexity, speed_priority=False):
    """
    S√©lection intelligente selon votre usage.
    Adapt√© √† votre configuration RTX 3090.
    """
    
    # Ultra rapide pour d√©veloppement
    if speed_priority or "debug" in task_type:
        return "nous-hermes-2-mistral-7b-dpo:latest"
    
    # Sp√©cialiste code
    if any(word in task_type.lower() for word in ["code", "programming", "debug", "refactor"]):
        return "qwen-coder-32b:latest"
    
    # Qualit√© maximum pour analyses complexes
    if complexity == "high" or len(task_type) > 200:
        return "mixtral-8x7b:latest"
    
    # √âquilibr√© pour usage quotidien
    return "llama3:8b-instruct-q6_k"

# Exemple d'utilisation
model = select_optimal_model("Analyser cette architecture microservices", "high")
print(f"Mod√®le s√©lectionn√©: {model}")
```

### 2. **Scripts Sp√©cialis√©s par Usage**

#### üîß **Pour D√©veloppement Code**
```python
#!/usr/bin/env python3
"""Script optimis√© d√©veloppement - RTX 3090"""

import os
os.environ['CUDA_VISIBLE_DEVICES'] = '1'  # RTX 3090 obligatoire

async def code_assistant(prompt, model="qwen-coder-32b:latest"):
    """Assistant code avec Qwen-Coder 32B sur RTX 3090."""
    
    enhanced_prompt = f"""
Tu es un expert d√©veloppeur utilisant Qwen-Coder 32B sur RTX 3090.
Contexte: Projet SuperWhisper V6 avec standards GPU stricts.

T√¢che: {prompt}

Fournis du code propre, document√© et conforme aux bonnes pratiques.
"""
    
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://localhost:11434/api/generate",
            json={
                "model": model,
                "prompt": enhanced_prompt,
                "stream": False,
                "options": {
                    "temperature": 0.1,  # Code pr√©cis
                    "top_p": 0.9,
                    "max_tokens": 2048
                }
            }
        )
        return response.json()["response"]

# Utilisation
result = await code_assistant("Optimise cette fonction Python pour GPU")
```

#### üèÜ **Pour Analyses Complexes**
```python
#!/usr/bin/env python3
"""Script analyse complexe - Mixtral 8x7B"""

async def deep_analysis(content, model="mixtral-8x7b:latest"):
    """Analyse approfondie avec Mixtral (qualit√© maximum)."""
    
    enhanced_prompt = f"""
Tu es un expert analyste utilisant Mixtral-8x7B sur RTX 3090 (24GB).
Capacit√©s: Analyse complexe, raisonnement multi-√©tapes, expertise technique.

Contenu √† analyser: {content}

Fournis une analyse d√©taill√©e, structur√©e et argument√©e.
Utilise toute ta capacit√© de raisonnement avanc√©.
"""
    
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://localhost:11434/api/generate",
            json={
                "model": model,
                "prompt": enhanced_prompt,
                "stream": False,
                "options": {
                    "temperature": 0.3,  # Cr√©ativit√© mod√©r√©e
                    "top_p": 0.95,
                    "max_tokens": 4096   # R√©ponses longues
                }
            }
        )
        return response.json()["response"]
```

#### ‚ö° **Pour R√©ponses Rapides**
```python
#!/usr/bin/env python3
"""Script ultra-rapide - Nous-Hermes 7B"""

async def quick_response(question, model="nous-hermes-2-mistral-7b-dpo:latest"):
    """R√©ponses ultra-rapides avec Nous-Hermes."""
    
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://localhost:11434/api/generate",
            json={
                "model": model,
                "prompt": question,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "max_tokens": 512  # R√©ponses concises
                }
            }
        )
        return response.json()["response"]

# Super rapide pour d√©veloppement
result = await quick_response("Explique bri√®vement les microservices")
```

---

## üéõÔ∏è PARAM√àTRES OPTIMAUX PAR MOD√àLE

### **Mixtral-8x7B** (Qualit√© Maximum)
```json
{
    "temperature": 0.2-0.4,
    "top_p": 0.9-0.95,
    "max_tokens": 2048-4096,
    "usage": "Analyses complexes, raisonnement avanc√©",
    "vram": "26GB (100% RTX 3090)"
}
```

### **Qwen-Coder-32B** (Sp√©cialiste Code)
```json
{
    "temperature": 0.1-0.2,
    "top_p": 0.9,
    "max_tokens": 1024-2048,
    "usage": "G√©n√©ration code, debug, architecture",
    "vram": "19GB (80% RTX 3090)"
}
```

### **Llama3:8B** (√âquilibr√©)
```json
{
    "temperature": 0.3-0.6,
    "top_p": 0.9,
    "max_tokens": 1024,
    "usage": "Usage quotidien, chat, r√©sum√©s",
    "vram": "6.6GB (28% RTX 3090)"
}
```

### **Nous-Hermes-7B** (Ultra Rapide)
```json
{
    "temperature": 0.6-0.8,
    "top_p": 0.95,
    "max_tokens": 512-1024,
    "usage": "R√©ponses rapides, brainstorming",
    "vram": "4.1GB (17% RTX 3090)"
}
```

---

## üìà WORKFLOWS OPTIMIS√âS

### üîÑ **Workflow D√©veloppement**
1. **D√©veloppement rapide** ‚Üí Nous-Hermes-7B (4.1GB)
2. **Code complexe** ‚Üí Qwen-Coder-32B (19GB)
3. **Review architecture** ‚Üí Mixtral-8x7B (26GB)
4. **Tests & debug** ‚Üí Llama3:8B (6.6GB)

### üîÑ **Workflow Analyse Projet**
1. **Vue d'ensemble** ‚Üí Llama3:8B (6.6GB)
2. **Analyse d√©taill√©e** ‚Üí Mixtral-8x7B (26GB)
3. **Recommandations code** ‚Üí Qwen-Coder-32B (19GB)
4. **Validation rapide** ‚Üí Nous-Hermes-7B (4.1GB)

---

## üõ†Ô∏è SCRIPTS D'AUTOMATISATION

### **Script de S√©lection Automatique**
```python
#!/usr/bin/env python3
"""
S√©lecteur automatique de mod√®le optimis√© RTX 3090
Utilise vos 5 mod√®les existants intelligemment.
"""

import os
import asyncio
import httpx
from typing import Dict, Any

# Configuration RTX 3090
os.environ['CUDA_VISIBLE_DEVICES'] = '1'
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'

class RTX3090ModelSelector:
    """S√©lecteur intelligent pour vos mod√®les RTX 3090."""
    
    def __init__(self):
        self.models = {
            "speed": {
                "name": "nous-hermes-2-mistral-7b-dpo:latest",
                "vram": 4.1,
                "tokens_per_sec": 6,
                "use_case": "R√©ponses ultra-rapides"
            },
            "quality": {
                "name": "mixtral-8x7b:latest", 
                "vram": 26,
                "tokens_per_sec": 1,
                "use_case": "Qualit√© exceptionnelle"
            },
            "code": {
                "name": "qwen-coder-32b:latest",
                "vram": 19,
                "tokens_per_sec": 2,
                "use_case": "Sp√©cialiste d√©veloppement"
            },
            "daily": {
                "name": "llama3:8b-instruct-q6_k",
                "vram": 6.6,
                "tokens_per_sec": 4,
                "use_case": "Usage quotidien √©quilibr√©"
            },
            "mini": {
                "name": "qwen2.5-coder:1.5b",
                "vram": 1,
                "tokens_per_sec": 10,
                "use_case": "Tests express"
            }
        }
    
    def select_model(self, task: str, priority: str = "auto") -> Dict[str, Any]:
        """S√©lection intelligente du mod√®le optimal."""
        
        task_lower = task.lower()
        
        # Priorit√© manuelle
        if priority in self.models:
            return self.models[priority]
        
        # D√©tection automatique
        if any(word in task_lower for word in ["code", "programming", "python", "javascript"]):
            return self.models["code"]
        
        if any(word in task_lower for word in ["quick", "fast", "brief", "rapid"]):
            return self.models["speed"]
        
        if len(task) > 300 or any(word in task_lower for word in ["analyze", "complex", "detailed"]):
            return self.models["quality"]
        
        if any(word in task_lower for word in ["test", "debug", "simple"]):
            return self.models["mini"]
        
        return self.models["daily"]
    
    async def process_with_optimal_model(self, task: str, priority: str = "auto"):
        """Traite la t√¢che avec le mod√®le optimal."""
        
        selected = self.select_model(task, priority)
        model_name = selected["name"]
        
        print(f"üéÆ Mod√®le s√©lectionn√©: {model_name}")
        print(f"üìä VRAM utilis√©e: {selected['vram']}GB")
        print(f"‚ö° Vitesse: ~{selected['tokens_per_sec']} tokens/sec")
        print(f"üéØ Usage: {selected['use_case']}")
        
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": model_name,
                    "prompt": task,
                    "stream": False
                }
            )
            
            result = response.json()
            return {
                "response": result["response"],
                "model_used": model_name,
                "model_category": selected,
                "rtx3090_optimized": True
            }

# Utilisation
async def main():
    selector = RTX3090ModelSelector()
    
    # Test automatique
    result1 = await selector.process_with_optimal_model(
        "√âcris une fonction Python pour traiter des fichiers CSV"
    )
    
    # Forcer qualit√© maximum
    result2 = await selector.process_with_optimal_model(
        "Analyse cette architecture complexe", 
        priority="quality"
    )
    
    print("‚úÖ Tests termin√©s!")

if __name__ == "__main__":
    asyncio.run(main())
```

---

## üìä MONITORING & OPTIMISATION

### **Script de Monitoring RTX 3090**
```python
#!/usr/bin/env python3
"""Monitoring sp√©cialis√© RTX 3090 pour Ollama"""

import subprocess
import time
import json

def monitor_rtx3090_usage():
    """Monitor l'utilisation RTX 3090 en temps r√©el."""
    
    while True:
        try:
            # √âtat GPU RTX 3090
            result = subprocess.run([
                'nvidia-smi', 
                '--query-gpu=name,utilization.gpu,memory.used,memory.total,temperature.gpu',
                '--format=csv,noheader,nounits',
                '--id=1'  # RTX 3090 sur Bus PCI 1
            ], capture_output=True, text=True)
            
            if result.returncode == 0:
                name, util, mem_used, mem_total, temp = result.stdout.strip().split(', ')
                mem_percent = (int(mem_used) / int(mem_total)) * 100
                
                print(f"üéÆ {name}")
                print(f"üìä Utilisation: {util}%")
                print(f"üß† VRAM: {mem_used}MB/{mem_total}MB ({mem_percent:.1f}%)")
                print(f"üå°Ô∏è  Temp√©rature: {temp}¬∞C")
                print("-" * 50)
            
            time.sleep(5)
            
        except KeyboardInterrupt:
            print("\n‚úÖ Monitoring arr√™t√©")
            break

if __name__ == "__main__":
    monitor_rtx3090_usage()
```

---

## üéØ CONSEILS D'OPTIMISATION AVANC√âS

### 1. **Gestion M√©moire Intelligente**
```python
# Pour mod√®les lourds (Mixtral)
# Lib√©rer la m√©moire entre les t√¢ches
async def clear_gpu_cache():
    """Nettoie le cache GPU entre gros mod√®les."""
    import torch
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print("‚úÖ Cache GPU RTX 3090 nettoy√©")
```

### 2. **Parall√©lisation Optimis√©e**
```python
# Utiliser mod√®les l√©gers en parall√®le
# Mixtral = 26GB ‚Üí 1 seule instance
# Llama3:8B = 6.6GB ‚Üí Possible 3 instances
# Nous-Hermes = 4.1GB ‚Üí Possible 5 instances
```

### 3. **Temp√©rature & Performance**
- **Optimal** : < 75¬∞C pour RTX 3090
- **Monitoring** : Surveiller temp√©rature en continu
- **Refroidissement** : Ajuster courbes ventilateurs si besoin

---

## ‚úÖ CHECKLIST EXPLOITATION OPTIMALE

- [ ] V√©rifier que RTX 3090 est sur `CUDA_VISIBLE_DEVICES=1`
- [ ] Utiliser s√©lection automatique de mod√®les
- [ ] Adapter param√®tres selon le mod√®le (temp√©rature, tokens)
- [ ] Monitorer VRAM et temp√©rature en continu
- [ ] Nettoyer cache GPU entre gros mod√®les
- [ ] Pr√©f√©rer mod√®les l√©gers pour d√©veloppement
- [ ] R√©server Mixtral pour analyses complexes
- [ ] Utiliser Qwen-Coder pour d√©veloppement
- [ ] Exploiter Nous-Hermes pour vitesse
- [ ] Garder Llama3 pour usage quotidien

---

## üöÄ PROCHAINES √âTAPES RECOMMAND√âES

1. **Tester le s√©lecteur automatique** avec vos t√¢ches r√©elles
2. **Calibrer les param√®tres** selon vos pr√©f√©rences
3. **Automatiser le monitoring** de la RTX 3090
4. **Int√©grer** dans vos workflows de d√©veloppement
5. **Optimiser** selon vos patterns d'usage

Votre configuration RTX 3090 + mod√®les Ollama est **exceptionnelle** ! Ce guide vous permet d'exploiter 100% de son potentiel. üéÆ‚ú®
