# 🎮 Guide Exploitation Ollama RTX 3090 - Configuration Optimisée
## Tirez le Maximum de Votre Setup SuperWhisper V6

---

**Configuration Système :** RTX 5060 Ti + RTX 3090  
**GPU IA Principal :** RTX 3090 (24GB VRAM) - Bus PCI 1  
**Standards :** SuperWhisper V6 - Conformité GPU RTX 3090  
**Date :** 12/06/2025 - Version Optimisée  

---

## 🎯 VOTRE CONFIGURATION ACTUELLE - EXCELLENTE !

### ✅ **Modèles Installés (Optimisés RTX 3090)**
Vous avez fait d'excellents choix ! Voici votre arsenal :

```bash
🏆 mixtral-8x7b:latest          (26GB) - Qualité Exceptionnelle
🔧 qwen-coder-32b:latest        (19GB) - Spécialiste Code
⚡ llama3:8b-instruct-q6_k      (6.6GB) - Usage Quotidien
🚀 nous-hermes-2-mistral-7b-dpo (4.1GB) - Ultra Rapide
🧪 qwen2.5-coder:1.5b          (986MB) - Tests Express
```

### 📊 **Utilisation VRAM Optimale**
- **Mixtral-8x7B** : 26GB → Utilise 100% RTX 3090 (qualité maximum)
- **Qwen-Coder-32B** : 19GB → 80% VRAM (excellent compromis)
- **Llama3:8B** : 6.6GB → 28% VRAM (permet multitâche)
- **Nous-Hermes** : 4.1GB → 17% VRAM (ultra rapide)

---

## 🚀 STRATÉGIES D'UTILISATION OPTIMALES

### 1. **Sélection Automatique par Contexte**

```python
def select_optimal_model(task_type, complexity, speed_priority=False):
    """
    Sélection intelligente selon votre usage.
    Adapté à votre configuration RTX 3090.
    """
    
    # Ultra rapide pour développement
    if speed_priority or "debug" in task_type:
        return "nous-hermes-2-mistral-7b-dpo:latest"
    
    # Spécialiste code
    if any(word in task_type.lower() for word in ["code", "programming", "debug", "refactor"]):
        return "qwen-coder-32b:latest"
    
    # Qualité maximum pour analyses complexes
    if complexity == "high" or len(task_type) > 200:
        return "mixtral-8x7b:latest"
    
    # Équilibré pour usage quotidien
    return "llama3:8b-instruct-q6_k"

# Exemple d'utilisation
model = select_optimal_model("Analyser cette architecture microservices", "high")
print(f"Modèle sélectionné: {model}")
```

### 2. **Scripts Spécialisés par Usage**

#### 🔧 **Pour Développement Code**
```python
#!/usr/bin/env python3
"""Script optimisé développement - RTX 3090"""

import os
os.environ['CUDA_VISIBLE_DEVICES'] = '1'  # RTX 3090 obligatoire

async def code_assistant(prompt, model="qwen-coder-32b:latest"):
    """Assistant code avec Qwen-Coder 32B sur RTX 3090."""
    
    enhanced_prompt = f"""
Tu es un expert développeur utilisant Qwen-Coder 32B sur RTX 3090.
Contexte: Projet SuperWhisper V6 avec standards GPU stricts.

Tâche: {prompt}

Fournis du code propre, documenté et conforme aux bonnes pratiques.
"""
    
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://localhost:11434/api/generate",
            json={
                "model": model,
                "prompt": enhanced_prompt,
                "stream": False,
                "options": {
                    "temperature": 0.1,  # Code précis
                    "top_p": 0.9,
                    "max_tokens": 2048
                }
            }
        )
        return response.json()["response"]

# Utilisation
result = await code_assistant("Optimise cette fonction Python pour GPU")
```

#### 🏆 **Pour Analyses Complexes**
```python
#!/usr/bin/env python3
"""Script analyse complexe - Mixtral 8x7B"""

async def deep_analysis(content, model="mixtral-8x7b:latest"):
    """Analyse approfondie avec Mixtral (qualité maximum)."""
    
    enhanced_prompt = f"""
Tu es un expert analyste utilisant Mixtral-8x7B sur RTX 3090 (24GB).
Capacités: Analyse complexe, raisonnement multi-étapes, expertise technique.

Contenu à analyser: {content}

Fournis une analyse détaillée, structurée et argumentée.
Utilise toute ta capacité de raisonnement avancé.
"""
    
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://localhost:11434/api/generate",
            json={
                "model": model,
                "prompt": enhanced_prompt,
                "stream": False,
                "options": {
                    "temperature": 0.3,  # Créativité modérée
                    "top_p": 0.95,
                    "max_tokens": 4096   # Réponses longues
                }
            }
        )
        return response.json()["response"]
```

#### ⚡ **Pour Réponses Rapides**
```python
#!/usr/bin/env python3
"""Script ultra-rapide - Nous-Hermes 7B"""

async def quick_response(question, model="nous-hermes-2-mistral-7b-dpo:latest"):
    """Réponses ultra-rapides avec Nous-Hermes."""
    
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://localhost:11434/api/generate",
            json={
                "model": model,
                "prompt": question,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "max_tokens": 512  # Réponses concises
                }
            }
        )
        return response.json()["response"]

# Super rapide pour développement
result = await quick_response("Explique brièvement les microservices")
```

---

## 🎛️ PARAMÈTRES OPTIMAUX PAR MODÈLE

### **Mixtral-8x7B** (Qualité Maximum)
```json
{
    "temperature": 0.2-0.4,
    "top_p": 0.9-0.95,
    "max_tokens": 2048-4096,
    "usage": "Analyses complexes, raisonnement avancé",
    "vram": "26GB (100% RTX 3090)"
}
```

### **Qwen-Coder-32B** (Spécialiste Code)
```json
{
    "temperature": 0.1-0.2,
    "top_p": 0.9,
    "max_tokens": 1024-2048,
    "usage": "Génération code, debug, architecture",
    "vram": "19GB (80% RTX 3090)"
}
```

### **Llama3:8B** (Équilibré)
```json
{
    "temperature": 0.3-0.6,
    "top_p": 0.9,
    "max_tokens": 1024,
    "usage": "Usage quotidien, chat, résumés",
    "vram": "6.6GB (28% RTX 3090)"
}
```

### **Nous-Hermes-7B** (Ultra Rapide)
```json
{
    "temperature": 0.6-0.8,
    "top_p": 0.95,
    "max_tokens": 512-1024,
    "usage": "Réponses rapides, brainstorming",
    "vram": "4.1GB (17% RTX 3090)"
}
```

---

## 📈 WORKFLOWS OPTIMISÉS

### 🔄 **Workflow Développement**
1. **Développement rapide** → Nous-Hermes-7B (4.1GB)
2. **Code complexe** → Qwen-Coder-32B (19GB)
3. **Review architecture** → Mixtral-8x7B (26GB)
4. **Tests & debug** → Llama3:8B (6.6GB)

### 🔄 **Workflow Analyse Projet**
1. **Vue d'ensemble** → Llama3:8B (6.6GB)
2. **Analyse détaillée** → Mixtral-8x7B (26GB)
3. **Recommandations code** → Qwen-Coder-32B (19GB)
4. **Validation rapide** → Nous-Hermes-7B (4.1GB)

---

## 🛠️ SCRIPTS D'AUTOMATISATION

### **Script de Sélection Automatique**
```python
#!/usr/bin/env python3
"""
Sélecteur automatique de modèle optimisé RTX 3090
Utilise vos 5 modèles existants intelligemment.
"""

import os
import asyncio
import httpx
from typing import Dict, Any

# Configuration RTX 3090
os.environ['CUDA_VISIBLE_DEVICES'] = '1'
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'

class RTX3090ModelSelector:
    """Sélecteur intelligent pour vos modèles RTX 3090."""
    
    def __init__(self):
        self.models = {
            "speed": {
                "name": "nous-hermes-2-mistral-7b-dpo:latest",
                "vram": 4.1,
                "tokens_per_sec": 6,
                "use_case": "Réponses ultra-rapides"
            },
            "quality": {
                "name": "mixtral-8x7b:latest", 
                "vram": 26,
                "tokens_per_sec": 1,
                "use_case": "Qualité exceptionnelle"
            },
            "code": {
                "name": "qwen-coder-32b:latest",
                "vram": 19,
                "tokens_per_sec": 2,
                "use_case": "Spécialiste développement"
            },
            "daily": {
                "name": "llama3:8b-instruct-q6_k",
                "vram": 6.6,
                "tokens_per_sec": 4,
                "use_case": "Usage quotidien équilibré"
            },
            "mini": {
                "name": "qwen2.5-coder:1.5b",
                "vram": 1,
                "tokens_per_sec": 10,
                "use_case": "Tests express"
            }
        }
    
    def select_model(self, task: str, priority: str = "auto") -> Dict[str, Any]:
        """Sélection intelligente du modèle optimal."""
        
        task_lower = task.lower()
        
        # Priorité manuelle
        if priority in self.models:
            return self.models[priority]
        
        # Détection automatique
        if any(word in task_lower for word in ["code", "programming", "python", "javascript"]):
            return self.models["code"]
        
        if any(word in task_lower for word in ["quick", "fast", "brief", "rapid"]):
            return self.models["speed"]
        
        if len(task) > 300 or any(word in task_lower for word in ["analyze", "complex", "detailed"]):
            return self.models["quality"]
        
        if any(word in task_lower for word in ["test", "debug", "simple"]):
            return self.models["mini"]
        
        return self.models["daily"]
    
    async def process_with_optimal_model(self, task: str, priority: str = "auto"):
        """Traite la tâche avec le modèle optimal."""
        
        selected = self.select_model(task, priority)
        model_name = selected["name"]
        
        print(f"🎮 Modèle sélectionné: {model_name}")
        print(f"📊 VRAM utilisée: {selected['vram']}GB")
        print(f"⚡ Vitesse: ~{selected['tokens_per_sec']} tokens/sec")
        print(f"🎯 Usage: {selected['use_case']}")
        
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": model_name,
                    "prompt": task,
                    "stream": False
                }
            )
            
            result = response.json()
            return {
                "response": result["response"],
                "model_used": model_name,
                "model_category": selected,
                "rtx3090_optimized": True
            }

# Utilisation
async def main():
    selector = RTX3090ModelSelector()
    
    # Test automatique
    result1 = await selector.process_with_optimal_model(
        "Écris une fonction Python pour traiter des fichiers CSV"
    )
    
    # Forcer qualité maximum
    result2 = await selector.process_with_optimal_model(
        "Analyse cette architecture complexe", 
        priority="quality"
    )
    
    print("✅ Tests terminés!")

if __name__ == "__main__":
    asyncio.run(main())
```

---

## 📊 MONITORING & OPTIMISATION

### **Script de Monitoring RTX 3090**
```python
#!/usr/bin/env python3
"""Monitoring spécialisé RTX 3090 pour Ollama"""

import subprocess
import time
import json

def monitor_rtx3090_usage():
    """Monitor l'utilisation RTX 3090 en temps réel."""
    
    while True:
        try:
            # État GPU RTX 3090
            result = subprocess.run([
                'nvidia-smi', 
                '--query-gpu=name,utilization.gpu,memory.used,memory.total,temperature.gpu',
                '--format=csv,noheader,nounits',
                '--id=1'  # RTX 3090 sur Bus PCI 1
            ], capture_output=True, text=True)
            
            if result.returncode == 0:
                name, util, mem_used, mem_total, temp = result.stdout.strip().split(', ')
                mem_percent = (int(mem_used) / int(mem_total)) * 100
                
                print(f"🎮 {name}")
                print(f"📊 Utilisation: {util}%")
                print(f"🧠 VRAM: {mem_used}MB/{mem_total}MB ({mem_percent:.1f}%)")
                print(f"🌡️  Température: {temp}°C")
                print("-" * 50)
            
            time.sleep(5)
            
        except KeyboardInterrupt:
            print("\n✅ Monitoring arrêté")
            break

if __name__ == "__main__":
    monitor_rtx3090_usage()
```

---

## 🎯 CONSEILS D'OPTIMISATION AVANCÉS

### 1. **Gestion Mémoire Intelligente**
```python
# Pour modèles lourds (Mixtral)
# Libérer la mémoire entre les tâches
async def clear_gpu_cache():
    """Nettoie le cache GPU entre gros modèles."""
    import torch
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print("✅ Cache GPU RTX 3090 nettoyé")
```

### 2. **Parallélisation Optimisée**
```python
# Utiliser modèles légers en parallèle
# Mixtral = 26GB → 1 seule instance
# Llama3:8B = 6.6GB → Possible 3 instances
# Nous-Hermes = 4.1GB → Possible 5 instances
```

### 3. **Température & Performance**
- **Optimal** : < 75°C pour RTX 3090
- **Monitoring** : Surveiller température en continu
- **Refroidissement** : Ajuster courbes ventilateurs si besoin

---

## ✅ CHECKLIST EXPLOITATION OPTIMALE

- [ ] Vérifier que RTX 3090 est sur `CUDA_VISIBLE_DEVICES=1`
- [ ] Utiliser sélection automatique de modèles
- [ ] Adapter paramètres selon le modèle (température, tokens)
- [ ] Monitorer VRAM et température en continu
- [ ] Nettoyer cache GPU entre gros modèles
- [ ] Préférer modèles légers pour développement
- [ ] Réserver Mixtral pour analyses complexes
- [ ] Utiliser Qwen-Coder pour développement
- [ ] Exploiter Nous-Hermes pour vitesse
- [ ] Garder Llama3 pour usage quotidien

---

## 🚀 PROCHAINES ÉTAPES RECOMMANDÉES

1. **Tester le sélecteur automatique** avec vos tâches réelles
2. **Calibrer les paramètres** selon vos préférences
3. **Automatiser le monitoring** de la RTX 3090
4. **Intégrer** dans vos workflows de développement
5. **Optimiser** selon vos patterns d'usage

Votre configuration RTX 3090 + modèles Ollama est **exceptionnelle** ! Ce guide vous permet d'exploiter 100% de son potentiel. 🎮✨
