# üöÄ Guide des Mod√®les LLM Locaux pour RTX 3090

## üéØ Recommandations Optimis√©es RTX 3090 (24GB VRAM)

### üèÜ Mod√®les TOP Recommand√©s

#### 1. **Llama 3.1 8B Instruct** (Recommand√© #1)
```bash
Taille: ~5GB VRAM
Performance: Excellent rapport qualit√©/vitesse
Temps: ~2-5 tokens/seconde
Usage VRAM: 20-25% (tr√®s √©conome)
```
**Pourquoi :** Mod√®le Meta ultra-optimis√©, excellent pour la plupart des t√¢ches

#### 2. **Mistral 7B Instruct v0.3** (Recommand√© #2)
```bash
Taille: ~4.5GB VRAM
Performance: Tr√®s bon, sp√©cialis√© code
Temps: ~3-6 tokens/seconde
Usage VRAM: 18-22%
```
**Pourquoi :** Excellent pour le code, moins censur√©, tr√®s rapide

#### 3. **CodeLlama 13B Instruct** (Sp√©cialis√© Code)
```bash
Taille: ~8GB VRAM
Performance: Excellent pour programmation
Temps: ~1-3 tokens/seconde
Usage VRAM: 35-40%
```
**Pourquoi :** Sp√©cialement con√ßu pour la g√©n√©ration de code

#### 4. **Llama 3.1 70B Instruct (Quantized 4-bit)** (Maximum Qualit√©)
```bash
Taille: ~20-22GB VRAM
Performance: Qualit√© proche GPT-4
Temps: ~0.5-1 token/seconde
Usage VRAM: 85-95%
```
**Pourquoi :** Maximum de qualit√© possible sur RTX 3090

### üìä Tableau Comparatif

| Mod√®le | VRAM | Vitesse | Qualit√© | Sp√©cialit√© |
|--------|------|---------|---------|------------|
| **Llama 3.1 8B** | 5GB | ‚ö°‚ö°‚ö° | üåüüåüüåüüåü | G√©n√©raliste |
| **Mistral 7B** | 4.5GB | ‚ö°‚ö°‚ö°‚ö° | üåüüåüüåü | Code/Tech |
| **CodeLlama 13B** | 8GB | ‚ö°‚ö° | üåüüåüüåüüåü | Programmation |
| **Llama 70B (4-bit)** | 22GB | ‚ö° | üåüüåüüåüüåüüåü | Qualit√© Max |

---

## üõ†Ô∏è Installation et Configuration

### 1. Installation d'Ollama (Recommand√©)

```powershell
# Installation Ollama (gestionnaire mod√®les local)
# T√©l√©charger depuis: https://ollama.ai/download/windows

# Apr√®s installation, t√©l√©charger les mod√®les:
ollama pull llama3.1:8b-instruct-q4_K_M
ollama pull mistral:7b-instruct-v0.3
ollama pull codellama:13b-instruct
```

### 2. Alternative : LM Studio (Interface Graphique)

```bash
# T√©l√©charger LM Studio: https://lmstudio.ai/
# Interface graphique conviviale
# Gestion automatique VRAM
# Compatible RTX 3090
```

### 3. R√©pertoire de Stockage

```powershell
# Cr√©er le r√©pertoire
New-Item -ItemType Directory -Path "D:\modeles_llm" -Force

# Configuration Ollama pour utiliser ce r√©pertoire
$env:OLLAMA_MODELS = "D:\modeles_llm"
```

---

## üîß Configuration Optimale RTX 3090

### Param√®tres GPU Recommand√©s

```json
{
  "gpu_layers": -1,
  "context_length": 4096,
  "batch_size": 512,
  "threads": 8,
  "gpu_memory_fraction": 0.9,
  "quantization": "q4_K_M"
}
```

### Scripts de Benchmark

```python
# test_gpu_performance.py
import time
import psutil
import GPUtil

def benchmark_model(model_name):
    """Benchmark d'un mod√®le local."""
    
    prompts = [
        "Explique l'intelligence artificielle en 2 phrases",
        "√âcris une fonction Python pour trier une liste",
        "Analyse les avantages du cloud computing"
    ]
    
    for prompt in prompts:
        start_time = time.time()
        
        # Appel au mod√®le local (via Ollama)
        response = call_local_model(model_name, prompt)
        
        end_time = time.time()
        tokens = len(response.split())
        speed = tokens / (end_time - start_time)
        
        print(f"Model: {model_name}")
        print(f"Prompt: {prompt[:30]}...")
        print(f"Speed: {speed:.1f} tokens/sec")
        print(f"VRAM: {get_gpu_usage():.1f}%")
        print("-" * 40)
```

---

## ‚ö° Optimisations Sp√©cifiques RTX 3090

### 1. Configuration CUDA

```bash
# V√©rifier CUDA
nvidia-smi

# Variables d'environnement optimales
set CUDA_VISIBLE_DEVICES=0
set PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024
```

### 2. Param√®tres M√©moire

```python
# Configuration optimale pour 24GB
config = {
    "max_memory": "22GB",  # Garde 2GB pour le syst√®me
    "offload_folder": "D:/temp_offload",
    "quantization": "4bit",  # √âconomise 50% VRAM
    "attention_implementation": "flash_attention_2"
}
```

### 3. Mod√®les Multiples Simultan√©s

Avec 24GB, vous pouvez faire tourner :
- **2 mod√®les 7B simultan√©ment** (pour comparaison)
- **1 mod√®le 13B + 1 mod√®le 7B**
- **1 seul mod√®le 70B quantized pour qualit√© max**

---

## üîó Int√©gration avec l'Orchestrateur

### Configuration dans config.py

```python
# Ajouter dans orchestrator/app/config.py

class Settings(BaseSettings):
    # ...existing code...
    
    # Configuration mod√®les locaux
    LOCAL_MODELS_ENABLED: bool = True
    LOCAL_MODELS_PATH: str = "D:/modeles_llm"
    OLLAMA_BASE_URL: str = "http://localhost:11434"
    
    # Mod√®les disponibles
    LOCAL_MODELS: List[str] = [
        "llama3.1:8b-instruct-q4_K_M",
        "mistral:7b-instruct-v0.3", 
        "codellama:13b-instruct"
    ]
```

### Worker Local

```python
# orchestrator/app/agents/local_worker.py

import httpx
from typing import Dict, Any

class LocalLLMWorker(BaseWorker):
    """Worker pour mod√®les LLM locaux via Ollama."""
    
    def __init__(self, config):
        super().__init__(config)
        self.ollama_url = config.OLLAMA_BASE_URL
        self.available_models = config.LOCAL_MODELS
    
    async def can_handle_task(self, task: str, requirements: List[str]) -> bool:
        """Pr√©f√®re les mod√®les locaux pour confidentialit√©."""
        
        if "local" in requirements:
            return True
        if "confidential" in requirements:
            return True
        if "fast" in requirements:
            return True  # Mod√®les locaux souvent plus rapides
            
        return False
    
    async def process_task(self, task: str, requirements: List[str] = None) -> Dict[str, Any]:
        """Traite avec mod√®le local optimal."""
        
        # S√©lection mod√®le selon t√¢che
        if "code" in task.lower():
            model = "codellama:13b-instruct"
        elif "quick" in requirements:
            model = "mistral:7b-instruct-v0.3"
        else:
            model = "llama3.1:8b-instruct-q4_K_M"
        
        try:
            response = await self._call_ollama(model, task)
            
            return {
                "result": response,
                "model_used": model,
                "agent_type": "local_llm",
                "privacy": "local_processing",
                "cost": "free"
            }
            
        except Exception as e:
            return {
                "result": f"Erreur mod√®le local: {str(e)}",
                "success": False
            }
    
    async def _call_ollama(self, model: str, prompt: str) -> str:
        """Appel √† Ollama."""
        
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.7,
                "top_p": 0.9,
                "num_ctx": 4096
            }
        }
        
        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(
                f"{self.ollama_url}/api/generate",
                json=payload
            )
            
            result = response.json()
            return result["response"]
```

---

## üìã Plan d'Installation √âtape par √âtape

### √âtape 1: Pr√©paration
```powershell
# Cr√©er r√©pertoire
New-Item -ItemType Directory -Path "D:\modeles_llm" -Force

# V√©rifier CUDA
nvidia-smi
```

### √âtape 2: Installation Ollama
```powershell
# T√©l√©charger et installer Ollama
# https://ollama.ai/download/windows

# Configurer r√©pertoire
$env:OLLAMA_MODELS = "D:\modeles_llm"
```

### √âtape 3: T√©l√©chargement Mod√®les
```powershell
# Mod√®les recommand√©s (dans l'ordre)
ollama pull llama3.1:8b-instruct-q4_K_M      # ~5GB
ollama pull mistral:7b-instruct-v0.3          # ~4.5GB  
ollama pull codellama:13b-instruct            # ~8GB
```

### √âtape 4: Tests
```powershell
# Test basique
ollama run llama3.1:8b-instruct-q4_K_M "Bonjour, peux-tu me dire l'heure ?"
```

---

## üéØ Recommandation Finale

**Pour commencer, je recommande :**

1. **Llama 3.1 8B Instruct** - Le meilleur √©quilibre qualit√©/vitesse
2. **Mistral 7B** - Excellent pour le code et tr√®s rapide
3. Si vous avez besoin de qualit√© max : **Llama 70B quantized**

**Avantages des mod√®les locaux :**
- üîí **Confidentialit√© totale** (rien ne sort de votre machine)
- üí∞ **Co√ªt z√©ro** (pas de frais API)
- ‚ö° **Latence r√©duite** (pas d'appels r√©seau)
- üõ†Ô∏è **Contr√¥le total** (param√®tres, prompts, etc.)

Voulez-vous que je vous aide √† installer et configurer l'un de ces mod√®les ?
