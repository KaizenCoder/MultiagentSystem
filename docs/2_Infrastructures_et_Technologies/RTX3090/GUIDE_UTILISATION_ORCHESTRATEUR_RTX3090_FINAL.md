# ğŸ® Guide Utilisation Orchestrateur RTX3090 - Configuration Finale
## NextGeneration Multi-Agent System - Production Ready

---

**Version :** Finale - Score 4/4 (100%)  
**Configuration :** RTX 3090 (24GB) + 4 ModÃ¨les OptimisÃ©s  
**Statut :** âœ… PRODUCTION READY  
**Performance :** 4.9-8.2 tokens/s selon modÃ¨le  

---

## ğŸš€ DÃ‰MARRAGE RAPIDE

### 1. **Configuration SystÃ¨me**
```powershell
# Ã‰tape 1 : Configuration environnement RTX3090
config_env_rtx3090.bat

# Ã‰tape 2 : VÃ©rification variables
echo $env:CUDA_VISIBLE_DEVICES    # Doit Ãªtre "1"
echo $env:CUDA_DEVICE_ORDER       # Doit Ãªtre "PCI_BUS_ID"

# Ã‰tape 3 : DÃ©marrage Ollama
ollama serve
```

### 2. **Lancement Orchestrateur**
```python
# MÃ©thode 1 : Script direct
python start_orchestrator.py

# MÃ©thode 2 : Import manuel
from orchestrator.app.agents.ollama_worker import OllamaLocalWorker
from orchestrator.app.config_rtx3090_optimized import RTX3090_CONFIG

worker = OllamaLocalWorker(RTX3090_CONFIG)
```

### 3. **Test Fonctionnement**
```python
# Test intÃ©gration finale
python test_integration_finale_rtx3090.py
# â†’ Doit afficher : Score final : 4/4 (100%) - SUCCÃˆS TOTAL
```

---

## ğŸ¤– UTILISATION DU WORKER OLLAMA

### ğŸ¯ **SÃ©lection Automatique de ModÃ¨les**

Le systÃ¨me sÃ©lectionne automatiquement le modÃ¨le optimal selon les mots-clÃ©s :

```python
from orchestrator.app.agents.ollama_worker import OllamaLocalWorker
from orchestrator.app.config_rtx3090_optimized import RTX3090_CONFIG

worker = OllamaLocalWorker(RTX3090_CONFIG)

# 1. POUR CODE (â†’ qwen2.5-coder:1.5b, 8.2 tokens/s)
result = await worker.process_task(
    "Ã‰cris une fonction Python pour parser JSON",
    requirements=["code", "programming"]
)

# 2. POUR RAPIDITÃ‰ (â†’ nous-hermes-2-mistral-7b-dpo, 6.4 tokens/s)
result = await worker.process_task(
    "RÃ©sume rapidement ce document",
    requirements=["quick", "fast"]
)

# 3. POUR ANALYSE (â†’ mixtral:8x7b-instruct-v0.1-q3_k_m, 5.4 tokens/s)
result = await worker.process_task(
    "Analyse complexe de cette architecture microservices",
    requirements=["analysis", "complex"]
)

# 4. USAGE PAR DÃ‰FAUT (â†’ llama3:8b-instruct-q6_k, 4.9 tokens/s)
result = await worker.process_task(
    "Explique le concept de machine learning"
)
```

### ğŸ“Š **RÃ©ponse Format**
```python
{
    "result": "RÃ©ponse gÃ©nÃ©rÃ©e par le modÃ¨le",
    "model_used": "llama3:8b-instruct-q6_k",
    "agent_type": "ollama_local_rtx3090",
    "gpu_used": "RTX 3090",
    "performance": {
        "tokens_per_second": 4.9,
        "vram_usage": "28%",
        "generation_time": "5.36s"
    },
    "confidence": 0.95
}
```

---

## ğŸ¯ CAS D'USAGE SPÃ‰CIALISÃ‰S

### ğŸ’» **DÃ©veloppement Code**
```python
async def code_assistant(code_request):
    """Assistant code utilisant qwen-coder optimisÃ©."""
    
    result = await worker.process_task(
        f"""
        En tant qu'expert dÃ©veloppeur Python :
        
        Demande : {code_request}
        
        Fournis :
        1. Code propre et documentÃ©
        2. Gestion des erreurs
        3. Tests unitaires
        4. Bonnes pratiques
        """,
        requirements=["code", "programming", "best_practices"]
    )
    
    return result

# Exemple
code = await code_assistant("CrÃ©er une API REST avec FastAPI")
print(f"ModÃ¨le utilisÃ© : {code['model_used']}")  # qwen2.5-coder:1.5b
print(f"Performance : {code['performance']['tokens_per_second']} tokens/s")
```

### âš¡ **RÃ©ponses Rapides**
```python
async def quick_answer(question):
    """RÃ©ponses ultra-rapides avec nous-hermes."""
    
    result = await worker.process_task(
        question,
        requirements=["quick", "fast", "concise"]
    )
    
    return result

# Exemple
answer = await quick_answer("Qu'est-ce que Docker ?")
print(f"ModÃ¨le : {answer['model_used']}")  # nous-hermes-2-mistral-7b-dpo
print(f"Vitesse : {answer['performance']['tokens_per_second']} tokens/s")  # 6.4
```

### ğŸ§  **Analyses Complexes**
```python
async def deep_analysis(content):
    """Analyse approfondie avec Mixtral qualitÃ© maximum."""
    
    result = await worker.process_task(
        f"""
        Analyse experte approfondie :
        
        Contenu : {content}
        
        Fournis :
        1. Analyse structurÃ©e
        2. Points clÃ©s identifiÃ©s
        3. Recommandations
        4. Perspectives multiples
        """,
        requirements=["analysis", "complex", "quality", "detailed"]
    )
    
    return result

# Exemple
analysis = await deep_analysis("Architecture microservices entreprise")
print(f"ModÃ¨le : {analysis['model_used']}")  # mixtral:8x7b-instruct-v0.1-q3_k_m
print(f"VRAM : {analysis['performance']['vram_usage']}")  # 92%
```

---

## ğŸ“Š MONITORING ET PERFORMANCE

### ğŸ›ï¸ **Dashboard Temps RÃ©el**
```powershell
# DÃ©marrage dashboard
python dashboard_rtx3090.py

# Affiche :
# - Utilisation GPU RTX 3090 en temps rÃ©el
# - ModÃ¨les actifs et performance
# - MÃ©moire VRAM utilisÃ©e
# - TempÃ©rature GPU
```

### ğŸ“ˆ **Benchmarks Performance**
```python
# Tests performance complets
python benchmark_rtx3090_complet.py

# RÃ©sultats attendus :
# qwen-coder: 8.2 tokens/s (4% VRAM)
# nous-hermes: 6.4 tokens/s (17% VRAM)  
# mixtral-q3k: 5.4 tokens/s (92% VRAM)
# llama3-q6k: 4.9 tokens/s (28% VRAM)
```

### ğŸ” **Surveillance Continue**
```powershell
# Service surveillance automatique
start_monitor_rtx3090.bat

# Logs dans : logs/rtx3090_monitoring.log
# Alertes si : VRAM > 95%, Temp > 80Â°C
```

---

## ğŸ”§ CONFIGURATION AVANCÃ‰E

### ğŸ¯ **Personnalisation SÃ©lection ModÃ¨les**
```python
# orchestrator/app/config_rtx3090_optimized.py
CUSTOM_MODEL_SELECTOR = {
    # Mots-clÃ©s pour qwen-coder
    "code_keywords": ["code", "programming", "debug", "function", "class", "api"],
    
    # Mots-clÃ©s pour nous-hermes (rapide)
    "quick_keywords": ["quick", "fast", "rapid", "brief", "summary"],
    
    # Mots-clÃ©s pour mixtral (qualitÃ©)
    "analysis_keywords": ["analysis", "complex", "detailed", "expert", "comprehensive"],
    
    # Seuils de sÃ©lection
    "complexity_threshold": 200,  # caractÃ¨res
    "quality_threshold": ["quality", "best", "expert", "detailed"]
}
```

### âš™ï¸ **ParamÃ¨tres Ollama OptimisÃ©s**
```python
# Configuration par modÃ¨le
MODEL_CONFIGS = {
    "qwen2.5-coder:1.5b": {
        "temperature": 0.1,  # Code prÃ©cis
        "top_p": 0.9,
        "num_ctx": 4096,
        "num_gpu": 1
    },
    "nous-hermes-2-mistral-7b-dpo": {
        "temperature": 0.7,  # Ã‰quilibrÃ©
        "top_p": 0.95,
        "num_ctx": 2048,
        "num_gpu": 1
    },
    "mixtral:8x7b-instruct-v0.1-q3_k_m": {
        "temperature": 0.3,  # QualitÃ©
        "top_p": 0.95,
        "num_ctx": 8192,
        "num_gpu": 1
    },
    "llama3:8b-instruct-q6_k": {
        "temperature": 0.5,  # Standard
        "top_p": 0.9,
        "num_ctx": 4096,
        "num_gpu": 1
    }
}
```

---

## ğŸš¨ DÃ‰PANNAGE ET MAINTENANCE

### âŒ **ProblÃ¨mes Courants**

**1. Erreur "CUDA out of memory"**
```python
# Solution : VÃ©rifier modÃ¨le sÃ©lectionnÃ©
if vram_usage > 95:
    fallback_model = "nous-hermes-2-mistral-7b-dpo"  # Seulement 17% VRAM
```

**2. Ollama ne rÃ©pond pas**
```powershell
# RedÃ©marrage Ollama
taskkill /F /IM ollama.exe
ollama serve
```

**3. Mauvaise sÃ©lection GPU**
```powershell
# VÃ©rification variables
echo $env:CUDA_VISIBLE_DEVICES  # Doit Ãªtre "1"
config_env_rtx3090.bat          # RÃ©exÃ©cuter config
```

### ğŸ”„ **Maintenance RÃ©guliÃ¨re**
```powershell
# Nettoyage modÃ¨les inutilisÃ©s
ollama list  # Lister modÃ¨les
ollama rm nom_modele  # Supprimer si nÃ©cessaire

# Monitoring espace disque
dir D:\modeles_llm  # VÃ©rifier taille

# Logs de performance
type logs\rtx3090_monitoring.log | tail -100
```

---

## ğŸ“ˆ OPTIMISATIONS RÃ‰ALISÃ‰ES

### âœ… **Espace Disque - 33GB LibÃ©rÃ©s**
- SupprimÃ© `mixtral-8x7b:latest` (26GB) â†’ RemplacÃ© par version Q3_K (22GB)
- SupprimÃ© modÃ¨les redondants : `deepseek-coder:33b`, `starcoder2:3b`, `code-stral`
- **RÃ©sultat** : 34% d'optimisation espace disque

### âš¡ **Performance GPU**
- Configuration Device 1 (RTX 3090) exclusivement
- DÃ©sactivation RTX 5060 Ti (incompatibilitÃ© CUDA/PyTorch)
- Optimisation charge VRAM selon usage : 4%-92%

### ğŸ¯ **ModÃ¨les OptimisÃ©s**
| ModÃ¨le | Taille | Performance | Usage Optimal |
|--------|--------|-------------|---------------|
| qwen-coder | 986MB | 8.2 tok/s | Code rapide |
| nous-hermes | 4.1GB | 6.4 tok/s | RÃ©ponses express |
| llama3-q6k | 6.6GB | 4.9 tok/s | Usage quotidien |
| mixtral-q3k | 22GB | 5.4 tok/s | Analyse qualitÃ© |

---

## ğŸ¯ VALIDATION FINALE

### âœ… **Score 4/4 (100%) - Tests ValidÃ©s**
- Configuration RTX3090 optimisÃ©e importÃ©e âœ…
- OllamaLocalWorker fonctionnel âœ…
- SÃ©lection intelligente testÃ©e âœ…
- GÃ©nÃ©ration test rÃ©ussie (2.8 tokens/s) âœ…

### ğŸš€ **SystÃ¨me Production Ready**
- Orchestrateur intÃ©grÃ© avec 4 modÃ¨les optimisÃ©s
- Worker Ollama avec sÃ©lection automatique
- Monitoring temps rÃ©el configurÃ©
- 33GB d'espace disque optimisÃ©s

**ğŸ‰ FÃ‰LICITATIONS ! Votre systÃ¨me NextGeneration RTX3090 est opÃ©rationnel !**

---

## ğŸ“ SUPPORT

### ğŸ“‹ **Checklist DÃ©marrage**
- [ ] Variables environnement configurÃ©es (`config_env_rtx3090.bat`)
- [ ] Ollama dÃ©marrÃ© (`ollama serve`)
- [ ] Test intÃ©gration rÃ©ussi (score 4/4)
- [ ] Monitoring actif (`dashboard_rtx3090.py`)

### ğŸ”§ **Commandes Utiles**
```powershell
# Statut systÃ¨me
ollama list
nvidia-smi
python test_integration_finale_rtx3090.py

# Performance
python benchmark_rtx3090_complet.py
python dashboard_rtx3090.py

# Maintenance
config_env_rtx3090.bat
start_monitor_rtx3090.bat
```

**SystÃ¨me NextGeneration RTX3090 - PrÃªt pour production ! ğŸš€** 