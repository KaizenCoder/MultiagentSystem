#!/usr/bin/env python3
"""
üöÄ REAL AGENT 12 - BACKUP MANAGER
Agent Factory Pattern - Sprint 4 - Agent Autonome

Mission: Agent autonome pour gestion backups production + versioning
- Surveillance continue des changements
- Backups automatiques avec Git versioning
- Proc√©dures rollback test√©es
- Validation int√©grit√© SHA-256
- R√©tention intelligente

Version: 1.0.0 - Agent R√©el Autonome
"""

import asyncio
import json
import sys
from pathlib import Path
from core import logging_manager
import hashlib
import shutil
import tarfile
import time
import git
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any
import aiofiles
import watchdog
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import signal
import sys
import logging

# Configuration
try:
    AGENT_ROOT = Path(__file__).parent
    PROJECT_ROOT = AGENT_ROOT.parent
except NameError:
    # Fallback si __file__ n'est pas d√©fini
    AGENT_ROOT = Path.cwd() / "agents"
    PROJECT_ROOT = Path.cwd()

BACKUPS_DIR = PROJECT_ROOT / "backups"
LOGS_DIR = PROJECT_ROOT / "logs"
VERSIONING_DIR = PROJECT_ROOT / "versioning"

# Cr√©er r√©pertoires
for directory in [BACKUPS_DIR, LOGS_DIR, VERSIONING_DIR]:
    directory.mkdir(exist_ok=True)

@dataclass
class BackupMetadata:
    """M√©tadonn√©es backup"""
    backup_id: str
    timestamp: datetime
    backup_type: str  # full, incremental, critical
    file_count: int
    total_size_bytes: int
    checksum_sha256: str
    retention_days: int
    git_commit_hash: Optional[str]
    description: str

@dataclass
class BackupState:
    """√âtat backup temps r√©el"""
    timestamp: datetime
    total_backups: int
    total_size_gb: float
    last_backup_time: datetime
    files_monitored: int
    git_commits: int
    retention_cleanups: int
    integrity_checks_passed: int

class FileChangeHandler(FileSystemEventHandler):
    """Gestionnaire changements fichiers"""
    
    def __init__(self, backup_manager, loop):
        self.backup_manager = backup_manager
        self.loop = loop
        self.logger = backup_manager.logger
        
    def on_modified(self, event):
        if not event.is_directory:
            self.logger.debug(f"üìù Fichier modifi√©: {event.src_path}")
            asyncio.run_coroutine_threadsafe(
                self.backup_manager.handle_file_change(event.src_path), 
                self.loop
            )
    
    def on_created(self, event):
        if not event.is_directory:
            self.logger.debug(f"üìÑ Fichier cr√©√©: {event.src_path}")
            asyncio.run_coroutine_threadsafe(
                self.backup_manager.handle_file_change(event.src_path),
                self.loop
            )

class RealAgent12BackupManager:
    """
    üöÄ AGENT 12 R√âEL - GESTIONNAIRE BACKUPS AUTONOME
    
    Agent qui surveille et sauvegarde automatiquement:
    - Surveillance temps r√©el des changements fichiers
    - Backups automatiques avec compression
    - Versioning Git avec commits automatiques
    - Validation int√©grit√© cryptographique
    - Nettoyage automatique selon r√©tention
    """
    
    def __init__(self):
        self.agent_id = "real_agent_12"
        self.agent_name = "Backup Manager (Autonome)"
        self.version = "1.0.0"
        self.status = "INITIALIZING"
        
        # Configuration
        self.running = True
        self.shutdown_event = asyncio.Event()
        
        # Politiques r√©tention
        self.retention_policies = {
            "critical": 365,    # 1 an
            "production": 90,   # 3 mois
            "development": 30,  # 1 mois
            "temporary": 7      # 1 semaine
        }
        
        # √âtat interne
        self.current_state = None
        self.backup_history = []
        self.monitored_paths = [
            PROJECT_ROOT / "agents",
            PROJECT_ROOT / "code_expert",
            PROJECT_ROOT / "documentation",
            PROJECT_ROOT / "tests"
        ]
        
        # Logging (setup first!)
        self._setup_logging()
        
        # Git repository
        self.git_repo = None
        self._setup_git_repository()
        
        # File watcher
        self.observer = Observer()
        self.file_handler = None  # Initialis√© plus tard avec la loop
        
        # Signal handlers
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
        self.logger.info(f"üöÄ {self.agent_name} initialis√©")
        self.logger.info(f"Surveillance: {len(self.monitored_paths)} r√©pertoires")
        self.logger.info(f"R√©tention: {self.retention_policies}")
    
    def _setup_logging(self):
        """Configuration logging agent"""
        log_file = LOGS_DIR / f"{self.agent_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        
        # LoggingManager NextGeneration - Agent
        self.logger = logging_manager.LoggingManager().get_agent_logger(
            agent_name="class",
            role="ai_processor",
            domain="general",
            async_enabled=True
        )
    
    def _setup_git_repository(self):
        """Initialisation repository Git"""
        try:
            git_dir = VERSIONING_DIR / ".git"
            
            if git_dir.exists():
                self.git_repo = git.Repo(VERSIONING_DIR)
                self.logger.info("‚úÖ Repository Git existant charg√©")
            else:
                self.git_repo = git.Repo.init(VERSIONING_DIR)
                
                # Configuration initiale
                with self.git_repo.config_writer() as config:
                    config.set_value("user", "name", "Agent 12 Backup Manager")
                    config.set_value("user", "email", "agent12@factory.local")
                
                # Commit initial
                initial_file = VERSIONING_DIR / "README.md"
                initial_file.write_text("# Agent Factory Versioning\n\nRepository de versioning automatique par Agent 12\n")
                
                self.git_repo.index.add([str(initial_file)])
                self.git_repo.index.commit("Initial commit by Agent 12")
                
                self.logger.info("‚úÖ Repository Git initialis√©")
                
        except Exception as e:
            self.logger.error(f"‚ùå Erreur Git: {e}")
            self.git_repo = None
    
    def _signal_handler(self, signum, frame):
        """Gestionnaire signaux pour arr√™t propre"""
        self.logger.info(f"üõë Signal {signum} re√ßu - Arr√™t en cours...")
        self.running = False
        self.shutdown_event.set()
    
    def calculate_file_checksum(self, file_path: Path) -> str:
        """Calcule checksum SHA-256 d'un fichier"""
        try:
            sha256_hash = hashlib.sha256()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    sha256_hash.update(chunk)
            return sha256_hash.hexdigest()
        except Exception as e:
            self.logger.error(f"‚ùå Erreur checksum {file_path}: {e}")
            return ""
    
    async def create_backup(self, backup_type: str = "incremental", description: str = "") -> Optional[BackupMetadata]:
        """Cr√©e un backup avec m√©tadonn√©es"""
        try:
            timestamp = datetime.now()
            backup_id = f"backup_{timestamp.strftime('%Y%m%d_%H%M%S')}_{backup_type}"
            
            # Cr√©er archive tar.gz
            backup_file = BACKUPS_DIR / f"{backup_id}.tar.gz"
            
            total_files = 0
            total_size = 0
            
            with tarfile.open(backup_file, "w:gz") as tar:
                for monitored_path in self.monitored_paths:
                    if monitored_path.exists():
                        for file_path in monitored_path.rglob("*"):
                            if file_path.is_file() and not file_path.name.startswith('.'):
                                try:
                                    tar.add(file_path, arcname=file_path.relative_to(PROJECT_ROOT))
                                    total_files += 1
                                    total_size += file_path.stat().st_size
                                except Exception as e:
                                    self.logger.warning(f"‚ö†Ô∏è Impossible d'ajouter {file_path}: {e}")
            
            # Calcul checksum
            checksum = self.calculate_file_checksum(backup_file)
            
            # Commit Git si disponible
            git_commit_hash = None
            if self.git_repo:
                try:
                    # Copier fichiers importants dans repo versioning
                    for monitored_path in self.monitored_paths:
                        if monitored_path.exists():
                            dest_path = VERSIONING_DIR / monitored_path.name
                            if dest_path.exists():
                                shutil.rmtree(dest_path)
                            shutil.copytree(monitored_path, dest_path, ignore=shutil.ignore_patterns('__pycache__', '*.pyc'))
                    
                    # Commit
                    self.git_repo.git.add(all=True)
                    commit_msg = f"Backup {backup_id}: {description or 'Automatic backup'}"
                    commit = self.git_repo.index.commit(commit_msg)
                    git_commit_hash = commit.hexsha
                    
                    self.logger.info(f"üìù Git commit: {git_commit_hash[:8]}")
                    
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Erreur Git commit: {e}")
            
            # M√©tadonn√©es
            metadata = BackupMetadata(
                backup_id=backup_id,
                timestamp=timestamp,
                backup_type=backup_type,
                file_count=total_files,
                total_size_bytes=total_size,
                checksum_sha256=checksum,
                retention_days=self.retention_policies.get(backup_type, 30),
                git_commit_hash=git_commit_hash,
                description=description or f"Automatic {backup_type} backup"
            )
            
            # Sauvegarder m√©tadonn√©es
            metadata_file = BACKUPS_DIR / f"{backup_id}_metadata.json"
            async with aiofiles.open(metadata_file, 'w') as f:
                await f.write(json.dumps(asdict(metadata), default=str, indent=2))
            
            self.backup_history.append(metadata)
            
            self.logger.info(
                f"‚úÖ Backup cr√©√©: {backup_id} "
                f"({total_files} fichiers, {total_size/1024/1024:.1f}MB)"
            )
            
            return metadata
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur cr√©ation backup: {e}")
            return None
    
    async def validate_backup_integrity(self, backup_metadata: BackupMetadata) -> bool:
        """Valide int√©grit√© d'un backup"""
        try:
            backup_file = BACKUPS_DIR / f"{backup_metadata.backup_id}.tar.gz"
            
            if not backup_file.exists():
                self.logger.error(f"‚ùå Fichier backup manquant: {backup_file}")
                return False
            
            # V√©rification checksum
            current_checksum = self.calculate_file_checksum(backup_file)
            if current_checksum != backup_metadata.checksum_sha256:
                self.logger.error(f"‚ùå Checksum invalide pour {backup_metadata.backup_id}")
                return False
            
            # Test extraction
            try:
                with tarfile.open(backup_file, "r:gz") as tar:
                    # V√©rifier que l'archive peut √™tre lue
                    members = tar.getmembers()
                    if len(members) != backup_metadata.file_count:
                        self.logger.warning(f"‚ö†Ô∏è Nombre fichiers diff√©rent: {len(members)} vs {backup_metadata.file_count}")
            except Exception as e:
                self.logger.error(f"‚ùå Archive corrompue {backup_metadata.backup_id}: {e}")
                return False
            
            self.logger.debug(f"‚úÖ Backup {backup_metadata.backup_id} valid√©")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur validation backup: {e}")
            return False
    
    async def cleanup_old_backups(self):
        """Nettoyage backups selon politiques r√©tention"""
        try:
            cleaned_count = 0
            current_time = datetime.now()
            
            for backup_file in BACKUPS_DIR.glob("*_metadata.json"):
                try:
                    async with aiofiles.open(backup_file, 'r') as f:
                        metadata_data = json.loads(await f.read())
                    
                    backup_time = datetime.fromisoformat(metadata_data['timestamp'])
                    retention_days = metadata_data.get('retention_days', 30)
                    age_days = (current_time - backup_time).days
                    
                    if age_days > retention_days:
                        # Supprimer backup et m√©tadonn√©es
                        backup_archive = BACKUPS_DIR / f"{metadata_data['backup_id']}.tar.gz"
                        
                        if backup_archive.exists():
                            backup_archive.unlink()
                        backup_file.unlink()
                        
                        cleaned_count += 1
                        self.logger.info(f"üóëÔ∏è Backup supprim√©: {metadata_data['backup_id']} (√¢ge: {age_days} jours)")
                
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Erreur nettoyage {backup_file}: {e}")
            
            if cleaned_count > 0:
                self.logger.info(f"üßπ Nettoyage termin√©: {cleaned_count} backups supprim√©s")
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur nettoyage: {e}")
    
    async def collect_backup_state(self) -> BackupState:
        """Collecte √©tat backup temps r√©el"""
        try:
            # Compter backups
            backup_files = list(BACKUPS_DIR.glob("*.tar.gz"))
            total_backups = len(backup_files)
            
            # Taille totale
            total_size = sum(f.stat().st_size for f in backup_files)
            total_size_gb = total_size / (1024**3)
            
            # Dernier backup
            metadata_files = list(BACKUPS_DIR.glob("*_metadata.json"))
            last_backup_time = datetime.min
            
            for metadata_file in metadata_files:
                try:
                    async with aiofiles.open(metadata_file, 'r') as f:
                        metadata = json.loads(await f.read())
                    backup_time = datetime.fromisoformat(metadata['timestamp'])
                    if backup_time > last_backup_time:
                        last_backup_time = backup_time
                except:
                    continue
            
            # Fichiers surveill√©s
            files_monitored = 0
            for path in self.monitored_paths:
                if path.exists():
                    files_monitored += len(list(path.rglob("*.py")))
            
            # Commits Git
            git_commits = 0
            if self.git_repo:
                try:
                    git_commits = len(list(self.git_repo.iter_commits()))
                except:
                    pass
            
            state = BackupState(
                timestamp=datetime.now(),
                total_backups=total_backups,
                total_size_gb=total_size_gb,
                last_backup_time=last_backup_time,
                files_monitored=files_monitored,
                git_commits=git_commits,
                retention_cleanups=0,  # √Ä impl√©menter
                integrity_checks_passed=0  # √Ä impl√©menter
            )
            
            self.current_state = state
            return state
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur collecte √©tat: {e}")
            return None
    
    async def handle_file_change(self, file_path: str):
        """G√®re changement de fichier"""
        try:
            # √âviter trop de backups rapproch√©s
            if self.current_state and self.current_state.last_backup_time:
                time_since_last = datetime.now() - self.current_state.last_backup_time
                if time_since_last.total_seconds() < 300:  # 5 minutes minimum
                    return
            
            # Cr√©er backup incr√©mental
            await self.create_backup(
                backup_type="incremental",
                description=f"Auto backup apr√®s modification {Path(file_path).name}"
            )
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur gestion changement: {e}")
    
    async def start_file_monitoring(self):
        """D√©marre surveillance fichiers"""
        try:
            for path in self.monitored_paths:
                if path.exists():
                    self.observer.schedule(self.file_handler, str(path), recursive=True)
            
            self.observer.start()
            self.logger.info("üëÅÔ∏è Surveillance fichiers d√©marr√©e")
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur surveillance: {e}")
    
    async def backup_loop(self):
        """Boucle backups automatiques"""
        self.logger.info("üîÑ D√©marrage boucle backups")
        
        while self.running:
            try:
                # Backup quotidien
                current_hour = datetime.now().hour
                if current_hour == 2:  # 2h du matin
                    await self.create_backup(
                        backup_type="production",
                        description="Daily automatic backup"
                    )
                
                # Nettoyage hebdomadaire
                if datetime.now().weekday() == 0 and current_hour == 3:  # Lundi 3h
                    await self.cleanup_old_backups()
                
                await asyncio.sleep(3600)  # V√©rifier chaque heure
                
            except Exception as e:
                self.logger.error(f"‚ùå Erreur boucle backup: {e}")
                await asyncio.sleep(300)
    
    async def monitoring_loop(self):
        """Boucle monitoring principal"""
        self.logger.info("üîÑ D√©marrage boucle monitoring")
        
        while self.running:
            try:
                # Collecte √©tat
                await self.collect_backup_state()
                
                # Log √©tat toutes les 10 minutes
                if int(time.time()) % 600 == 0 and self.current_state:
                    self.logger.info(
                        f"üìä √âtat: {self.current_state.total_backups} backups "
                        f"({self.current_state.total_size_gb:.1f}GB) "
                        f"- {self.current_state.files_monitored} fichiers surveill√©s"
                    )
                
                await asyncio.sleep(60)  # Collecte chaque minute
                
            except Exception as e:
                self.logger.error(f"‚ùå Erreur boucle monitoring: {e}")
                await asyncio.sleep(60)
    
    async def run(self):
        """Point d'entr√©e principal agent"""
        self.logger.info(f"üöÄ D√©marrage {self.agent_name}")
        self.status = "RUNNING"
        
        # R√©cup√©rer la loop asyncio et configurer le handler
        self.loop = asyncio.get_running_loop()
        self.file_handler = FileChangeHandler(self, self.loop)

        try:
            # D√©marrage surveillance fichiers
            await self.start_file_monitoring()
            
            # Backup initial
            await self.create_backup(
                backup_type="critical",
                description="Initial backup at agent startup"
            )
            
            # D√©marrage t√¢ches asynchrones
            tasks = [
                asyncio.create_task(self.monitoring_loop()),
                asyncio.create_task(self.backup_loop())
            ]
            
            # Attendre arr√™t
            await asyncio.gather(*tasks, return_exceptions=True)
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur ex√©cution agent: {e}")
        finally:
            await self.shutdown()
    
    async def shutdown(self):
        """Arr√™t propre agent"""
        self.logger.info("üõë Arr√™t agent en cours...")
        self.status = "SHUTTING_DOWN"
        
        try:
            # Arr√™t surveillance
            if self.observer.is_alive():
                self.observer.stop()
                self.observer.join()
            
            # Backup final
            await self.create_backup(
                backup_type="critical",
                description="Final backup at agent shutdown"
            )
            
            self.status = "STOPPED"
            self.logger.info("‚úÖ Agent arr√™t√© proprement")
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur arr√™t: {e}")

async def main():
    """Point d'entr√©e principal"""
    print("üöÄ REAL AGENT 12 - BACKUP MANAGER")
    print("=" * 50)
    
    agent = RealAgent12BackupManager()
    
    try:
        await agent.run()
    except KeyboardInterrupt:
        print("\nüõë Arr√™t demand√© par utilisateur")
    except Exception as e:
        print(f"‚ùå Erreur fatale: {e}")
    finally:
        print("‚úÖ Agent termin√©")

if __name__ == "__main__":
    asyncio.run(main()) 
