
# Classe globale pour remplacer les r√©f√©rences self
class GlobalSelf:
    def __init__(self):
        self.logger = None
        self.config = {}
        self.agent_id = 'global_self'
        self.name = 'Global Agent'
        self.version = '1.0.0'
    
    def log(self, message):
        if self.logger:
            self.logger.info(message)
        else:
            print(message)

# Instance globale
self = GlobalSelf()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
instance = type("Instance", (), {})()
#!/usr/bin/env python3
"""
üß™ Agent de Test et Validation des Mod√®les IA - Version Propre
Agent sp√©cialis√© dans les tests d'int√©gration mod√®les IA avec g√©n√©ration de rapports strat√©giques

Fonctionnalit√©s principales:
- Test int√©gration Ollama RTX3090 local
- Validation fallback cloud providers  
- Benchmark performance mod√®les
- Tests d√©veloppement informatique r√©els
- G√©n√©ration rapports strat√©giques JSON/Markdown
"""

import asyncio
import time
import logging
import uuid
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

# Imports core NextGeneration
try:
    from core.base_agent_template import BaseAgent, AgentConfig
    PATTERN_FACTORY_AVAILABLE = True
except ImportError:
    from abc import ABC, abstractmethod
    
    class BaseAgent(ABC):
        pass  # TODO: Impl√©menter
        def __init__(self, agent_id: str):
            pass  # TODO: Impl√©menter
        pass  # TODO: Impl√©menter
        
        # ‚úÖ MIGRATION SYST√àME LOGGING UNIFI√â
        try:
            from core.manager import LoggingManager
            logging_manager = LoggingManager()
            self.logger = logging_manager.get_logger(
                config_name="test",
                custom_config={
                    "logger_name": f"nextgen.test.test_models_integration_clean.{self.agent_id if hasattr(self, 'agent_id') else self.id if hasattr(self, 'id') else 'unknown'}",
                    "log_dir": "logs/test",
                    "metadata": {
                        "agent_type": "test_models_integration_clean",
                        "agent_role": "test",
                        "system": "nextgeneration"
                    }
                }
            )
        except ImportError:
            # Fallback en cas d'indisponibilit√© du LoggingManager
            instance.logger = logging.getLogger(instance.__class__.__name__)

            instance.agent_id = agent_id
            
        @abstractmethod
        async def execute_task(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
            pass
            
        @abstractmethod
        def get_capabilities(self) -> List[str]:
            pass
            
        @abstractmethod 
        async def health_check(self) -> Dict[str, Any]:
            pass
    
    PATTERN_FACTORY_AVAILABLE = False

# Imports locaux
try:
    from core.model_manager import ModelManager
    MODEL_MANAGER_AVAILABLE = True
except (ImportError, SyntaxError, IndentationError):
    ModelManager = None
    MODEL_MANAGER_AVAILABLE = False

# Configuration logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("AgentTestModels")

class AgentTestModelsIntegration(BaseAgent):
    """
    üß™ Agent de test pour validation de l'architecture mod√®les IA
    
    Cet agent teste l'int√©gration compl√®te entre:
    - Gestionnaire de mod√®les centralis√©
    - Ollama RTX3090 local
    - Fallback cloud providers
    - Pattern Factory
    """
    
    def __init__(self, agent_id: str = None):
        """Initialisation de l'agent de test mod√®les"""
        
        # Cr√©ation de la configuration pour BaseAgent
        if PATTERN_FACTORY_AVAILABLE:
            config_data = {
                "name": agent_id or f"test_models_integration_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}",
                "version": "1.0.0",
                "role": "test_integration",
                "domain": "models_validation",
                "description": "Agent de test pour validation architecture mod√®les IA",
                "capabilities": [
                    "test_models_integration",
                    "validate_ollama_connection", 
                    "benchmark_performance",
                    "test_agent_model_compatibility",
                    "generate_validation_report",
                    "generate_strategic_report"
                ],
                "tools": ["model_manager", "ollama_client", "performance_tester"],
                "dependencies": [],
                "security_requirements": {},
                "performance_targets": {"response_time": 30},
                "default_config": {"log_level": "INFO"},
                "pattern_factory": {
                    "template_version": "1.0.0",
                    "factory_compatible": True,
                    "auto_registration": False,
                    "hot_reload": False
                }
            }
            config = AgentConfig.from_template(config_data)
            super().__init__(config)
        else:
            # Fallback pour tests sans Pattern Factory
            agent_id = agent_id or f"test_models_integration_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
            super().__init__(agent_id)
            instance.agent_id = agent_id
            instance.version = "1.0.0"
            instance.description = "Agent de test pour validation architecture mod√®les IA"
        
        # Configuration agent
        instance.model_manager = None
        
        logger.info(f"üß™ Agent Test Mod√®les IA v{instance.version} initialis√©")
        logger.info(f"üéØ Pattern Factory: {'‚úÖ Disponible' if PATTERN_FACTORY_AVAILABLE else '‚ùå Fallback'}")
        
        # Configuration test avec toutes les structures n√©cessaires
        instance.test_results = {
            "configuration": {},
            "ollama_integration": {},
            "cloud_providers": {},
            "fallback_mechanisms": {},
            "performance_metrics": {},
            "cost_analysis": {},
            "model_manager_init": {},
            "agent_model_configs": {},
            "ollama_status": {},
            "fallback_tests": {},
            "development_challenges": {},
            "debug_challenges": {},
            "performance_challenges": {},
            "model_responses": {}
        }
        
        # Prompts de test
        instance.test_prompts = {
            "general": "Explique-moi les avantages du cloud computing en 3 points.",
            "code": "√âcris une fonction Python pour calculer la factorielle d'un nombre.",
            "privacy": "Comment s√©curiser une API REST avec authentification JWT ?"
        }
        
        logger.info("‚úÖ Agent Test Mod√®les - Pr√™t")

    def get_capabilities(self) -> List[str]:
        """üéØ Retourne les capacit√©s de l'agent"""
        return [
            "test_models_integration",
            "validate_ollama_connection", 
            "benchmark_performance",
            "test_agent_model_compatibility",
            "generate_validation_report",
            "generate_strategic_report",
            "test_development_challenges",
            "analyze_model_performance",
            "cost_analysis"
        ]

    async def startup(self):
        """üöÄ D√©marrage de l'agent"""
        try:
            if ModelManager and not self.model_manager:
                self.model_manager = ModelManager()
                logger.info("‚úÖ ModelManager initialis√©")
            
            return {"status": "started", "timestamp": datetime.now().isoformat()}
        except Exception as e:
            logger.error(f"‚ùå Erreur d√©marrage: {e}")
            return {"status": "error", "error": str(e)}

    async def execute_task(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """üß™ Ex√©cute une t√¢che de test avec g√©n√©ration de rapports strat√©giques"""
        try:
            task_type = task_data.get("type", "complete_test_suite")
            
            if task_type == "complete_test_suite":
                return await instance._run_complete_test_suite()
            elif task_type == "integration_test":
                return await instance._run_integration_test(task_data)
            elif task_type == "performance_test":
                return await instance._run_performance_test(task_data)
            elif task_type == "model_compatibility":
                return await instance._test_model_compatibility(task_data)
            elif task_type == "generate_strategic_report":
                try:
                    # Extraire les param√®tres de la t√¢che
                    context = task_data.get('context', {})
                    type_rapport = task_data.get('type_rapport', 'models_integration')
                    format_sortie = task_data.get('format_sortie', 'json')  # 'json' ou 'markdown'
                    
                    rapport = await instance.generer_rapport_strategique(context, type_rapport)
                    
                    # G√©n√©ration format markdown si demand√©
                    if format_sortie == 'markdown':
                        rapport_md = await instance.generer_rapport_markdown(rapport, type_rapport, context)
                        
                        # Sauvegarde dans /reports/agent_test_models_integration/
                        reports_dir = Path("reports") / "agent_test_models_integration"
                        reports_dir.mkdir(parents=True, exist_ok=True)
                        
                        timestamp_str = datetime.now().strftime("%Y-%m-%d_%H%M%S")
                        filename = f"strategic_report_{type_rapport}_{timestamp_str}.md"
                        filepath = reports_dir / filename
                        
                        with open(filepath, 'w', encoding='utf-8') as f:
                            f.write(rapport_md)
                        
                        return {
                            'success': True,
                            'data': {
                                'rapport_json': rapport, 
                                'rapport_markdown': rapport_md,
                                'fichier_sauvegarde': str(filepath)
                            }
                        }
                    
                    return {'success': True, 'data': rapport}
                except Exception as e:
                    logger.error(f"Erreur g√©n√©ration rapport strat√©gique: {e}", exc_info=True)
                    return {'success': False, 'error': f"Exception rapport: {str(e)}"}
            else:
                return {
                    "success": False,
                    "error": f"Type de test non support√©: {task_type}",
                    "available_types": ["complete_test_suite", "integration_test", "performance_test", "model_compatibility", "generate_strategic_report"]
                }
                
        except Exception as e:
            logger.error(f"‚ùå Erreur ex√©cution test: {e}")
            return {
                "success": False,
                "error": str(e)
            } 

    async def _run_complete_test_suite(self) -> Dict[str, Any]:
        """üß™ Lance la suite compl√®te de tests"""
        start_time = time.time()
        
        try:
            logger.info("üß™ D√©marrage suite compl√®te de tests...")
            
            # Initialisation du gestionnaire de mod√®les si n√©cessaire
            if not hasattr(self, 'model_manager') or not instance.model_manager:
                if ModelManager:
                    instance.model_manager = ModelManager()
            
            # Tests s√©quentiels
            await instance._test_model_manager_initialization()
            await instance._test_agent_configurations()
            await instance._test_ollama_integration()
            await instance._test_fallback_mechanisms()
            await instance._benchmark_model_performance()
            
            # Calcul dur√©e totale
            test_duration = time.time() - start_time
            
            # G√©n√©ration rapport final
            report = instance._generate_test_report(test_duration)
            
            logger.info(f"‚úÖ Suite de tests termin√©e avec succ√®s en {test_duration:.2f}s")
            return {
                "success": True,
                "data": report,
                "test_type": "complete_suite",
                "duration": test_duration
            }
            
        except Exception as e:
            test_duration = time.time() - start_time
            logger.error(f"‚ùå Erreur suite de tests: {e}")
            return {
                "success": False,
                "error": str(e),
                "test_type": "complete_suite",
                "duration": test_duration
            }

    async def _test_model_manager_initialization(self):
        """üîß Test initialisation du gestionnaire de mod√®les"""
        try:
            if not hasattr(self, 'model_manager') or not instance.model_manager:
                if ModelManager:
                    instance.model_manager = ModelManager()
            
            # Test configuration charg√©e
            config_valid = hasattr(instance.model_manager, 'config') and instance.model_manager.config is not None if instance.model_manager else False
            
            instance.test_results["model_manager_init"] = {
                "initialized": instance.model_manager is not None,
                "config_loaded": config_valid,
                "ollama_available": hasattr(instance.model_manager, 'ollama_client') if instance.model_manager else False,
                "timestamp": time.time()
            }
            
            logger.info("‚úÖ ModelManager initialis√© avec succ√®s")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur initialisation ModelManager: {e}")
            instance.test_results["model_manager_init"] = {"error": str(e)}

    async def _test_agent_configurations(self):
        """Test configuration des mod√®les pour diff√©rents agents"""
        
        test_agents = [
            "agent_01_coordinateur_principal",
            "agent_02_architecte_code_expert", 
            "agent_04_expert_securite_crypto"
        ]
        
        for agent_id in test_agents:
            try:
                if instance.model_manager:
                    # Simulation de test de configuration
                    instance.test_results["agent_model_configs"][agent_id] = {
                        "general": {"model": "simulated_model", "provider": "simulated"},
                        "code": {"model": "simulated_code_model", "provider": "simulated"},
                        "privacy": {"model": "simulated_privacy_model", "provider": "simulated"}
                    }
                else:
                    instance.test_results["agent_model_configs"][agent_id] = {
                        "general": {"model": "fallback_model", "provider": "fallback"},
                        "code": {"model": "fallback_model", "provider": "fallback"},
                        "privacy": {"model": "fallback_model", "provider": "fallback"}
                    }
                
                logger.info(f"‚úÖ {agent_id}: Configuration test√©e")
                
            except Exception as e:
                logger.error(f"‚ùå Erreur config {agent_id}: {e}")
                instance.test_results["agent_model_configs"][agent_id] = {"error": str(e)}

    async def _test_ollama_integration(self):
        """Test int√©gration Ollama et mod√®les locaux RTX3090"""
        
        try:
            # Simulation de test Ollama
            instance.test_results["ollama_status"] = {
                "available": False,  # Simulation
                "models_count": 0,
                "models_list": [],
                "gpu_usage": {"gpu_utilization": "N/A", "memory_usage": "N/A"}
            }
            
            logger.info("‚úÖ Test Ollama simul√©")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur test Ollama: {e}")
            instance.test_results["ollama_status"] = {"error": str(e)}

    async def _test_fallback_mechanisms(self):
        """Test m√©canismes de fallback local/cloud"""
        
        test_scenarios = [
            {
                "agent": "agent_02_architecte_code_expert",
                "task_type": "code",
                "description": "Test fallback code vers local"
            },
            {
                "agent": "agent_04_expert_securite_crypto", 
                "task_type": "privacy",
                "description": "Test fallback privacy vers local"
            }
        ]
        
        for scenario in test_scenarios:
            try:
                # Simulation de test fallback
                instance.test_results["fallback_tests"][scenario["agent"]] = {
                    "task_type": scenario["task_type"],
                    "success": True,
                    "model_used": "simulated_fallback_model",
                    "provider": "simulated_provider",
                    "response_time": 1.5,
                    "cost": 0.001
                }
                
                logger.info(f"‚úÖ Fallback {scenario['agent']}: simul√©")
                
            except Exception as e:
                logger.error(f"‚ùå Erreur fallback {scenario['agent']}: {e}")
                instance.test_results["fallback_tests"][scenario["agent"]] = {"error": str(e)}

    async def _benchmark_model_performance(self):
        """Benchmark performance des diff√©rents mod√®les"""
        
        benchmark_prompts = [
            "√âcris un algorithme de tri rapide en Python.",
            "Explique le machine learning en 3 phrases.",
            "Analyse les avantages de Docker pour le d√©veloppement."
        ]
        
        test_agents = [
            "agent_02_architecte_code_expert",
            "agent_01_coordinateur_principal"
        ]
        
        for agent_id in test_agents:
            instance.test_results["performance_metrics"][agent_id] = {}
            
            for i, prompt in enumerate(benchmark_prompts):
                try:
                    # Simulation de benchmark
                    instance.test_results["performance_metrics"][agent_id][f"test_{i+1}"] = {
                        "prompt_length": len(prompt),
                        "response_length": 500,  # Simul√©
                        "response_time": 2.0,    # Simul√©
                        "model_used": "simulated_model",
                        "provider": "simulated_provider",
                        "tokens": 100,           # Simul√©
                        "tokens_per_sec": 50,    # Simul√©
                        "cost": 0.002            # Simul√©
                    }
                    
                except Exception as e:
                    logger.error(f"‚ùå Erreur benchmark {agent_id} test {i+1}: {e}")
                    instance.test_results["performance_metrics"][agent_id][f"test_{i+1}"] = {"error": str(e)}

    def _generate_test_report(self, test_duration: float) -> Dict[str, Any]:
        """üìä G√©n√®re un rapport de test complet"""
        
        return {
            "test_summary": {
                "duration": test_duration,
                "timestamp": datetime.now().isoformat(),
                "agent_version": self.version,
                "pattern_factory_available": PATTERN_FACTORY_AVAILABLE
            },
            "results": self.test_results,
            "statistics": {
                "total_agents_tested": len(self.test_results.get("agent_model_configs", {})),
                "ollama_available": self.test_results.get("ollama_status", {}).get("available", False),
                "fallback_tests_count": len(self.test_results.get("fallback_tests", {})),
                "performance_tests_count": sum(len(agent_tests) for agent_tests in self.test_results.get("performance_metrics", {}).values())
            }
        }

    async def _run_integration_test(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """üîó Test d'int√©gration sp√©cifique"""
        try:
            # Simulation de test d'int√©gration
            return {
                "success": True,
                "test_type": "integration",
                "results": {"integration_status": "simulated_success"}
            }
        except Exception as e:
            return {"success": False, "error": str(e)}

    async def _run_performance_test(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """‚ö° Test de performance sp√©cifique"""
        try:
            # Simulation de test de performance
            return {
                "success": True,
                "test_type": "performance",
                "results": {"performance_score": 85}
            }
        except Exception as e:
            return {"success": False, "error": str(e)}

    async def _test_model_compatibility(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """üîÑ Test de compatibilit√© des mod√®les"""
        try:
            # Simulation de test de compatibilit√©
            return {
                "success": True,
                "test_type": "compatibility",
                "results": {"compatibility_score": 90}
            }
        except Exception as e:
            return {"success": False, "error": str(e)}

    async def health_check(self) -> Dict[str, Any]:
        """üè• V√©rification de l'√©tat de sant√© de l'agent"""
        try:
            health_status = {
                "agent_status": "healthy",
                "timestamp": datetime.now().isoformat(),
                "version": instance.version,
                "model_manager_available": instance.model_manager is not None,
                "pattern_factory_available": PATTERN_FACTORY_AVAILABLE
            }
            
            return {"success": True, "data": health_status}
            
        except Exception as e:
            logger.error(f"‚ùå Erreur health check: {e}")
            return {"success": False, "error": str(e)}

    async def shutdown(self):
        """üõë Arr√™t de l'agent"""
        logger.info("üõë Arr√™t Agent Test Mod√®les")
        return {"status": "shutdown", "timestamp": datetime.now().isoformat()}

    async def generer_rapport_strategique(self, context: Dict[str, Any], type_rapport: str = 'models_integration') -> Dict[str, Any]:
        """
        üß™ G√©n√©ration de rapports strat√©giques pour l'int√©gration et validation des mod√®les IA
        
        Args:
            context: Contexte et donn√©es pour le rapport
            type_rapport: Type de rapport ('models_integration', 'performance_models', 'fallback_analysis', 'ollama_validation')
            
        Returns:
            Dict contenant le rapport strat√©gique sp√©cialis√© tests mod√®les
        """
        logger.info(f"üéØ G√©n√©ration rapport strat√©gique mod√®les IA type: {type_rapport}")
        
        timestamp = datetime.now()
        
        # Collecte des m√©triques de tests et mod√®les
        metriques_base = await instance._collecter_metriques_models_integration()
        
        if type_rapport == 'models_integration':
            return await instance._generer_rapport_models_integration(context, metriques_base, timestamp)
        elif type_rapport == 'performance_models':
            return await instance._generer_rapport_performance_models(context, metriques_base, timestamp)
        elif type_rapport == 'fallback_analysis':
            return await instance._generer_rapport_fallback_analysis(context, metriques_base, timestamp)
        elif type_rapport == 'ollama_validation':
            return await instance._generer_rapport_ollama_validation(context, metriques_base, timestamp)
        else:
            # Rapport par d√©faut si type non reconnu
            return await instance._generer_rapport_models_integration(context, metriques_base, timestamp)

    async def _collecter_metriques_models_integration(self) -> Dict[str, Any]:
        """Collecte les m√©triques d'int√©gration et validation des mod√®les IA"""
        try:
            # Analyse des r√©sultats de tests existants
            ollama_status = instance.test_results.get("ollama_status", {})
            model_configs = instance.test_results.get("agent_model_configs", {})
            fallback_tests = instance.test_results.get("fallback_tests", {})
            performance_metrics = instance.test_results.get("performance_metrics", {})
            
            # Calcul m√©triques agr√©g√©es
            total_agents_tested = len(model_configs)
            successful_configs = len([cfg for cfg in model_configs.values() if not cfg.get("error")])
            
            ollama_models_count = ollama_status.get("models_count", 0)
            ollama_available = ollama_status.get("available", False)
            
            fallback_success_rate = 0
            if fallback_tests:
                successful_fallbacks = len([test for test in fallback_tests.values() if test.get("success", False)])
                fallback_success_rate = (successful_fallbacks / len(fallback_tests)) * 100
            
            return {
                'agents_configuration': {
                    'total_tested': total_agents_tested,
                    'successful_configs': successful_configs,
                    'config_success_rate': (successful_configs / total_agents_tested * 100) if total_agents_tested > 0 else 0
                },
                'ollama_integration': {
                    'available': ollama_available,
                    'models_count': ollama_models_count,
                    'gpu_usage': ollama_status.get("gpu_usage", {}),
                    'integration_status': 'operational' if ollama_available and ollama_models_count > 0 else 'limited'
                },
                'fallback_mechanisms': {
                    'tests_executed': len(fallback_tests),
                    'success_rate': fallback_success_rate,
                    'status': 'robust' if fallback_success_rate > 80 else 'needs_improvement'
                },
                'performance_overview': {
                    'agents_benchmarked': len(performance_metrics),
                    'total_tests': sum(len(agent_tests) for agent_tests in performance_metrics.values())
                },
                'pattern_factory_status': PATTERN_FACTORY_AVAILABLE,
                'derniere_maj': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Erreur collecte m√©triques mod√®les: {e}")
            return {'erreur': str(e), 'metriques_partielles': True}

    async def _generer_rapport_models_integration(self, context: Dict, metriques: Dict, timestamp: datetime) -> Dict[str, Any]:
        """G√©n√®re un rapport strat√©gique global d'int√©gration des mod√®les"""
        
        # Calcul des scores d'int√©gration
        agent_config = metriques.get('agents_configuration', {})
        ollama_integration = metriques.get('ollama_integration', {})
        fallback_mechs = metriques.get('fallback_mechanisms', {})
        
        score_integration_global = min(100, agent_config.get('config_success_rate', 0))
        score_ollama = 100 if ollama_integration.get('available', False) and ollama_integration.get('models_count', 0) > 2 else 50
        score_fallback = fallback_mechs.get('success_rate', 0)
        
        score_global = (score_integration_global + score_ollama + score_fallback) / 3
        
        # Recommandations strat√©giques
        recommandations = []
        if score_integration_global < 80:
            recommandations.append("üîß CONFIGURATION: Optimiser configuration agents-mod√®les")
        if not ollama_integration.get('available', False):
            recommandations.append("üöÄ OLLAMA: Installer et configurer Ollama RTX3090")
        if score_fallback < 80:
            recommandations.append("üîÑ FALLBACK: Am√©liorer m√©canismes de basculement")
        
        if not recommandations:
            recommandations.append("‚úÖ EXCELLENT: Int√©gration mod√®les IA optimale")
            
        return {
            'type_rapport': 'integration_modeles_ia',
            'timestamp': timestamp.isoformat(),
            'agent_id': instance.agent_id,
            'specialisation': 'test_validation_models',
            
            'resume_executif': {
                'score_integration_global': round(score_global, 1),
                'score_configuration_agents': score_integration_global,
                'score_ollama_local': score_ollama,
                'score_fallback_cloud': score_fallback,
                'statut_integration': 'OPTIMAL' if score_global > 80 else 'ATTENTION' if score_global > 60 else 'CRITIQUE'
            },
            
            'analyse_configuration': {
                'agents_testes': agent_config.get('total_tested', 0),
                'configurations_reussies': agent_config.get('successful_configs', 0),
                'taux_succes_config': agent_config.get('config_success_rate', 0),
                'pattern_factory_actif': metriques.get('pattern_factory_status', False)
            },
            
            'integration_ollama': {
                'disponible': ollama_integration.get('available', False),
                'modeles_locaux': ollama_integration.get('models_count', 0),
                'utilisation_gpu': ollama_integration.get('gpu_usage', {}),
                'statut': ollama_integration.get('integration_status', 'unknown')
            },
            
            'fallback_cloud': {
                'tests_executes': fallback_mechs.get('tests_executed', 0),
                'taux_succes': fallback_mechs.get('success_rate', 0),
                'statut_robustesse': fallback_mechs.get('status', 'unknown')
            },
            
            'recommandations_strategiques': recommandations,
            
            'prochaines_actions': [
                "Valider configuration mod√®les production",
                "Optimiser performance Ollama RTX3090",
                "Tester m√©canismes fallback en charge",
                "Renforcer intelligence technique mod√®les"
            ],
            
            'metadonnees': {
                'version_rapport': '1.0',
                'agent_version': instance.version,
                'specialisation': 'test_models_integration',
                'fiabilite_donnees': 'haute' if not metriques.get('metriques_partielles') else 'partielle'
            }
        }

    async def _generer_rapport_performance_models(self, context: Dict, metriques: Dict, timestamp: datetime) -> Dict[str, Any]:
        """G√©n√®re un rapport sp√©cialis√© sur les performances des mod√®les"""
        
        perf_overview = metriques.get('performance_overview', {})
        ollama_integration = metriques.get('ollama_integration', {})
        
        return {
            'type_rapport': 'performance_modeles_ia',
            'timestamp': timestamp.isoformat(),
            'focus_performance': 'local_vs_cloud',
            'agents_benchmarkes': perf_overview.get('agents_benchmarked', 0),
            'tests_performance': perf_overview.get('total_tests', 0),
            'ollama_performance': {
                'modeles_disponibles': ollama_integration.get('models_count', 0),
                'utilisation_gpu': ollama_integration.get('gpu_usage', {}),
                'statut_performance': 'optimal' if ollama_integration.get('available', False) else 'degraded'
            },
            'recommandation_prioritaire': 'Optimiser RTX3090' if ollama_integration.get('available', False) else 'Installer Ollama'
        }

    async def _generer_rapport_fallback_analysis(self, context: Dict, metriques: Dict, timestamp: datetime) -> Dict[str, Any]:
        """G√©n√®re un rapport sp√©cialis√© sur l'analyse des m√©canismes de fallback"""
        
        fallback_mechs = metriques.get('fallback_mechanisms', {})
        
        return {
            'type_rapport': 'analyse_fallback_mechanisms',
            'timestamp': timestamp.isoformat(),
            'tests_fallback': fallback_mechs.get('tests_executed', 0),
            'taux_succes_fallback': fallback_mechs.get('success_rate', 0),
            'robustesse_systeme': fallback_mechs.get('status', 'unknown'),
            'recommandation_fallback': 'Syst√®me robuste' if fallback_mechs.get('success_rate', 0) > 80 else 'Am√©liorer fallback'
        }

    async def _generer_rapport_ollama_validation(self, context: Dict, metriques: Dict, timestamp: datetime) -> Dict[str, Any]:
        """G√©n√®re un rapport sp√©cialis√© sur la validation Ollama RTX3090"""
        
        ollama_integration = metriques.get('ollama_integration', {})
        
        return {
            'type_rapport': 'validation_ollama_rtx3090',
            'timestamp': timestamp.isoformat(),
            'ollama_disponible': ollama_integration.get('available', False),
            'modeles_locaux_count': ollama_integration.get('models_count', 0),
            'gpu_utilization': ollama_integration.get('gpu_usage', {}),
            'statut_validation': ollama_integration.get('integration_status', 'unknown'),
            'recommandation_ollama': 'RTX3090 optimal' if ollama_integration.get('models_count', 0) > 2 else 'Configurer mod√®les'
        }

    async def generer_rapport_markdown(self, rapport_json: Dict[str, Any], type_rapport: str, context: Dict) -> str:
        """
        üß™ G√©n√®re un rapport Markdown format√© pour les tests mod√®les IA
        
        Args:
            rapport_json: Donn√©es du rapport JSON
            type_rapport: Type du rapport
            context: Contexte additionnel
            
        Returns:
            Rapport format√© en Markdown
        """
        
        if type_rapport == 'models_integration':
            return instance._generer_markdown_models_integration(rapport_json)
        elif type_rapport == 'performance_models':
            return instance._generer_markdown_performance_models(rapport_json)
        elif type_rapport == 'fallback_analysis':
            return instance._generer_markdown_fallback_analysis(rapport_json)
        elif type_rapport == 'ollama_validation':
            return instance._generer_markdown_ollama_validation(rapport_json)
        else:
            return instance._generer_markdown_models_integration(rapport_json)

    def _generer_markdown_models_integration(self, rapport: Dict[str, Any]) -> str:
        """
        G√©n√®re dynamiquement un rapport Markdown structur√© et d√©taill√© pour l'int√©gration des mod√®les IA,
        au m√™me niveau d'exigence que l'agent monitoring (titre, scores, statuts, recommandations, issues, d√©tails techniques, m√©triques, m√©tadonn√©es).
        """
        from datetime import datetime
        timestamp = rapport.get('timestamp', datetime.now().isoformat())
        agent_id = rapport.get('agent_id', 'N/A')
        agent_file = agent_id
        score = rapport.get('resume_executif', {}).get('score_integration_global', 0)
        niveau_qualite = rapport.get('resume_executif', {}).get('statut_integration', 'N/A')
        conformite = '‚úÖ CONFORME' if score >= 70 else '‚ùå NON CONFORME'
        issues_count = 0
        issues_list = []
        # D√©tection des issues critiques
        if score < 70:
            if rapport.get('resume_executif', {}).get('score_configuration_agents', 0) < 70:
                issues_list.append("Configuration agents-mod√®les insuffisante")
            if rapport.get('resume_executif', {}).get('score_ollama_local', 0) < 70:
                issues_list.append("Ollama RTX3090 indisponible ou sous-utilis√©")
            if rapport.get('resume_executif', {}).get('score_fallback_cloud', 0) < 70:
                issues_list.append("Fallback cloud non robuste ou absent")
        issues_count = len(issues_list)
        if issues_count == 0:
            issues_list = ["Aucun issue critique majeur d√©tect√©. L'int√©gration est optimale."]
        # Architecture
        archi = rapport.get('analyse_configuration', {})
        ollama = rapport.get('integration_ollama', {})
        fallback = rapport.get('fallback_cloud', {})
        recommandations = rapport.get('recommandations_strategiques', [])
        details_tech = {
            'configurations_reussies': archi.get('configurations_reussies', 0),
            'agents_testes': archi.get('agents_testes', 0),
            'taux_succes_config': archi.get('taux_succes_config', 0),
            'pattern_factory_actif': archi.get('pattern_factory_actif', False),
            'ollama_disponible': ollama.get('disponible', False),
            'modeles_locaux': ollama.get('modeles_locaux', 0),
            'statut_ollama': ollama.get('statut', 'N/A'),
            'tests_fallback': fallback.get('tests_executes', 0),
            'taux_succes_fallback': fallback.get('taux_succes', 0),
            'statut_fallback': fallback.get('statut_robustesse', 'N/A'),
        }
        metriques = rapport.get('resume_executif', {})
        # Bloc Markdown
        md = f"""# ü§ñ **RAPPORT STRAT√âGIQUE INT√âGRATION MOD√àLES IA : {agent_id}**\n\n"""
        md += f"**Date :** {timestamp}\n**Module :** {agent_file}\n**Score Global** : {score:.1f}/100\n**Niveau Qualit√©** : {niveau_qualite}\n**Conformit√©** : {conformite}\n**Issues Critiques** : {issues_count}\n\n"
        md += "## üèóÔ∏è Architecture Int√©gration Mod√®les IA\n"
        md += f"- **Agents test√©s :** {details_tech['agents_testes']}\n"
        md += f"- **Configurations r√©ussies :** {details_tech['configurations_reussies']}\n"
        md += f"- **Taux de succ√®s configuration :** {details_tech['taux_succes_config']:.1f}%\n"
        md += f"- **Pattern Factory :** {'‚úÖ Actif' if details_tech['pattern_factory_actif'] else '‚ùå Inactif'}\n"
        md += f"- **Ollama RTX3090 disponible :** {'‚úÖ Oui' if details_tech['ollama_disponible'] else '‚ùå Non'}\n"
        md += f"- **Mod√®les locaux Ollama :** {details_tech['modeles_locaux']}\n"
        md += f"- **Statut Ollama :** {details_tech['statut_ollama']}\n"
        md += f"- **Tests fallback ex√©cut√©s :** {details_tech['tests_fallback']}\n"
        md += f"- **Taux de succ√®s fallback :** {details_tech['taux_succes_fallback']:.1f}%\n"
        md += f"- **Robustesse fallback :** {details_tech['statut_fallback']}\n\n"
        md += "## üîß Recommandations Strat√©giques\n"
        for rec in recommandations:
            md += f"- {rec}\n"
        md += "\n## üö® Issues Critiques\n"
        for issue in issues_list:
            md += f"- üî¥ {issue}\n"
        md += "\n## üìã D√©tails Techniques\n"
        md += f"- **Pattern Factory actif :** {'‚úÖ' if details_tech['pattern_factory_actif'] else '‚ùå'}\n"
        md += f"- **Ollama disponible :** {'‚úÖ' if details_tech['ollama_disponible'] else '‚ùå'}\n"
        md += f"- **Mod√®les locaux :** {details_tech['modeles_locaux']}\n"
        md += f"- **Tests fallback :** {details_tech['tests_fallback']}\n"
        md += f"- **Taux de succ√®s fallback :** {details_tech['taux_succes_fallback']:.1f}%\n"
        md += f"- **Robustesse fallback :** {details_tech['statut_fallback']}\n"
        md += f"- **Agents test√©s :** {details_tech['agents_testes']}\n"
        md += f"- **Configurations r√©ussies :** {details_tech['configurations_reussies']}\n"
        md += f"- **Taux de succ√®s configuration :** {details_tech['taux_succes_config']:.1f}%\n\n"
        md += "## üìä M√©triques D√©taill√©es\n"
        md += f"- **Score global int√©gration :** {metriques.get('score_integration_global', 0):.1f}/100\n"
        md += f"- **Score configuration agents :** {metriques.get('score_configuration_agents', 0):.1f}/100\n"
        md += f"- **Score Ollama local :** {metriques.get('score_ollama_local', 0):.1f}/100\n"
        md += f"- **Score fallback cloud :** {metriques.get('score_fallback_cloud', 0):.1f}/100\n"
        md += f"- **Statut int√©gration :** {metriques.get('statut_integration', 'N/A')}\n\n"
        md += "---\n"
        md += f"*Rapport g√©n√©r√© automatiquement par {agent_id} - {timestamp}*\n"
        md += f"*ü§ñ Sp√©cialiste Int√©gration Mod√®les IA*\n"
        md += f"*üìÇ Sauvegard√© dans : reports/agent_test_models_integration/*\n"
        return md

    def _generer_markdown_performance_models(self, rapport: Dict[str, Any]) -> str:
        """G√©n√®re le rapport Markdown pour les performances des mod√®les"""
        
        return f"""# ‚ö° Rapport Performance Mod√®les IA

**Focus:** {rapport.get('focus_performance', 'N/A')}  
**Timestamp:** {rapport.get('timestamp', 'N/A')}

## üìä M√©triques Performance

- **Agents benchmark√©s:** {rapport.get('agents_benchmarkes', 0)}
- **Tests performance:** {rapport.get('tests_performance', 0)}

## üöÄ Performance Ollama

- **Mod√®les disponibles:** {rapport.get('ollama_performance', {}).get('modeles_disponibles', 0)}
- **Statut performance:** {rapport.get('ollama_performance', {}).get('statut_performance', 'N/A')}

## üéØ Recommandation Prioritaire

**{rapport.get('recommandation_prioritaire', 'N/A')}**
"""

    def _generer_markdown_fallback_analysis(self, rapport: Dict[str, Any]) -> str:
        """G√©n√®re le rapport Markdown pour l'analyse des fallbacks"""
        
        return f"""# üîÑ Analyse M√©canismes Fallback

**Timestamp:** {rapport.get('timestamp', 'N/A')}

## üìä Statistiques Fallback

- **Tests fallback:** {rapport.get('tests_fallback', 0)}
- **Taux de succ√®s:** {rapport.get('taux_succes_fallback', 0):.1f}%
- **Robustesse syst√®me:** {rapport.get('robustesse_systeme', 'N/A')}

## üéØ Recommandation

**{rapport.get('recommandation_fallback', 'N/A')}**
"""

    def _generer_markdown_ollama_validation(self, rapport: Dict[str, Any]) -> str:
        """G√©n√®re le rapport Markdown pour la validation Ollama"""
        
        return f"""# üöÄ Validation Ollama RTX3090

**Timestamp:** {rapport.get('timestamp', 'N/A')}

## üìä Statut Ollama

- **Disponible:** {'‚úÖ Oui' if rapport.get('ollama_disponible', False) else '‚ùå Non'}
- **Mod√®les locaux:** {rapport.get('modeles_locaux_count', 0)}
- **Statut validation:** {rapport.get('statut_validation', 'N/A')}

## üéØ Recommandation

**{rapport.get('recommandation_ollama', 'N/A')}**
"""


# üß™ Fonction de test et d√©monstration
async def main():
    """Fonction principale de test de l'agent"""
    
    print("üß™ Test Agent Mod√®les IA - Version Propre")
    print("=" * 50)
    
    # Cr√©ation de l'agent
    agent = AgentTestModelsIntegration()
    
    # D√©marrage
    startup_result = await agent.startup()
    print(f"üöÄ D√©marrage: {startup_result}")
    
    # Health check
    health = await agent.health_check()
    print(f"üè• Health Check: {health}")
    
    # Test suite compl√®te
    print("\nüß™ Lancement suite de tests...")
    test_result = await agent.execute_task({"type": "complete_test_suite"})
    print(f"üìä R√©sultats tests: {test_result.get('success', False)}")
    
    # Test g√©n√©ration rapport strat√©gique
    print("\nüìä G√©n√©ration rapport strat√©gique...")
    rapport_task = {
        "type": "generate_strategic_report",
        "context": {"environnement": "test", "focus": "integration"},
        "type_rapport": "models_integration",
        "format_sortie": "markdown"
    }
    
    rapport_result = await agent.execute_task(rapport_task)
    
    if rapport_result.get('success', False):
        print("‚úÖ Rapport strat√©gique g√©n√©r√© avec succ√®s")
        if 'fichier_sauvegarde' in rapport_result.get('data', {}):
            print(f"üìÅ Sauvegard√©: {rapport_result['data']['fichier_sauvegarde']}")
    else:
        print(f"‚ùå Erreur g√©n√©ration rapport: {rapport_result.get('error', 'Inconnue')}")
    
    # Arr√™t
    shutdown_result = await agent.shutdown()
    print(f"üõë Arr√™t: {shutdown_result}")
    
    print("\n‚úÖ Test termin√© avec succ√®s!")

    # ‚úÖ M√âTHODES STANDARDIS√âES DE RAPPORT

    def _calculate_report_score(self, metrics: Dict[str, Any]) -> int:
        """Calcule le score global du rapport bas√© sur les m√©triques."""
        score = 0
        issues_critiques = []
        
        # Logique de scoring sp√©cifique √† l'agent
        # √Ä adapter selon le type d'agent
        
        return score
    
    def _assess_conformity(self, score: int) -> str:
        """√âvalue la conformit√© bas√©e sur le score."""
        if score >= 90:
            return "‚úÖ CONFORME - OPTIMAL"
        elif score >= 70:
            return "‚úÖ CONFORME - ACCEPTABLE"
        else:
            return "‚ùå NON CONFORME - CRITIQUE"
    
    def _get_quality_level(self, score: int) -> str:
        """D√©termine le niveau de qualit√©."""
        if score >= 90:
            return "OPTIMAL"
        elif score >= 70:
            return "ACCEPTABLE"
        else:
            return "CRITIQUE"
    
    def _generate_recommendations(self, metrics: Dict[str, Any], issues: List[str]) -> List[str]:
        """G√©n√®re les recommandations bas√©es sur l'analyse."""
        recommendations = []
        
        # Logique de g√©n√©ration de recommandations
        # √Ä adapter selon le type d'agent
        
        return recommendations
    
    def _generate_standard_report(self, context: Dict, metrics: Dict, timestamp) -> Dict[str, Any]:
        """G√©n√®re un rapport selon le format standard de l'agent 06."""
        
        score = self._calculate_report_score(metrics)
        conformity = self._assess_conformity(score)
        quality_level = self._get_quality_level(score)
        
        agent_filename = Path(__file__).name
        
        # Issues critiques (√† personnaliser selon l'agent)
        issues_critiques = []
        
        return {
            'agent_id': getattr(self, 'agent_id', 'unknown'),
            'agent_file_name': agent_filename,
            'type_rapport': 'standard',  # √Ä personnaliser
            'timestamp': timestamp.isoformat(),
            'specialisation': 'Agent Sp√©cialis√©',  # √Ä personnaliser
            'score_global': score,
            'niveau_qualite': quality_level,
            'conformite': conformity,
            'signature_cryptographique': 'N/A (Fonctionnalit√© non impl√©ment√©e pour cet agent)',
            'issues_critiques_identifies': len(issues_critiques),
            'architecture': {
                'description': "Description de l'architecture de l'agent",
                'statut_operationnel': f"Syst√®me {getattr(self, 'agent_id', 'unknown')} op√©rationnel.",
                'confirmation_specialisation': f"{getattr(self, 'agent_id', 'unknown')} confirm√© comme sp√©cialiste.",
                'objectifs_principaux': [
                    "Objectif principal 1",
                    "Objectif principal 2",
                    "Objectif principal 3"
                ],
                'technologies_cles': ["Technologie 1", "Technologie 2"]
            },
            'recommandations': instance._generate_recommendations(metrics, issues_critiques),
            'issues_critiques_details': issues_critiques if issues_critiques else [
                "Aucun issue critique majeur d√©tect√©. Le syst√®me fonctionne dans les param√®tres attendus."
            ],
            'details_techniques': {
                'strategie': "Strat√©gie technique de l'agent",
                'composants_actifs': [],
                'metriques_collectees': metrics
            },
            'metriques_detaillees': {
                'score_global': {'actuel': score, 'cible': 100},
                'conformite_pourcentage': {'actuel': score, 'cible': 100, 'unite': '%'}
            },
            'impact_business': {
                'criticite': 'MOYENNE' if score >= 70 else 'HAUTE',
                'domaines_impactes': [],
                'actions_requises': []
            }
        }


    def _generate_markdown_report(self, rapport_json: Dict, context: Dict, timestamp) -> str:
        """G√©n√®re un rapport Markdown selon le format standard."""
        
        agent_name = rapport_json.get('agent_id', 'Agent Inconnu')
        type_rapport = rapport_json.get('type_rapport', 'standard')
        score = rapport_json.get('score_global', 0)
        quality = rapport_json.get('niveau_qualite', 'UNKNOWN')
        conformity = rapport_json.get('conformite', 'NON √âVALU√â')
        
        markdown_content = f"""# üìä RAPPORT STRAT√âGIQUE : {agent_name.upper()}

## üéØ R√âSUM√â EX√âCUTIF

**Agent :** {agent_name}  
**Type de Rapport :** {type_rapport}  
**Date de G√©n√©ration :** {timestamp.strftime('%Y-%m-%d %H:%M:%S')}  
**Score Global :** {score}/100  
**Niveau de Qualit√© :** {quality}  
**Conformit√© :** {conformity}  

## üìà ANALYSE GLOBALE

### Score de Performance
- **Score Actuel :** {score}/100
- **Objectif :** 100/100
- **Statut :** {'üü¢ ACCEPTABLE' if score >= 70 else 'üî¥ CRITIQUE'}

### Architecture
{rapport_json.get('architecture', {}).get('description', 'Description non disponible')}

**Objectifs Principaux :**
"""
        
        # Ajouter les objectifs
        for obj in rapport_json.get('architecture', {}).get('objectifs_principaux', []):
            markdown_content += f"- {obj}\n"
        
        markdown_content += f"""
**Technologies Cl√©s :**
"""
        
        # Ajouter les technologies
        for tech in rapport_json.get('architecture', {}).get('technologies_cles', []):
            markdown_content += f"- {tech}\n"
        
        markdown_content += f"""

## üîç RECOMMANDATIONS

"""
        
        # Ajouter les recommandations
        for reco in rapport_json.get('recommandations', []):
            markdown_content += f"- {reco}\n"
        
        markdown_content += f"""

## ‚ö†Ô∏è ISSUES CRITIQUES

"""
        
        # Ajouter les issues critiques
        for issue in rapport_json.get('issues_critiques_details', []):
            markdown_content += f"- {issue}\n"
        
        markdown_content += f"""

## üìä M√âTRIQUES D√âTAILL√âES

### Performance Globale
- **Score Global :** {rapport_json.get('metriques_detaillees', {}).get('score_global', {}).get('actuel', 0)}/{rapport_json.get('metriques_detaillees', {}).get('score_global', {}).get('cible', 100)}
- **Conformit√© :** {rapport_json.get('metriques_detaillees', {}).get('conformite_pourcentage', {}).get('actuel', 0)}%

## üéØ IMPACT BUSINESS

**Criticit√© :** {rapport_json.get('impact_business', {}).get('criticite', 'NON √âVALU√â')}

### Domaines Impact√©s
"""
        
        # Ajouter les domaines impact√©s
        for domaine in rapport_json.get('impact_business', {}).get('domaines_impactes', []):
            markdown_content += f"- {domaine}\n"
        
        markdown_content += f"""

### Actions Requises
"""
        
        # Ajouter les actions requises
        for action in rapport_json.get('impact_business', {}).get('actions_requises', []):
            markdown_content += f"- {action}\n"
        
        markdown_content += f"""

---
*Rapport g√©n√©r√© automatiquement par {agent_name} - NextGeneration System*  
*Timestamp: {timestamp.isoformat()}*
"""
        
        return markdown_content



if __name__ == "__main__":
    print("üß™ Agent Test Mod√®les IA - D√©marrage")
    asyncio.run(main()) 