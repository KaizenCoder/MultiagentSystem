#!/usr/bin/env python3
"""

# üîß CONVERTI AUTOMATIQUEMENT SYNC ‚Üí ASYNC
# Date: 2025-06-19 19h35 - Correction architecture Pattern Factory
# Raison: Harmonisation async/sync avec core/agent_factory_architecture.py

üöß DRAFT VERSION üöß
Planificateur pour l'Agent M√©ta-Strat√©gique - VERSION DRAFT/PROTOTYPE
Mission: Ex√©cuter p√©riodiquement l'analyse strat√©gique et g√©n√©rer les rapports

‚ö†Ô∏è ATTENTION: CETTE VERSION EST UN PROTOTYPE EN D√âVELOPPEMENT
- Ne pas utiliser en production
- Fonctionnalit√©s en cours de test et validation
- Rapports g√©n√©r√©s √† titre exp√©rimental uniquement

Fonctionnalit√©s:
- Planification automatique des analyses
- Int√©gration avec l'API GitHub pour les m√©triques CI/CD
- Notifications des insights critiques
- Archivage des rapports historiques
"""

import asyncio
import json
import sys
from pathlib import Path
from core import logging_manager
import schedule
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import logging
import time
import importlib.util
import importlib

# Import de notre agent m√©ta-strat√©gique
AGENT_META_PATH = Path(__file__).parent.parent / '20250621_1527_agent_racine_repertoire' / 'agent_meta_strategique.py'
if AGENT_META_PATH.exists():
    spec = importlib.util.spec_from_file_location('agent_meta_strategique', str(AGENT_META_PATH))
    agent_meta_module = importlib.util.module_from_spec(spec)
    sys.modules['agent_meta_strategique'] = agent_meta_module
    try:
        spec.loader.exec_module(agent_meta_module)
        AgentMetaStrategique = agent_meta_module.AgentMetaStrategique
    except Exception as e:
        print(f"ERREUR CRITIQUE: L'import de AgentMetaStrategique a √©chou√© malgr√© la d√©tection du fichier. D√©tail: {e}")
        sys.exit(1)
else:
    print(f"ERREUR CRITIQUE: Le fichier agent_meta_strategique.py est introuvable √† l'emplacement attendu: {AGENT_META_PATH}")
    sys.exit(1)

class AgentMetaStrategiqueScheduler:
    """Planificateur universel pour agents Pattern Factory"""
    def __init__(self, workspace_root: Path = None, config_path: str = "config/meta_strategique_config.json", agent_class_name: str = "Agent111AuditeurQualite", agent_module_path: str = "agents.agent_111_auditeur_qualite", task_method: str = "execute_task", task_params: dict = None):
        self.workspace_root = workspace_root if workspace_root else Path(__file__).parent.parent
        self.config_path = self.workspace_root / config_path
        self.logger = self._setup_logging()
        
        # Configuration par d√©faut
        self.default_config = {
            "execution_schedule": {
                "daily_report": "09:00",
                "weekly_deep_analysis": "MON:08:00",
                "critical_monitoring": 30  # minutes
            },
            "notifications": {
                "email_enabled": False,
                "email_recipients": [],
                "smtp_server": "smtp.gmail.com",
                "smtp_port": 587,
                "webhook_url": None
            },
            "github_integration": {
                "enabled": False,
                "api_token": None,
                "repo_owner": "nextgeneration",
                "repo_name": "nextgeneration"
            },
            "analysis_settings": {
                "retention_days": 30,
                "critical_threshold": 0.8,
                "archive_reports": True
            }
        }
        
        self.config = self.load_config()
        self.task_method = task_method
        self.task_params = task_params or {"task": None}
        # Import dynamique de l'agent
        try:
            agent_module = importlib.import_module(agent_module_path)
            agent_class = getattr(agent_module, agent_class_name)
            self.agent = agent_class(workspace_root=self.workspace_root)
        except Exception as e:
            self.logger.critical(f"Impossible d'instancier {agent_class_name} depuis {agent_module_path}: {e}")
            sys.exit(1)
            
        self.last_analysis_results = None

    def _setup_logging(self):
        """Configuration du logging pour le scheduler."""
        logger = logging.getLogger(self.__class__.__name__)
        logger.setLevel(logging.INFO)

        if not logger.handlers:
            log_dir = self.workspace_root / "logs" / "agents" / "meta_strategique_scheduler"
            log_dir.mkdir(parents=True, exist_ok=True)
            log_file = log_dir / "scheduler.log"

            # Handler Fichier
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            file_handler.setFormatter(file_formatter)
            logger.addHandler(file_handler)

            # Handler Console
            console_handler = logging.StreamHandler(sys.stdout)
            console_formatter = logging.Formatter('üìÖ (Scheduler) - %(levelname)s - %(message)s')
            console_handler.setFormatter(console_formatter)
            logger.addHandler(console_handler)

        return logger
        
    def load_config(self) -> Dict[str, Any]:
        """Chargement de la configuration"""
        if self.config_path.exists():
            try:
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    # Fusion avec la config par d√©faut
                    merged_config = self.default_config.copy()
                    merged_config.update(config)
                    return merged_config
            except Exception as e:
                self.logger.error(f"Erreur chargement config: {e}")
        
        # Cr√©er la configuration par d√©faut
        self.save_config(self.default_config)
        return self.default_config
    
    def save_config(self, config: Dict[str, Any]):
        """Sauvegarde de la configuration"""
        self.config_path.parent.mkdir(parents=True, exist_ok=True)
        with open(self.config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
    
    async def run_scheduler(self):
        """D√©marrage du planificateur en mode asynchrone"""
        self.logger.info("üöß D√©marrage du planificateur Agent M√©ta-Strat√©gique ASYNC")
        self.setup_schedules()
        self.logger.info("üîç Ex√©cution d'analyse initiale...")
        await self.execute_daily_analysis()
        try:
            while True:
                await self.run_pending_async()
                await asyncio.sleep(60)
        except KeyboardInterrupt:
            self.logger.info("üõë Arr√™t du planificateur")

    def setup_schedules(self):
        """Configuration des planifications (async compatible)"""
        config = self.config["execution_schedule"]
        self.schedules = []
        # Rapport quotidien
        daily_time = config["daily_report"]
        self.schedules.append((self.parse_time(daily_time), self.execute_daily_analysis))
        self.logger.info(f"üìÖ Rapport quotidien planifi√© √† {daily_time}")
        # Analyse hebdomadaire approfondie
        weekly_schedule = config["weekly_deep_analysis"]
        if ":" in weekly_schedule:
            day, time_str = weekly_schedule.split(":")
            self.schedules.append((self.parse_time(time_str, day=day), self.execute_weekly_deep_analysis))
            self.logger.info(f"üìä Analyse hebdomadaire planifi√©e {day} √† {time_str}")
        # Monitoring critique
        critical_interval = config["critical_monitoring"]
        self.critical_monitoring_interval = critical_interval
        self.last_critical_monitoring = datetime.now()
        self.logger.info(f"üö® Monitoring critique toutes les {critical_interval} minutes")

    def parse_time(self, time_str, day=None):
        # Retourne un tuple (heure, minute, jour) pour la planification
        h, m = map(int, time_str.split(":"))
        return (h, m, day)

    async def run_pending_async(self):
        now = datetime.now()
        for sched in self.schedules:
            h, m, day = sched[0]
            if (now.hour, now.minute) == (h, m):
                if day is None or now.strftime("%a").upper().startswith(day.upper()[:3]):
                    await self._maybe_async_call(sched[1])
        # Monitoring critique
        if (datetime.now() - self.last_critical_monitoring).total_seconds() > self.critical_monitoring_interval * 60:
            await self._maybe_async_call(self.execute_critical_monitoring)
            self.last_critical_monitoring = datetime.now()

    async def _maybe_async_call(self, func):
        if asyncio.iscoroutinefunction(func):
            await func()
        else:
            func()

    async def execute_daily_analysis(self):
        self.logger.info("üìä D√©but analyse quotidienne (universel)")
        try:
            # Ex√©cution de la t√¢che g√©n√©rique
            if hasattr(self.agent, self.task_method):
                method = getattr(self.agent, self.task_method)
                task_obj = self.task_params.get("task")
                if not task_obj:
                    # Construction dynamique si param√®tres individuels fournis
                    from core.agent_factory_architecture import Task
                    task_type = self.task_params.get("type")
                    task_params = self.task_params.get("params", {})
                    if task_type:
                        task_obj = Task(type=task_type, params=task_params)
                        self.logger.info(f"[DEBUG] T√¢che construite dynamiquement : type={task_type}, params={task_params}")
                    else:
                        self.logger.error("‚ùå Aucun objet t√¢che ni type de t√¢che fourni dans task_params !")
                        return
                else:
                    self.logger.info(f"[DEBUG] T√¢che re√ßue directement dans task_params : {task_obj}")
                # DEBUG : log du type et des attributs de la t√¢che
                self.logger.info(f"[DEBUG] Type de la t√¢che : {type(task_obj)}")
                self.logger.info(f"[DEBUG] Attributs de la t√¢che : {dir(task_obj)}")
                if hasattr(task_obj, '__dict__'):
                    self.logger.info(f"[DEBUG] __dict__ de la t√¢che : {task_obj.__dict__}")
                result = await method(task_obj)
                self.logger.info(f"‚úÖ T√¢che '{self.task_method}' ex√©cut√©e avec succ√®s. R√©sultat: {result}")
                # G√©n√©ration d'un artefact JSON s√©rialisable
                artefacts_dir = self.workspace_root / 'logs' / 'agents' / 'meta_strategique_scheduler' / 'artefacts'
                artefacts_dir.mkdir(parents=True, exist_ok=True)
                artefact_path = artefacts_dir / f"artefact_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                # Conversion du r√©sultat en dict si possible
                if hasattr(result, 'to_dict'):
                    result_data = result.to_dict()
                elif hasattr(result, '__dict__'):
                    result_data = dict(result.__dict__)
                else:
                    result_data = result
                with open(artefact_path, 'w', encoding='utf-8') as f:
                    json.dump(result_data, f, ensure_ascii=False, indent=2, default=str)
            else:
                self.logger.error(f"‚ùå M√©thode '{self.task_method}' non trouv√©e sur l'agent.")
        except Exception as e:
            self.logger.error(f"‚ùå Erreur ex√©cution t√¢che universelle: {e}", exc_info=True)

    async def execute_weekly_deep_analysis(self):
        self.logger.info("üîç D√©but analyse hebdomadaire approfondie (async)")
        try:
            await self.execute_daily_analysis()
            trends_analysis = await asyncio.to_thread(self.analyze_weekly_trends)
            weekly_report = await asyncio.to_thread(self.generate_weekly_executive_report, trends_analysis)
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            weekly_report_path = self.agent.reports_path / f"RAPPORT_HEBDOMADAIRE_{timestamp}.md"
            with open(weekly_report_path, 'w', encoding='utf-8') as f:
                f.write(weekly_report)
            self.logger.info(f"‚úÖ Analyse hebdomadaire termin√©e - Rapport: {weekly_report_path}")
        except Exception as e:
            self.logger.error(f"‚ùå Erreur analyse hebdomadaire: {e}")

    async def execute_critical_monitoring(self):
        try:
            critical_metrics = await asyncio.to_thread(self.check_critical_metrics)
            if critical_metrics["alerts"]:
                self.logger.warning(f"üö® {len(critical_metrics['alerts'])} alertes critiques d√©tect√©es")
                if self.config["notifications"]["email_enabled"]:
                    await asyncio.to_thread(self.send_critical_alert, critical_metrics["alerts"])
                await asyncio.to_thread(self.execute_emergency_analysis, critical_metrics)
        except Exception as e:
            self.logger.error(f"‚ùå Erreur monitoring critique: {e}")

    def check_critical_metrics(self) -> Dict[str, Any]:
        """V√©rification rapide des m√©triques critiques"""
        alerts = []
        
        # V√©rification des logs r√©cents
        recent_logs = list(self.agent.logs_path.glob("*.log"))
        for log_file in recent_logs[-5:]:  # 5 derniers logs
            if log_file.stat().st_size > 1024 * 1024:  # > 1MB
                try:
                    with open(log_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                        error_count = content.count("ERROR")
                        if error_count > 10:
                            alerts.append({
                                "type": "high_error_rate",
                                "source": log_file.name,
                                "error_count": error_count,
                                "severity": "CRITICAL"
                            })
                except Exception:
                    continue
        
        # V√©rification des m√©triques de performance r√©centes
        recent_metrics = list(self.agent.metrics_path.glob("*.json"))
        if recent_metrics:
            latest_metric = max(recent_metrics, key=lambda x: x.stat().st_mtime)
            if datetime.now() - datetime.fromtimestamp(latest_metric.stat().st_mtime) > timedelta(hours=2):
                alerts.append({
                    "type": "stale_metrics",
                    "message": "Aucune m√©trique r√©cente d√©tect√©e",
                    "severity": "HIGH"
                })
        
        return {"alerts": alerts, "timestamp": datetime.now().isoformat()}
    
    def handle_critical_insights(self, critical_insights: List[Dict[str, Any]]):
        """Gestion des insights critiques"""
        self.logger.warning(f"üö® {len(critical_insights)} insights critiques d√©tect√©s")
        
        # Notification par email si configur√©e
        if self.config["notifications"]["email_enabled"]:
            self.send_critical_insights_notification(critical_insights)
        
        # Webhook si configur√©
        webhook_url = self.config["notifications"].get("webhook_url")
        if webhook_url:
            self.send_webhook_notification(critical_insights, webhook_url)
        
        # Log d√©taill√©
        for insight in critical_insights:
            self.logger.critical(f"INSIGHT CRITIQUE: {insight['title']} - {insight['description']}")
    
    def send_critical_alert(self, alerts: List[Dict[str, Any]]):
        """Envoi d'alerte critique par email"""
        if not self.config["notifications"]["email_enabled"]:
            return
        
        try:
            subject = f"üö® ALERTE CRITIQUE - NextGeneration - {datetime.now().strftime('%Y-%m-%d %H:%M')}"
            
            body = f"""
ALERTE CRITIQUE D√âTECT√âE

Nombre d'alertes: {len(alerts)}
Timestamp: {datetime.now().isoformat()}

D√âTAILS DES ALERTES:
"""
            
            for alert in alerts:
                body += f"""
- Type: {alert['type']}
- S√©v√©rit√©: {alert['severity']}
- D√©tails: {alert.get('message', 'N/A')}
"""
            
            body += """
Action requise: V√©rifier imm√©diatement le syst√®me NextGeneration.

---
Alerte g√©n√©r√©e automatiquement par l'Agent M√©ta-Strat√©gique
"""
            
            self.send_email(subject, body)
            
        except Exception as e:
            self.logger.error(f"Erreur envoi alerte email: {e}")
    
    def send_email(self, subject: str, body: str):
        """Envoi d'email"""
        config = self.config["notifications"]
        
        msg = MIMEMultipart()
        msg['From'] = config.get("email_from", "agent-meta@nextgeneration.com")
        msg['To'] = ", ".join(config["email_recipients"])
        msg['Subject'] = subject
        
        msg.attach(MIMEText(body, 'plain', 'utf-8'))
        
        server = smtplib.SMTP(config["smtp_server"], config["smtp_port"])
        server.starttls()
        
        # Authentification si configur√©e
        email_user = config.get("email_user")
        email_password = config.get("email_password")
        if email_user and email_password:
            server.login(email_user, email_password)
        
        server.send_message(msg)
        server.quit()
    
    def analyze_weekly_trends(self) -> Dict[str, Any]:
        """Analyse des tendances hebdomadaires"""
        # Collecte des rapports de la semaine
        week_ago = datetime.now() - timedelta(days=7)
        
        weekly_reports = []
        for report_file in self.agent.reports_path.glob("REVUE_STRATEGIQUE_*.md"):
            file_time = datetime.fromtimestamp(report_file.stat().st_mtime)
            if file_time >= week_ago:
                weekly_reports.append({
                    "file": report_file.name,
                    "timestamp": file_time,
                    "size": report_file.stat().st_size
                })
        
        return {
            "reports_analyzed": len(weekly_reports),
            "period_start": week_ago.isoformat(),
            "period_end": datetime.now().isoformat(),
            "trend_summary": "Analyse des tendances hebdomadaires en cours de d√©veloppement"
        }
    
    def generate_weekly_executive_report(self, trends: Dict[str, Any]) -> str:
        """G√©n√©ration du rapport ex√©cutif hebdomadaire"""
        return f"""# üöß RAPPORT EX√âCUTIF HEBDOMADAIRE - VERSION DRAFT üöß
## Agent M√©ta-Strat√©gique PROTOTYPE - Semaine du {datetime.now().strftime('%Y-%m-%d')}

‚ö†Ô∏è **ATTENTION: RAPPORT G√âN√âR√â PAR VERSION EXP√âRIMENTALE**
- **Statut**: DRAFT/PROTOTYPE v0.1.0-draft
- **Usage**: Tests et validation uniquement

---

## üéØ R√âSUM√â DE LA SEMAINE

### Activit√© de Monitoring
- **Rapports g√©n√©r√©s**: {trends['reports_analyzed']}
- **P√©riode analys√©e**: {trends['period_start']} ‚Üí {trends['period_end']}

### Tendances Identifi√©es
{trends['trend_summary']}

---

## üìà √âVOLUTION DES M√âTRIQUES

*Analyse des tendances hebdomadaires en d√©veloppement*

---

## üí° RECOMMANDATIONS STRAT√âGIQUES

1. **Maintenir la surveillance continue** - Le syst√®me de monitoring fonctionne correctement
2. **Analyser les patterns √©mergents** - Identifier les tendances √† long terme
3. **Optimiser les processus** - Am√©liorer l'efficacit√© bas√©e sur les donn√©es collect√©es

---

*Rapport g√©n√©r√© automatiquement par l'Agent M√©ta-Strat√©gique*
*Prochaine analyse: {(datetime.now() + timedelta(days=7)).strftime('%Y-%m-%d')}*
"""
    
    def cleanup_old_reports(self):
        """Nettoyage des anciens rapports"""
        retention_days = self.config["analysis_settings"]["retention_days"]
        cutoff_date = datetime.now() - timedelta(days=retention_days)
        
        cleaned_count = 0
        for report_file in self.agent.reports_path.glob("REVUE_STRATEGIQUE_*.md"):
            file_time = datetime.fromtimestamp(report_file.stat().st_mtime)
            if file_time < cutoff_date:
                if self.config["analysis_settings"]["archive_reports"]:
                    # Archiver au lieu de supprimer
                    archive_path = self.agent.reports_path / "archive"
                    archive_path.mkdir(exist_ok=True)
                    report_file.rename(archive_path / report_file.name)
                else:
                    report_file.unlink()
                cleaned_count += 1
        
        if cleaned_count > 0:
            self.logger.info(f"üßπ {cleaned_count} anciens rapports nettoy√©s/archiv√©s")
    
    def execute_emergency_analysis(self, critical_metrics: Dict[str, Any]):
        """Analyse d'urgence en cas de probl√®me critique"""
        self.logger.critical("üö® Ex√©cution d'analyse d'urgence")
        
        try:
            # Analyse focalis√©e sur le probl√®me
            emergency_analysis = self.agent.analyser_performance_globale()
            
            # Ajout du contexte d'urgence
            emergency_analysis["emergency_context"] = {
                "trigger": "critical_monitoring",
                "alerts": critical_metrics["alerts"],
                "timestamp": datetime.now().isoformat()
            }
            
            # Sauvegarde du rapport d'urgence
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            emergency_report_path = self.agent.reports_path / f"URGENCE_STRATEGIQUE_{timestamp}.json"
            
            with open(emergency_report_path, 'w', encoding='utf-8') as f:
                json.dump(emergency_analysis, f, indent=2, ensure_ascii=False)
            
            self.logger.critical(f"üö® Rapport d'urgence sauvegard√©: {emergency_report_path}")
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur analyse d'urgence: {e}")

def main():
    """Fonction principale asynchrone"""
    try:
        scheduler = AgentMetaStrategiqueScheduler()
        asyncio.run(scheduler.run_scheduler())
    except Exception as e:
        print(f"Une erreur fatale est survenue: {e}")

if __name__ == "__main__":
    main() 
