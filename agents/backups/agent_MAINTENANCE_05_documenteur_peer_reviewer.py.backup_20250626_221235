"""
üîç PEER-REVIEWER ENRICHI / DOCUMENTEUR - Agent 05
==============================================================

üéØ Mission : G√©n√©rer un rapport de mission d√©taill√© et analytique.
‚ö° Capacit√©s : Analyse fine des erreurs, g√©n√©ration de diff, synth√®se de mission.
üè¢ √âquipe : NextGeneration Tools Migration

Author: √âquipe de Maintenance NextGeneration
Version: 5.1.0 - Mission Display
"""

import asyncio
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import sys
import difflib
import logging
import ast
import subprocess
import dataclasses

# --- Configuration Robuste du Chemin d'Importation ---
try:
    project_root = Path(__file__).resolve().parents[1]
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
except (IndexError, NameError):
    if '.' not in sys.path:
        sys.path.insert(0, '.')

from core.agent_factory_architecture import Agent, Task, Result

# Ajout du DataClass pour les probl√®mes de qualit√©
@dataclasses.dataclass
class UniversalQualityIssue:
    severity: str
    description: str
    code: str
    details: Optional[Any] = None
    line: Optional[int] = None
    column: Optional[int] = None

    def to_dict(self) -> Dict[str, Any]:
        return dataclasses.asdict(self)

# Ajout de la fonction helper pour les docstrings de module (similaire √† agent_111)
def _has_module_docstring_manual(tree: ast.Module) -> bool:
    """V√©rifie manuellement la pr√©sence d'un docstring de module."""
    if not hasattr(tree, 'body') or not tree.body:
        return False
    first_node = tree.body[0]
    if sys.version_info < (3, 8) and isinstance(first_node, ast.Expr) and isinstance(first_node.value, ast.Str):
        return True
    if sys.version_info >= (3, 8) and isinstance(first_node, ast.Expr) and isinstance(first_node.value, ast.Constant) and isinstance(first_node.value.value, str):
        return True
    return False

class AgentMAINTENANCE05DocumenteurPeerReviewer(Agent):
    """G√©n√®re des rapports de mission de maintenance d√©taill√©s et analytiques."""
    
    def __init__(self, **kwargs):
        super().__init__(agent_type="documenteur", **kwargs)
        self.logger = logging.getLogger(self.__class__.__name__)
        self.agent_id = self.id
        self.logger.info(f"üîç Agent Documenteur ({self.agent_id}) initialis√©")

    async def startup(self):
        """D√©marre l'agent Documenteur."""
        self.logger.info("Agent Documenteur pr√™t.")

    async def shutdown(self):
        """Arr√™te l'agent Documenteur."""
        self.logger.info("Agent Documenteur √©teint.")

    async def health_check(self) -> Dict[str, Any]:
        """V√©rifie l'√©tat de sant√© de l'agent."""
        return {"status": "healthy", "version": getattr(self, 'version', 'N/A')}

    def get_capabilities(self) -> List[str]:
        return ["generate_mission_report", "audit_universal_quality"]

    async def execute_task(self, task: Task) -> Result:
        self.logger.info(f"üéØ Ex√©cution t√¢che: {task.type}")
        if task.type == "generate_mission_report":
            report_data = task.params.get("report_data")
            if not report_data:
                return Result(success=False, error="Donn√©es du rapport manquantes.")
            
            md_content = self._generer_rapport_md_enrichi(report_data)
            return Result(success=True, data={"md_content": md_content})
        elif task.type == "audit_universal_quality":
            file_path = task.params.get("file_path")
            if not file_path:
                return Result(success=False, error="file_path est requis pour audit_universal_quality.")
            try:
                report = await self.audit_universal_quality(file_path=file_path)
                return Result(success=True, data={"audit_report": report})
            except Exception as e:
                self.logger.error(f"Erreur lors de l'audit universel de {file_path}: {e}", exc_info=True)
                return Result(success=False, error=f"Erreur audit universel: {str(e)}")
        else:
            return Result(success=False, error=f"T√¢che non reconnue: {task.type}")

    def _generer_diff(self, original_code: str, final_code: str) -> str:
        if not original_code or not final_code or original_code == final_code:
            return "Aucune modification de code n'a √©t√© effectu√©e."
        diff = difflib.unified_diff(
            original_code.splitlines(keepends=True),
            final_code.splitlines(keepends=True),
            fromfile='original', tofile='corrected'
        )
        diff_str = "".join(diff)
        return f"```diff\n{diff_str}\n```" if diff_str else "Aucune modification de code d√©tect√©e."

    def _format_history(self, history: List[Dict]) -> List[str]:
        lines = ["- **Historique de R√©paration :**", "  <details><summary>Cliquer pour voir les √©tapes</summary>", "  "]
        for attempt in history:
            lines.append(f"  - **Tentative {attempt.get('iteration', '?')}**")
            lines.append(f"    - **Erreur D√©tect√©e :** `{attempt.get('error_detected', 'N/A')}`")
            adaptations = attempt.get('adaptation_attempted', ['N/A'])
            lines.append(f"    - **Adaptation Tent√©e :** `{adaptations[0]}`")
            lines.append(f"    - **R√©sultat du Test :** {attempt.get('test_result', 'N/A')}")
        lines.append("\n  </details>")
        return lines

    def _generer_rapport_md_enrichi(self, rapport_data: Dict[str, Any]) -> str:
        mission_id = rapport_data.get('mission_id', 'N/A')
        statut = rapport_data.get('statut_mission', 'INCONNU')
        duree = rapport_data.get('duree_totale_sec', 0)
        equipe = rapport_data.get('equipe_maintenance_roles', [])
        
        lines = [f"# Rapport de Mission de Maintenance : `{mission_id}`"]
        lines.append(f"**Statut Final :** {statut} | **Dur√©e :** {duree:.2f}s")
        
        if equipe:
            lines.append("\n## √âquipe de Maintenance Active")
            lines.append("La mission a √©t√© men√©e par les agents suivants :")
            for role in equipe:
                lines.append(f"- `{role}`")

        lines.append("\n---")
        
        lines.append("## R√©sultats D√©taill√©s par Agent\n")

        for agent_result in rapport_data.get("resultats_par_agent", []):
            agent_name = agent_result.get('agent_name', 'Agent Inconnu')
            agent_mission = agent_result.get('agent_mission', 'Non sp√©cifi√©e')
            status = agent_result.get('status', 'INCONNU')
            
            icon = "‚úÖ" if status in ["REPAIRED", "NO_REPAIR_NEEDED"] else "‚ùå"
            lines.append(f"### {icon} Agent : `{agent_name}`")
            lines.append(f"- **Mission de l'agent :** *{agent_mission}*")
            lines.append(f"- **Statut Final :** {status}")

            # Section √âvaluation Initiale
            initial_eval = agent_result.get("initial_evaluation", {})
            if initial_eval:
                score = initial_eval.get('score', 'N/A')
                reason = initial_eval.get('reason', 'N/A')
                lines.append(f"- **√âvaluation Initiale :** Score de {score}/100. (Raison: {reason})")
            
            # Section Historique de R√©paration
            history = agent_result.get("repair_history", [])
            if history:
                lines.extend(self._format_history(history))
            
            # Section Analyse de Performance
            perf_analysis = agent_result.get("performance_analysis", {})
            if perf_analysis and not perf_analysis.get('error'):
                score = perf_analysis.get('score', 'N/A')
                lines.append(f"- **Analyse de Performance :** Score de {score}/100.")
            
            # Section Diff
            if status == "REPAIRED":
                lines.append("- **Diff des Modifications :**")
                lines.append("  <details><summary>Cliquer pour voir les changements</summary>\n")
                diff_str = self._generer_diff(agent_result.get("original_code"), agent_result.get("final_code"))
                lines.append(diff_str)
                lines.append("\n  </details>")

            if status == "REPAIR_FAILED":
                 lines.append(f"- **Derni√®re Erreur :** `{agent_result.get('last_error', 'N/A')}`")

            lines.append("\n---\n")

        lines.append(self._generer_conclusion(rapport_data))

        return "\n".join(lines)

    def _generer_conclusion(self, rapport_data: Dict[str, Any]) -> str:
        """G√©n√®re une conclusion synth√©tique pour la mission."""
        results = rapport_data.get("resultats_par_agent", [])
        total_agents = len(results)
        repaired = sum(1 for r in results if r['status'] == 'REPAIRED')
        no_repair = sum(1 for r in results if r['status'] == 'NO_REPAIR_NEEDED')
        failed = sum(1 for r in results if r['status'] == 'REPAIR_FAILED')

        lines = ["## Conclusion de la Mission"]

        if not results:
            lines.append("Aucun agent n'a √©t√© trait√© durant cette mission.")
            return "\n".join(lines)

        success_rate = (repaired + no_repair) / total_agents * 100
        
        if success_rate == 100:
            conclusion = f"La mission est un succ√®s total. L'ensemble des {total_agents} agents trait√©s sont stables et op√©rationnels."
            if no_repair == total_agents:
                conclusion += " Aucun n'a n√©cessit√© de r√©paration."
            else:
                conclusion += f" {repaired} agents ont √©t√© r√©par√©s avec succ√®s."
        elif success_rate > 70:
            conclusion = f"La mission s'est globalement bien d√©roul√©e avec un taux de succ√®s de {success_rate:.0f}%. Sur {total_agents} agents, {repaired + no_repair} sont op√©rationnels."
        else:
            conclusion = f"La mission r√©v√®le des probl√®mes de stabilit√© significatifs avec un taux de succ√®s de seulement {success_rate:.0f}%."

        lines.append(conclusion)
        
        if failed > 0:
            lines.append(f"**Point d'attention :** {failed} agent(s) n'ont pas pu √™tre r√©par√©s et n√©cessitent une investigation manuelle.")

        return "\n".join(lines)

    # --- Nouvelles m√©thodes pour l'audit universel ---

    async def _run_flake8(self, file_path: str) -> List[UniversalQualityIssue]:
        """Ex√©cute Flake8 sur un fichier et retourne les probl√®mes."""
        issues = []
        try:
            clean_file_path = file_path.strip('\'')
            self.logger.info(f"Ex√©cution de Flake8 sur: {clean_file_path}")
            process = await asyncio.create_subprocess_shell(
                f'flake8 "{clean_file_path}"', 
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )
            stdout, stderr = await process.communicate()

            if stderr:
                self.logger.warning(f"Flake8 stderr pour {clean_file_path}: {stderr.decode(errors='ignore')}")

            if stdout:
                output = stdout.decode(errors='ignore').strip()
                if output:
                    self.logger.info(f"Flake8 output pour {clean_file_path}:\\n{output}")
                    for line in output.splitlines():
                        parts = line.split(':')
                        # Gestion des chemins Windows (ex: C:\path\file.py:line:col: msg)
                        # Un chemin Windows aura au moins 3 parties apr√®s split, et la premi√®re partie sera une seule lettre.
                        parsed_path = ""
                        line_num_str, col_num_str, code_msg_str = None, None, None
                        
                        if len(parts) >= 4 and len(parts[0]) == 1 and parts[1].startswith('\\'): # Probablement un chemin Windows
                            parsed_path = f"{parts[0]}:{parts[1]}"
                            try:
                                line_num_str = parts[2]
                                col_num_str = parts[3]
                                code_msg_str = ':' + ':'.join(parts[4:]).strip() # Le message peut contenir des ':'
                            except IndexError:
                                self.logger.warning(f"Ligne Flake8 (format Windows) incompl√®te: '{line}'")
                                continue
                        elif len(parts) >= 3: # Format Unix/Relatif attendu
                            parsed_path = parts[0]
                            try:
                                line_num_str = parts[1]
                                col_num_str = parts[2]
                                code_msg_str = ':' + ':'.join(parts[3:]).strip()
                            except IndexError:
                                self.logger.warning(f"Ligne Flake8 (format Unix) incompl√®te: '{line}'")
                                continue
                        else:
                            self.logger.warning(f"Ligne Flake8 au format inattendu (pas assez de parties): '{line}'")
                            continue

                        try:
                            line_num = int(line_num_str)
                            col_num = int(col_num_str)
                            code_val = code_msg_str.split(' ')[0]
                            description_val = code_msg_str.split(' ', 1)[1]
                            
                            issues.append(UniversalQualityIssue(
                                line=line_num,
                                column=col_num,
                                code=code_val,
                                description=description_val,
                                severity="MEDIUM" 
                            ))
                        except (ValueError, IndexError, AttributeError) as e:
                            self.logger.warning(f"Impossible de parser les composants de la ligne Flake8: '{line}\' (Path: {parsed_path}, Line: {line_num_str}, Col: {col_num_str}, Msg: {code_msg_str}). Erreur: {e}")
                else:
                    self.logger.info(f"Aucun probl√®me Flake8 trouv√© pour {clean_file_path}.")
            
            if process.returncode != 0 and not issues and output: # Si Flake8 sort avec un code d'erreur mais qu'on a quand m√™me pars√© des issues (cas warnings)
                 self.logger.info(f"Flake8 a retourn√© le code {process.returncode} pour {clean_file_path} mais des issues ont √©t√© pars√©es.")
            elif process.returncode != 0 and not output:
                 self.logger.error(f"Flake8 a retourn√© le code {process.returncode} pour {clean_file_path} sans output. Stderr: {stderr.decode(errors='ignore')}")

        except FileNotFoundError:
            self.logger.error(f"Flake8 non trouv√©. Veuillez l'installer (`pip install flake8`).")
            issues.append(UniversalQualityIssue(severity="CRITICAL", description="Flake8 non trouv√©. Installation requise.", code="FLAKE8_NOT_FOUND"))
        except Exception as e:
            self.logger.error(f"Erreur durant l'ex√©cution de Flake8 sur {clean_file_path}: {e}", exc_info=True)
            issues.append(UniversalQualityIssue(severity="CRITICAL", description=f"Erreur Flake8: {str(e)}", code="FLAKE8_ERROR"))
        return issues

    async def _perform_ast_audit(self, code: str, file_path: str) -> Dict[str, Any]:
        """Effectue un audit de qualit√© en utilisant l'AST (docstrings, complexit√©)."""
        self.logger.info(f"D√©but de l'audit AST pour {file_path}")
        report = {
            "quality_score": 100,
            "issues": [],
            "metrics": {
                "total_lines": len(code.splitlines()),
                "total_functions": 0,
                "total_classes": 0,
                "module_docstring": "NON_EVALUE",
                "functions_no_docstring": 0,
                "avg_complexity": 0 # Placeholder
            }
        }

        try:
            tree = ast.parse(code)
        except SyntaxError as e:
            self.logger.error(f"Erreur de syntaxe AST dans {file_path}: {e}")
            report["quality_score"] = 0
            report["issues"].append(UniversalQualityIssue(
                severity="CRITICAL", description=f"SyntaxError: {e.msg} √† la ligne {e.lineno}", 
                code="SYNTAX_ERROR", line=e.lineno, column=e.offset
            ).to_dict())
            return report

        # 1. Docstring de module
        has_module_d = _has_module_docstring_manual(tree)
        report["metrics"]["module_docstring"] = "‚úÖ Oui" if has_module_d else "‚ùå Non"
        if not has_module_d:
            report["quality_score"] -= 15
            report["issues"].append(UniversalQualityIssue(
                severity="HIGH", description="Docstring de module manquant.", code="MISSING_MODULE_DOCSTRING"
            ).to_dict())

        # 2. Docstrings fonctions/classes et complexit√© cyclomatique (basique)
        funcs_no_doc = []
        # complexite_totale = 0 # Placeholder pour future impl√©mentation
        
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                report["metrics"]["total_classes"] += 1
                if not ast.get_docstring(node):
                    report["quality_score"] -= 5 # Moins p√©nalisant que fonction
                    report["issues"].append(UniversalQualityIssue(
                        severity="MEDIUM", description=f"Docstring de classe manquant pour '{node.name}'.", 
                        code="MISSING_CLASS_DOCSTRING", line=node.lineno
                    ).to_dict())
            
            elif isinstance(node, ast.FunctionDef) or isinstance(node, ast.AsyncFunctionDef):
                report["metrics"]["total_functions"] += 1
                if not ast.get_docstring(node):
                    funcs_no_doc.append({"function": node.name, "line": node.lineno})
                
                # Complexit√© cyclomatique (tr√®s basique : compte les branches if/elif/else/for/while/try/except/with)
                # Une vraie mesure utiliserait un outil ou une logique plus pouss√©e.
                # complexity = 1
                # for sub_node in ast.walk(node):
                #     if isinstance(sub_node, (ast.If, ast.For, ast.AsyncFor, ast.While, ast.Try, ast.With, ast.AsyncWith)):
                #         complexity += 1
                #     if isinstance(sub_node, ast.orelse) and sub_node.orelse: # Pour les else/except etc.
                #          complexity +=1 
                # complexite_totale += complexity

        if funcs_no_doc:
            report["quality_score"] -= len(funcs_no_doc) * 10
            report["issues"].append(UniversalQualityIssue(
                severity="MEDIUM", description=f"{len(funcs_no_doc)} fonction(s) sans docstring.", 
                code="MISSING_FUNCTION_DOCSTRING", details=funcs_no_doc
            ).to_dict())
            report["metrics"]["functions_no_docstring"] = len(funcs_no_doc)
        
        # if report["metrics"]["total_functions"] > 0: # Placeholder
        #    report["metrics"]["avg_complexity"] = complexite_totale / report["metrics"]["total_functions"]

        report["quality_score"] = max(0, report["quality_score"])
        return report

    async def audit_universal_quality(self, file_path: str) -> Dict[str, Any]:
        """Orchestre les diff√©rents audits (Flake8, AST) et consolide le rapport."""
        self.logger.info(f"Lancement de l'audit universel pour: {file_path}")
        
        final_issues: List[Dict[str, Any]] = []
        # score_global = 100 # Sera recalcul√© √† partir des sous-scores

        # Lire le contenu du fichier une seule fois
        try:
            code_content = Path(file_path).read_text(encoding='utf-8')
        except FileNotFoundError:
            self.logger.error(f"Fichier non trouv√© pour l'audit universel: {file_path}")
            return {"error": "File not found", "quality_score": 0, "issues": [UniversalQualityIssue("CRITICAL", "Fichier non trouv√©", "FILE_NOT_FOUND").to_dict()]}
        except Exception as e:
            self.logger.error(f"Erreur de lecture du fichier {file_path}: {e}", exc_info=True)
            return {"error": f"File read error: {e}", "quality_score": 0, "issues": [UniversalQualityIssue("CRITICAL", f"Erreur lecture fichier: {e}", "FILE_READ_ERROR").to_dict()]}

        # 1. Audit Flake8
        flake8_issues_obj = await self._run_flake8(file_path)
        final_issues.extend([issue.to_dict() for issue in flake8_issues_obj])
        # score_flake8 = 100 - (len(flake8_issues_obj) * 5) # P√©nalit√© arbitraire

        # 2. Audit AST (docstrings, etc.)
        ast_report = await self._perform_ast_audit(code_content, file_path)
        final_issues.extend(ast_report.get("issues", []))
        score_ast = ast_report.get("quality_score", 0)
        
        # Calcul du score global (exemple simple, peut √™tre affin√©)
        # Ici, on prend le score AST comme base et on d√©duit pour Flake8 s'il y a des erreurs critiques non d√©j√† couvertes
        # Pour l'instant, on va juste utiliser le score AST et ajouter les probl√®mes Flake8
        score_global = score_ast 
        # On pourrait ajouter une logique pour r√©duire davantage le score_global en fonction des issues Flake8.
        # Par exemple, si Flake8 rapporte des erreurs (E) ou des erreurs critiques (F).

        # Consolidation du rapport
        consolidated_report = {
            "file_path": file_path,
            "quality_score": max(0, score_global),
            "metrics_ast": ast_report.get("metrics", {}),
            "issues": final_issues,
            "summary": {
                "total_issues": len(final_issues),
                "flake8_issues": len(flake8_issues_obj),
                "ast_issues": len(ast_report.get("issues", [])),
            }
        }
        self.logger.info(f"Audit universel termin√© pour {file_path}. Score: {consolidated_report['quality_score']}")
        return consolidated_report

    # --- Fin des nouvelles m√©thodes ---

def create_agent_MAINTENANCE_05_documenteur_peer_reviewer(**config) -> AgentMAINTENANCE05DocumenteurPeerReviewer:
    """Factory pour cr√©er une instance de l'Agent 5."""
    return AgentMAINTENANCE05DocumenteurPeerReviewer(**config)

# Ajout de la section main pour les tests
if __name__ == "__main__":
    async def run_tests():
        print("üöÄ D√©marrage des tests pour AgentMAINTENANCE05DocumenteurPeerReviewer...")
        # Configuration de base du logger pour voir les logs pendant les tests
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        
        agent_config = {"agent_id": "test_doc_peer_reviewer_001"}
        agent = create_agent_MAINTENANCE_05_documenteur_peer_reviewer(**agent_config)

        try:
            await agent.startup()
            health = await agent.health_check()
            agent.logger.info(f"MAIN_TEST - Health Check: {health}")
            print(f"üè• Health Check: {health}")

            capabilities = agent.get_capabilities()
            agent.logger.info(f"MAIN_TEST - Capabilities: {capabilities}")
            print(f"üõ†Ô∏è Capabilities: {capabilities}")
            assert "audit_universal_quality" in capabilities, "Capacit√© audit_universal_quality manquante !"

            # Cr√©er un fichier Python temporaire pour le test d'audit
            test_py_content = '''
# Test file for universal audit
def hello_world(): # Missing docstring
    print("Hello, world!")

class MyClass: # Missing docstring
    def __init__(self): # Missing docstring
        self.value = 10
        if self.value > 5: # Complexity point
            print("Big")

def another_func(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p): # Too many arguments (Flake8)
    pass # Missing docstring
'''
            test_file_path = Path(__file__).parent / "temp_test_audit_file.py"
            with open(test_file_path, "w", encoding="utf-8") as tf:
                tf.write(test_py_content)
            
            agent.logger.info(f"MAIN_TEST - Fichier de test cr√©√© : {test_file_path}")

            print("\nüî¨ Test de la t√¢che 'audit_universal_quality'...")
            # Utiliser str(test_file_path) pour s'assurer que c'est une cha√Æne
            audit_task_params = {"file_path": str(test_file_path)}
            audit_task = Task(id="audit_test_01", type="audit_universal_quality", params=audit_task_params)
            
            agent.logger.info(f"MAIN_TEST - Lancement de la t√¢che d'audit pour: {test_file_path}")
            audit_result = await agent.execute_task(audit_task)

            agent.logger.info(f"MAIN_TEST - Audit Result Success: {audit_result.success}")
            agent.logger.info(f"MAIN_TEST - Audit Result Data: {audit_result.data}")
            if audit_result.error:
                agent.logger.error(f"MAIN_TEST - Audit Result Error: {audit_result.error}")

            print(f"   R√©sultat de l'audit: {'Succ√®s' if audit_result.success else '√âchec'}")
            if audit_result.success and audit_result.data:
                report_data = audit_result.data.get('audit_report', {})
                print(f"   Score de qualit√© pour '{report_data.get('file_path')}': {report_data.get('quality_score', 'N/A')}/100")
                if report_data.get('issues'):
                    print("   Probl√®mes trouv√©s:")
                    for issue_dict in report_data.get('issues', []):
                        line_info = f"L{issue_dict.get('line')}" if issue_dict.get('line') else "N/A"
                        col_info = f":C{issue_dict.get('column')}" if issue_dict.get('column') else ""
                        print(f"     - {line_info}{col_info} [{issue_dict.get('code', 'N/A')}] {issue_dict.get('description', 'N/A')} ({issue_dict.get('severity', 'N/A')})")
                else:
                    print("   Aucun probl√®me substantiel trouv√© par l'audit.")
            elif not audit_result.success:
                print(f"   Erreur audit: {audit_result.error}")

            # Nettoyage du fichier de test
            if test_file_path.exists():
                test_file_path.unlink()
                agent.logger.info(f"MAIN_TEST - Fichier de test supprim√© : {test_file_path}")

        except Exception as e:
            print(f"‚ùå Erreur durant l'ex√©cution des tests de l'agent: {e}")
            agent.logger.error(f"MAIN_TEST - Erreur Exception dans run_tests(): {e}", exc_info=True)
        finally:
            await agent.shutdown()
            print("\n‚úÖ Tests termin√©s.")

    asyncio.run(run_tests())