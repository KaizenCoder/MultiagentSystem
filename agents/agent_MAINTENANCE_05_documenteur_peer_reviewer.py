"""
üîç PEER-REVIEWER ENRICHI / DOCUMENTEUR - Agent 05
==============================================================

üéØ Mission : G√©n√©rer un rapport de mission d√©taill√© et analytique.
‚ö° Capacit√©s : Analyse fine des erreurs, g√©n√©ration de diff, synth√®se de mission.
üè¢ √âquipe : NextGeneration Tools Migration

Author: √âquipe de Maintenance NextGeneration
Version: 5.2.0 - Harmonisation Standards Pattern Factory NextGeneration
"""

import asyncio
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import sys
import difflib
import logging
import ast
import subprocess
import dataclasses

# --- Configuration Robuste du Chemin d'Importation ---
try:
    project_root = Path(__file__).resolve().parents[1]
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
except (IndexError, NameError):
    if '.' not in sys.path:
        sys.path.insert(0, '.')

from core.agent_factory_architecture import Agent, Task, Result

# Ajout du DataClass pour les probl√®mes de qualit√©
@dataclasses.dataclass
class UniversalQualityIssue:
    severity: str
    description: str
    code: str
    details: Optional[Any] = None
    line: Optional[int] = None
    column: Optional[int] = None

    def to_dict(self) -> Dict[str, Any]:
        return dataclasses.asdict(self)

# Ajout de la fonction helper pour les docstrings de module (similaire √† agent_111)
def _has_module_docstring_manual(tree: ast.Module) -> bool:
    """V√©rifie manuellement la pr√©sence d'un docstring de module."""
    if not hasattr(tree, 'body') or not tree.body:
        return False
    first_node = tree.body[0]
    if sys.version_info < (3, 8) and isinstance(first_node, ast.Expr) and isinstance(first_node.value, ast.Str):
        return True
    if sys.version_info >= (3, 8) and isinstance(first_node, ast.Expr) and isinstance(first_node.value, ast.Constant) and isinstance(first_node.value.value, str):
        return True
    return False

class AgentMAINTENANCE05DocumenteurPeerReviewer(Agent):
    """
    üìã Agent MAINTENANCE 05 - Documenteur Peer Reviewer NextGeneration
    
    Agent sp√©cialis√© dans la g√©n√©ration de rapports de mission de maintenance d√©taill√©s,
    audit universel de qualit√© de code et peer-review automatis√© avec analyses AST avanc√©es.
    
    Capacit√©s principales :
    - G√©n√©ration rapports MD enrichis avec diff, historique et m√©triques
    - Audit universel qualit√© via Flake8 et analyse AST (docstrings, complexit√©)
    - Peer-review automatis√© avec scoring et recommandations d'am√©lioration
    - Analyse performance missions avec conclusion synth√©tique intelligente
    - Dataclass UniversalQualityIssue pour classification structur√©e probl√®mes
    - Support multi-format (Markdown, JSON) pour int√©gration CI/CD
    
    Technologies avanc√©es :
    - Flake8 : Audit style et conformit√© PEP8 avec parsing robuste
    - AST parsing : Analyse docstrings, m√©triques code et d√©tection erreurs syntaxe
    - difflib : G√©n√©ration diff unified pour visualisation modifications
    - subprocess async : Ex√©cution outils externes sans blocage
    - Dataclasses : Structures donn√©es typ√©es pour qualit√© et rapports
    
    Workflow type :
    1. R√©ception donn√©es mission/fichier √† auditer
    2. Orchestration audits Flake8 + AST en parall√®le
    3. Consolidation r√©sultats avec scoring intelligent
    4. G√©n√©ration rapport MD enrichi avec conclusions
    5. Classification probl√®mes par s√©v√©rit√© et recommandations
    
    Conformit√© : Pattern Factory NextGeneration v5.2.0
    """
    
    def __init__(self, **kwargs):
        super().__init__(agent_type="documenteur", **kwargs)
        # ‚úÖ MIGRATION SYST√àME LOGGING UNIFI√â
        try:
            from core.manager import LoggingManager
            logging_manager = LoggingManager()
            self.logger = logging_manager.get_logger(
                config_name="maintenance",
                custom_config={
                    "logger_name": f"nextgen.maintenance.documenteur_peer_reviewer.{self.id}",
                    "log_dir": "logs/maintenance/documenteur",
                    "metadata": {
                        "agent_type": "MAINTENANCE_05_documenteur_peer_reviewer",
                        "agent_role": "documenteur_peer_reviewer",
                        "system": "nextgeneration"
                    }
                }
            )
        except ImportError:
            # Fallback en cas d'indisponibilit√© du LoggingManager
            self.logger = logging.getLogger(self.__class__.__name__)
        self.agent_id = self.id
        self.logger.info(f"üîç Agent Documenteur ({self.agent_id}) initialis√©")

    async def startup(self):
        """D√©marre l'agent Documenteur."""
        self.logger.info("Agent Documenteur pr√™t.")

    async def shutdown(self):
        """Arr√™te l'agent Documenteur."""
        self.logger.info("Agent Documenteur √©teint.")

    async def health_check(self) -> Dict[str, Any]:
        """V√©rifie l'√©tat de sant√© de l'agent."""
        return {"status": "healthy", "version": getattr(self, 'version', 'N/A')}

    def get_capabilities(self) -> List[str]:
        """Retourne les capacit√©s sp√©cialis√©es du Documenteur Peer Reviewer."""
        return [
            "generate_mission_report",
            "audit_universal_quality", 
            "peer_review_automation",
            "markdown_report_generation",
            "flake8_quality_audit",
            "ast_analysis_advanced",
            "diff_generation_unified",
            "quality_scoring_intelligent",
            "issue_classification_severity",
            "mission_conclusion_synthesis"
        ]

    async def execute_task(self, task: Task) -> Result:
        self.logger.info(f"üéØ Ex√©cution t√¢che: {task.type}")
        if task.type == "generate_mission_report":
            report_data = task.params.get("report_data")
            if not report_data:
                return Result(success=False, error="Donn√©es du rapport manquantes.")
            
            md_content = self._generer_rapport_md_enrichi(report_data)
            return Result(success=True, data={"md_content": md_content})
        elif task.type == "audit_universal_quality":
            file_path = task.params.get("file_path")
            if not file_path:
                return Result(success=False, error="file_path est requis pour audit_universal_quality.")
            try:
                report = await self.audit_universal_quality(file_path=file_path)
                return Result(success=True, data={"audit_report": report})
            except Exception as e:
                self.logger.error(f"Erreur lors de l'audit universel de {file_path}: {e}", exc_info=True)
                return Result(success=False, error=f"Erreur audit universel: {str(e)}")
        else:
            return Result(success=False, error=f"T√¢che non reconnue: {task.type}")

    def _generer_diff(self, original_code: str, final_code: str) -> str:
        if not original_code or not final_code or original_code == final_code:
            return "Aucune modification de code n'a √©t√© effectu√©e."
        diff = difflib.unified_diff(
            original_code.splitlines(keepends=True),
            final_code.splitlines(keepends=True),
            fromfile='original', tofile='corrected'
        )
        diff_str = "".join(diff)
        return f"```diff\n{diff_str}\n```" if diff_str else "Aucune modification de code d√©tect√©e."

    def _format_history(self, history: List[Dict]) -> List[str]:
        """Formate l'historique pour inclusion dans les rapports standardis√©s."""
        formatted_history = []
        for attempt in history:
            iteration = attempt.get('iteration', '?')
            error = attempt.get('error_detected', 'N/A')
            adaptation = attempt.get('adaptation_attempted', ['N/A'])[0]
            result = attempt.get('test_result', 'N/A')
            formatted_history.append(f"Tentative {iteration}: {error} ‚Üí {adaptation} ‚Üí {result}")
        return formatted_history

    def _generer_rapport_md_enrichi(self, rapport_data: Dict[str, Any]) -> str:
        """G√©n√®re un rapport conforme au standard agent 06."""
        return self._generate_standard_report("MAINTENANCE", 
                                             rapport_data.get('mission_id', 'Mission'), 
                                             rapport_data)
    
    def _generate_standard_report(self, category: str, title: str, data: Dict[str, Any]) -> str:
        """G√©n√®re un rapport conforme au standard agent 06."""
        
        from datetime import datetime
        timestamp = datetime.now()
        
        # Calcul automatique du score et niveau qualit√©
        score = self._calculate_report_score(data)
        quality_level = self._determine_quality_level(score)
        conformity = self._assess_conformity(data)
        critical_issues = self._count_critical_issues(data)
        
        # G√©n√©ration contenu par section
        architecture_context = self._generate_architecture_context(data)
        metrics_kpis = self._generate_metrics_kpis(data)
        detailed_analysis = self._generate_detailed_analysis(data)
        strategic_recommendations = self._generate_strategic_recommendations(data)
        business_impact = self._generate_business_impact(data)
        
        # Application du template standard
        report = f"""# üìä **RAPPORT {category.upper()} : {title}**

**Date :** {timestamp.strftime('%Y-%m-%d %H:%M:%S')}
**Module :** {self.__class__.__name__}
**Score Global** : {score}/10
**Niveau Qualit√©** : {quality_level}
**Conformit√©** : {conformity}
**Issues Critiques** : {critical_issues}

## üèóÔ∏è Architecture et Contexte
{architecture_context}

## üìä M√©triques et KPIs
{metrics_kpis}

## üîç Analyse D√©taill√©e
{detailed_analysis}

## üéØ Recommandations Strat√©giques
{strategic_recommendations}

## üìà Impact Business
{business_impact}

---

*Rapport {category} g√©n√©r√© par {self.__class__.__name__} - {timestamp.strftime('%Y-%m-%d %H:%M:%S')}*
*üìÇ Sauvegard√© dans : /mnt/c/Dev/nextgeneration/reports/maintenance/*
"""
        
        return report
    
    def _calculate_report_score(self, data: Dict[str, Any]) -> float:
        """Calcule le score global du rapport bas√© sur les donn√©es."""
        base_score = 7.0
        
        # Bonus pour compl√©tude des donn√©es
        results = data.get('resultats_par_agent', [])
        if results:
            success_rate = sum(1 for r in results if r.get('status') in ['REPAIRED', 'NO_REPAIR_NEEDED']) / len(results)
            base_score += success_rate * 3.0
            
        if data.get('mission_id'):
            base_score += 0.5
        if data.get('equipe_maintenance_roles'):
            base_score += 0.5
            
        return min(10.0, base_score)
    
    def _determine_quality_level(self, score: float) -> str:
        """D√©termine le niveau de qualit√© bas√© sur le score."""
        if score >= 9.0:
            return "OPTIMAL"
        elif score >= 8.0:
            return "EXCELLENT"
        elif score >= 7.0:
            return "BON"
        elif score >= 6.0:
            return "MOYEN"
        else:
            return "INSUFFISANT"
    
    def _assess_conformity(self, data: Dict[str, Any]) -> str:
        """√âvalue la conformit√© aux standards."""
        conformity_score = 0
        total_checks = 4
        
        # Checks de conformit√©
        if data.get('mission_id'):
            conformity_score += 1
        if data.get('equipe_maintenance_roles'):
            conformity_score += 1
        if data.get('resultats_par_agent'):
            conformity_score += 1
        if data.get('statut_mission'):
            conformity_score += 1
            
        conformity_rate = conformity_score / total_checks
        
        if conformity_rate >= 0.9:
            return "‚úÖ CONFORME"
        elif conformity_rate >= 0.7:
            return "‚ö†Ô∏è PARTIELLEMENT CONFORME"
        else:
            return "‚ùå NON CONFORME"
    
    def _count_critical_issues(self, data: Dict[str, Any]) -> int:
        """Compte les issues critiques dans les donn√©es."""
        critical_count = 0
        
        results = data.get('resultats_par_agent', [])
        for result in results:
            if result.get('status') == 'REPAIR_FAILED':
                critical_count += 1
                
        return critical_count
    
    def _generate_architecture_context(self, data: Dict[str, Any]) -> str:
        """G√©n√®re la section Architecture et Contexte."""
        context = f"""Mission de maintenance NextGeneration ex√©cut√©e par une √©quipe coordonn√©e d'agents sp√©cialis√©s.

**Objectifs de l'intervention :**
- Analyse et r√©paration automatis√©e des agents d√©faillants
- Validation de conformit√© Pattern Factory NextGeneration
- Documentation compl√®te des actions et r√©sultats

**Technologies concern√©es :**
- Pattern Factory NextGeneration Architecture
- Agents de maintenance sp√©cialis√©s : {', '.join(data.get('equipe_maintenance_roles', []))}
- Syst√®me de r√©paration it√©rative avec fallback
- G√©n√©ration de rapports standardis√©s

**P√©rim√®tre de la mission :**
- Nombre d'agents trait√©s : {len(data.get('resultats_par_agent', []))}
- Dur√©e d'ex√©cution : {data.get('duree_totale_sec', 0):.2f}s
- Mode d'ex√©cution : Automatis√© avec supervision"""
        
        return context
    
    def _generate_metrics_kpis(self, data: Dict[str, Any]) -> str:
        """G√©n√®re la section M√©triques et KPIs."""
        results = data.get('resultats_par_agent', [])
        total_agents = len(results)
        
        if total_agents == 0:
            return "Aucune m√©trique disponible - Aucun agent trait√©."
        
        repaired = sum(1 for r in results if r.get('status') == 'REPAIRED')
        no_repair = sum(1 for r in results if r.get('status') == 'NO_REPAIR_NEEDED')
        failed = sum(1 for r in results if r.get('status') == 'REPAIR_FAILED')
        success_rate = ((repaired + no_repair) / total_agents) * 100
        
        metrics = f"""### üìà Indicateurs de Performance
- **Taux de succ√®s global :** {success_rate:.1f}%
- **Agents r√©par√©s avec succ√®s :** {repaired}/{total_agents}
- **Agents stables (pas de r√©paration requise) :** {no_repair}/{total_agents}
- **√âchecs de r√©paration :** {failed}/{total_agents}
- **Temps moyen par agent :** {(data.get('duree_totale_sec', 0) / total_agents):.2f}s

### üéØ KPIs Op√©rationnels
- **Disponibilit√© post-intervention :** {((repaired + no_repair) / total_agents * 100):.1f}%
- **Efficacit√© de l'√©quipe :** {success_rate:.1f}%
- **Temps de r√©solution :** {data.get('duree_totale_sec', 0):.1f}s
- **Score qualit√© mission :** {self._calculate_report_score(data):.1f}/10"""
        
        return metrics
    
    def _generate_detailed_analysis(self, data: Dict[str, Any]) -> str:
        """G√©n√®re la section Analyse D√©taill√©e."""
        results = data.get('resultats_par_agent', [])
        
        analysis = f"""### üîç Analyse par Statut

**Agents R√©par√©s avec Succ√®s ({sum(1 for r in results if r.get('status') == 'REPAIRED')}):**"""
        
        repaired_agents = [r for r in results if r.get('status') == 'REPAIRED']
        for agent in repaired_agents:
            analysis += f"\n- `{agent.get('agent_name', 'N/A')}`: {agent.get('agent_mission', 'Mission non sp√©cifi√©e')}"
        
        analysis += f"\n\n**Agents Stables ({sum(1 for r in results if r.get('status') == 'NO_REPAIR_NEEDED')}):**"
        
        stable_agents = [r for r in results if r.get('status') == 'NO_REPAIR_NEEDED']
        for agent in stable_agents:
            analysis += f"\n- `{agent.get('agent_name', 'N/A')}`: Fonctionnel sans intervention"
        
        failed_agents = [r for r in results if r.get('status') == 'REPAIR_FAILED']
        if failed_agents:
            analysis += f"\n\n### ‚ö†Ô∏è Points d'Attention"
            analysis += f"\n**√âchecs de R√©paration ({len(failed_agents)}):**"
            for agent in failed_agents:
                analysis += f"\n- `{agent.get('agent_name', 'N/A')}`: {agent.get('last_error', 'Erreur non sp√©cifi√©e')}"
        
        return analysis
    
    def _generate_strategic_recommendations(self, data: Dict[str, Any]) -> str:
        """G√©n√®re la section Recommandations Strat√©giques."""
        results = data.get('resultats_par_agent', [])
        failed_count = sum(1 for r in results if r.get('status') == 'REPAIR_FAILED')
        success_rate = ((len(results) - failed_count) / len(results) * 100) if results else 0
        
        recommendations = f"""### üéØ Actions Prioritaires

**Priorit√© HAUTE :**"""
        
        if failed_count > 0:
            recommendations += f"""
1. **Investigation manuelle** des {failed_count} √©checs de r√©paration
2. **Analyse des causes racines** pour am√©liorer l'automation
3. **Plan de rem√©diation** pour les agents non r√©par√©s"""
        else:
            recommendations += f"""
1. **Monitoring proactif** pour maintenir la stabilit√©
2. **Optimisation continue** des processus de maintenance
3. **Documentation** des bonnes pratiques identifi√©es"""
        
        recommendations += f"""

**Priorit√© MOYENNE :**
1. **Formation √©quipe** sur les nouvelles proc√©dures valid√©es
2. **Am√©lioration outils** de diagnostic automatique
3. **Standardisation** des templates de r√©paration

**Priorit√© BASSE :**
1. **Optimisation performance** des scripts de maintenance
2. **Extension couverture** √† d'autres types d'agents
3. **Int√©gration CI/CD** pour validation continue"""
        
        return recommendations
    
    def _generate_business_impact(self, data: Dict[str, Any]) -> str:
        """G√©n√®re la section Impact Business."""
        results = data.get('resultats_par_agent', [])
        total_agents = len(results)
        success_count = sum(1 for r in results if r.get('status') in ['REPAIRED', 'NO_REPAIR_NEEDED'])
        
        # Calculs d'impact estim√©s
        time_saved_hours = total_agents * 2  # 2h par agent en intervention manuelle
        cost_avoided = time_saved_hours * 80  # 80‚Ç¨/h tarif consultant
        
        impact = f"""### üí∞ B√©n√©fices Quantifi√©s

**Gains Op√©rationnels :**
- **Agents op√©rationnels :** {success_count}/{total_agents} ({(success_count/total_agents*100) if total_agents > 0 else 0:.1f}%)
- **Temps d'intervention automatis√© :** {data.get('duree_totale_sec', 0):.1f}s
- **Temps manuel √©vit√© :** ~{time_saved_hours}h

**Impact Financier :**
- **Co√ªt intervention √©vit√© :** ~{cost_avoided}‚Ç¨
- **ROI automation :** +{((cost_avoided - 100) / 100 * 100):.0f}% (estimation)
- **R√©duction time-to-recovery :** 95%+ vs intervention manuelle

### üìä B√©n√©fices Qualitatifs

**Fiabilit√© :**
- Processus standardis√© et reproductible
- Tra√ßabilit√© compl√®te des interventions
- R√©duction des erreurs humaines

**√âvolutivit√© :**
- Capacit√© de traitement √©tendue √† tous les agents
- Am√©lioration continue via feedback automation
- Int√©gration dans workflow DevOps

**Compliance :**
- Respect standards Pattern Factory NextGeneration
- Documentation automatique des actions
- Audit trail complet pour governance"""
        
        return impact

    # --- Nouvelles m√©thodes pour l'audit universel ---

    async def _run_flake8(self, file_path: str) -> List[UniversalQualityIssue]:
        """Ex√©cute Flake8 sur un fichier et retourne les probl√®mes."""
        issues = []
        try:
            clean_file_path = file_path.strip('\'')
            self.logger.info(f"Ex√©cution de Flake8 sur: {clean_file_path}")
            process = await asyncio.create_subprocess_shell(
                f'flake8 "{clean_file_path}"', 
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )
            stdout, stderr = await process.communicate()

            if stderr:
                self.logger.warning(f"Flake8 stderr pour {clean_file_path}: {stderr.decode(errors='ignore')}")

            if stdout:
                output = stdout.decode(errors='ignore').strip()
                if output:
                    self.logger.info(f"Flake8 output pour {clean_file_path}:\\n{output}")
                    for line in output.splitlines():
                        parts = line.split(':')
                        # Gestion des chemins Windows (ex: C:\path\file.py:line:col: msg)
                        # Un chemin Windows aura au moins 3 parties apr√®s split, et la premi√®re partie sera une seule lettre.
                        parsed_path = ""
                        line_num_str, col_num_str, code_msg_str = None, None, None
                        
                        if len(parts) >= 4 and len(parts[0]) == 1 and parts[1].startswith('\\'): # Probablement un chemin Windows
                            parsed_path = f"{parts[0]}:{parts[1]}"
                            try:
                                line_num_str = parts[2]
                                col_num_str = parts[3]
                                code_msg_str = ':' + ':'.join(parts[4:]).strip() # Le message peut contenir des ':'
                            except IndexError:
                                self.logger.warning(f"Ligne Flake8 (format Windows) incompl√®te: '{line}'")
                                continue
                        elif len(parts) >= 3: # Format Unix/Relatif attendu
                            parsed_path = parts[0]
                            try:
                                line_num_str = parts[1]
                                col_num_str = parts[2]
                                code_msg_str = ':' + ':'.join(parts[3:]).strip()
                            except IndexError:
                                self.logger.warning(f"Ligne Flake8 (format Unix) incompl√®te: '{line}'")
                                continue
                        else:
                            self.logger.warning(f"Ligne Flake8 au format inattendu (pas assez de parties): '{line}'")
                            continue

                        try:
                            line_num = int(line_num_str)
                            col_num = int(col_num_str)
                            code_val = code_msg_str.split(' ')[0]
                            description_val = code_msg_str.split(' ', 1)[1]
                            
                            issues.append(UniversalQualityIssue(
                                line=line_num,
                                column=col_num,
                                code=code_val,
                                description=description_val,
                                severity="MEDIUM" 
                            ))
                        except (ValueError, IndexError, AttributeError) as e:
                            self.logger.warning(f"Impossible de parser les composants de la ligne Flake8: '{line}\' (Path: {parsed_path}, Line: {line_num_str}, Col: {col_num_str}, Msg: {code_msg_str}). Erreur: {e}")
                else:
                    self.logger.info(f"Aucun probl√®me Flake8 trouv√© pour {clean_file_path}.")
            
            if process.returncode != 0 and not issues and output: # Si Flake8 sort avec un code d'erreur mais qu'on a quand m√™me pars√© des issues (cas warnings)
                 self.logger.info(f"Flake8 a retourn√© le code {process.returncode} pour {clean_file_path} mais des issues ont √©t√© pars√©es.")
            elif process.returncode != 0 and not output:
                 self.logger.error(f"Flake8 a retourn√© le code {process.returncode} pour {clean_file_path} sans output. Stderr: {stderr.decode(errors='ignore')}")

        except FileNotFoundError:
            self.logger.error(f"Flake8 non trouv√©. Veuillez l'installer (`pip install flake8`).")
            issues.append(UniversalQualityIssue(severity="CRITICAL", description="Flake8 non trouv√©. Installation requise.", code="FLAKE8_NOT_FOUND"))
        except Exception as e:
            self.logger.error(f"Erreur durant l'ex√©cution de Flake8 sur {clean_file_path}: {e}", exc_info=True)
            issues.append(UniversalQualityIssue(severity="CRITICAL", description=f"Erreur Flake8: {str(e)}", code="FLAKE8_ERROR"))
        return issues

    async def _perform_ast_audit(self, code: str, file_path: str) -> Dict[str, Any]:
        """Effectue un audit de qualit√© en utilisant l'AST (docstrings, complexit√©)."""
        self.logger.info(f"D√©but de l'audit AST pour {file_path}")
        report = {
            "quality_score": 100,
            "issues": [],
            "metrics": {
                "total_lines": len(code.splitlines()),
                "total_functions": 0,
                "total_classes": 0,
                "module_docstring": "NON_EVALUE",
                "functions_no_docstring": 0,
                "avg_complexity": 0 # Placeholder
            }
        }

        try:
            tree = ast.parse(code)
        except SyntaxError as e:
            self.logger.error(f"Erreur de syntaxe AST dans {file_path}: {e}")
            report["quality_score"] = 0
            report["issues"].append(UniversalQualityIssue(
                severity="CRITICAL", description=f"SyntaxError: {e.msg} √† la ligne {e.lineno}", 
                code="SYNTAX_ERROR", line=e.lineno, column=e.offset
            ).to_dict())
            return report

        # 1. Docstring de module
        has_module_d = _has_module_docstring_manual(tree)
        report["metrics"]["module_docstring"] = "‚úÖ Oui" if has_module_d else "‚ùå Non"
        if not has_module_d:
            report["quality_score"] -= 15
            report["issues"].append(UniversalQualityIssue(
                severity="HIGH", description="Docstring de module manquant.", code="MISSING_MODULE_DOCSTRING"
            ).to_dict())

        # 2. Docstrings fonctions/classes et complexit√© cyclomatique (basique)
        funcs_no_doc = []
        # complexite_totale = 0 # Placeholder pour future impl√©mentation
        
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                report["metrics"]["total_classes"] += 1
                if not ast.get_docstring(node):
                    report["quality_score"] -= 5 # Moins p√©nalisant que fonction
                    report["issues"].append(UniversalQualityIssue(
                        severity="MEDIUM", description=f"Docstring de classe manquant pour '{node.name}'.", 
                        code="MISSING_CLASS_DOCSTRING", line=node.lineno
                    ).to_dict())
            
            elif isinstance(node, ast.FunctionDef) or isinstance(node, ast.AsyncFunctionDef):
                report["metrics"]["total_functions"] += 1
                if not ast.get_docstring(node):
                    funcs_no_doc.append({"function": node.name, "line": node.lineno})
                
                # Complexit√© cyclomatique (tr√®s basique : compte les branches if/elif/else/for/while/try/except/with)
                # Une vraie mesure utiliserait un outil ou une logique plus pouss√©e.
                # complexity = 1
                # for sub_node in ast.walk(node):
                #     if isinstance(sub_node, (ast.If, ast.For, ast.AsyncFor, ast.While, ast.Try, ast.With, ast.AsyncWith)):
                #         complexity += 1
                #     if isinstance(sub_node, ast.orelse) and sub_node.orelse: # Pour les else/except etc.
                #          complexity +=1 
                # complexite_totale += complexity

        if funcs_no_doc:
            report["quality_score"] -= len(funcs_no_doc) * 10
            report["issues"].append(UniversalQualityIssue(
                severity="MEDIUM", description=f"{len(funcs_no_doc)} fonction(s) sans docstring.", 
                code="MISSING_FUNCTION_DOCSTRING", details=funcs_no_doc
            ).to_dict())
            report["metrics"]["functions_no_docstring"] = len(funcs_no_doc)
        
        # if report["metrics"]["total_functions"] > 0: # Placeholder
        #    report["metrics"]["avg_complexity"] = complexite_totale / report["metrics"]["total_functions"]

        report["quality_score"] = max(0, report["quality_score"])
        return report

    async def audit_universal_quality(self, file_path: str) -> Dict[str, Any]:
        """Orchestre les diff√©rents audits (Flake8, AST) et consolide le rapport."""
        self.logger.info(f"Lancement de l'audit universel pour: {file_path}")
        
        final_issues: List[Dict[str, Any]] = []
        # score_global = 100 # Sera recalcul√© √† partir des sous-scores

        # Lire le contenu du fichier une seule fois
        try:
            code_content = Path(file_path).read_text(encoding='utf-8')
        except FileNotFoundError:
            self.logger.error(f"Fichier non trouv√© pour l'audit universel: {file_path}")
            return {"error": "File not found", "quality_score": 0, "issues": [UniversalQualityIssue("CRITICAL", "Fichier non trouv√©", "FILE_NOT_FOUND").to_dict()]}
        except Exception as e:
            self.logger.error(f"Erreur de lecture du fichier {file_path}: {e}", exc_info=True)
            return {"error": f"File read error: {e}", "quality_score": 0, "issues": [UniversalQualityIssue("CRITICAL", f"Erreur lecture fichier: {e}", "FILE_READ_ERROR").to_dict()]}

        # 1. Audit Flake8
        flake8_issues_obj = await self._run_flake8(file_path)
        final_issues.extend([issue.to_dict() for issue in flake8_issues_obj])
        # score_flake8 = 100 - (len(flake8_issues_obj) * 5) # P√©nalit√© arbitraire

        # 2. Audit AST (docstrings, etc.)
        ast_report = await self._perform_ast_audit(code_content, file_path)
        final_issues.extend(ast_report.get("issues", []))
        score_ast = ast_report.get("quality_score", 0)
        
        # Calcul du score global (exemple simple, peut √™tre affin√©)
        # Ici, on prend le score AST comme base et on d√©duit pour Flake8 s'il y a des erreurs critiques non d√©j√† couvertes
        # Pour l'instant, on va juste utiliser le score AST et ajouter les probl√®mes Flake8
        score_global = score_ast 
        # On pourrait ajouter une logique pour r√©duire davantage le score_global en fonction des issues Flake8.
        # Par exemple, si Flake8 rapporte des erreurs (E) ou des erreurs critiques (F).

        # Consolidation du rapport
        consolidated_report = {
            "file_path": file_path,
            "quality_score": max(0, score_global),
            "metrics_ast": ast_report.get("metrics", {}),
            "issues": final_issues,
            "summary": {
                "total_issues": len(final_issues),
                "flake8_issues": len(flake8_issues_obj),
                "ast_issues": len(ast_report.get("issues", [])),
            }
        }
        self.logger.info(f"Audit universel termin√© pour {file_path}. Score: {consolidated_report['quality_score']}")
        return consolidated_report

    # --- Fin des nouvelles m√©thodes ---

def create_agent_MAINTENANCE_05_documenteur_peer_reviewer(**config) -> AgentMAINTENANCE05DocumenteurPeerReviewer:
    """Factory pour cr√©er une instance de l'Agent 5."""
    return AgentMAINTENANCE05DocumenteurPeerReviewer(**config)

# Ajout de la section main pour les tests
if __name__ == "__main__":
    async def run_tests():
        print("üöÄ D√©marrage des tests pour AgentMAINTENANCE05DocumenteurPeerReviewer...")
        # ‚úÖ MIGRATION SYST√àME LOGGING UNIFI√â - Configuration pour tests
        try:
            from core.manager import LoggingManager
            logging_manager = LoggingManager()
            logger = logging_manager.get_logger(
                config_name="maintenance",
                custom_config={
                    "logger_name": "nextgen.maintenance.documenteur_peer_reviewer.test",
                    "log_dir": "logs/maintenance/test",
                    "metadata": {"context": "test_cli", "agent": "MAINTENANCE_05"}
                }
            )
        except ImportError:
            # Fallback en cas d'indisponibilit√© du LoggingManager
            logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        
        agent_config = {"agent_id": "test_doc_peer_reviewer_001"}
        agent = create_agent_MAINTENANCE_05_documenteur_peer_reviewer(**agent_config)

        try:
            await agent.startup()
            health = await agent.health_check()
            agent.logger.info(f"MAIN_TEST - Health Check: {health}")
            print(f"üè• Health Check: {health}")

            capabilities = agent.get_capabilities()
            agent.logger.info(f"MAIN_TEST - Capabilities: {capabilities}")
            print(f"üõ†Ô∏è Capabilities: {capabilities}")
            assert "audit_universal_quality" in capabilities, "Capacit√© audit_universal_quality manquante !"

            # Cr√©er un fichier Python temporaire pour le test d'audit
            test_py_content = '''
# Test file for universal audit
def hello_world(): # Missing docstring
    print("Hello, world!")

class MyClass: # Missing docstring
    def __init__(self): # Missing docstring
        self.value = 10
        if self.value > 5: # Complexity point
            print("Big")

def another_func(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p): # Too many arguments (Flake8)
    pass # Missing docstring
'''
            test_file_path = Path(__file__).parent / "temp_test_audit_file.py"
            with open(test_file_path, "w", encoding="utf-8") as tf:
                tf.write(test_py_content)
            
            agent.logger.info(f"MAIN_TEST - Fichier de test cr√©√© : {test_file_path}")

            print("\nüî¨ Test de la t√¢che 'audit_universal_quality'...")
            # Utiliser str(test_file_path) pour s'assurer que c'est une cha√Æne
            audit_task_params = {"file_path": str(test_file_path)}
            audit_task = Task(id="audit_test_01", type="audit_universal_quality", params=audit_task_params)
            
            agent.logger.info(f"MAIN_TEST - Lancement de la t√¢che d'audit pour: {test_file_path}")
            audit_result = await agent.execute_task(audit_task)

            agent.logger.info(f"MAIN_TEST - Audit Result Success: {audit_result.success}")
            agent.logger.info(f"MAIN_TEST - Audit Result Data: {audit_result.data}")
            if audit_result.error:
                agent.logger.error(f"MAIN_TEST - Audit Result Error: {audit_result.error}")

            print(f"   R√©sultat de l'audit: {'Succ√®s' if audit_result.success else '√âchec'}")
            if audit_result.success and audit_result.data:
                report_data = audit_result.data.get('audit_report', {})
                print(f"   Score de qualit√© pour '{report_data.get('file_path')}': {report_data.get('quality_score', 'N/A')}/100")
                if report_data.get('issues'):
                    print("   Probl√®mes trouv√©s:")
                    for issue_dict in report_data.get('issues', []):
                        line_info = f"L{issue_dict.get('line')}" if issue_dict.get('line') else "N/A"
                        col_info = f":C{issue_dict.get('column')}" if issue_dict.get('column') else ""
                        print(f"     - {line_info}{col_info} [{issue_dict.get('code', 'N/A')}] {issue_dict.get('description', 'N/A')} ({issue_dict.get('severity', 'N/A')})")
                else:
                    print("   Aucun probl√®me substantiel trouv√© par l'audit.")
            elif not audit_result.success:
                print(f"   Erreur audit: {audit_result.error}")

            # Nettoyage du fichier de test
            if test_file_path.exists():
                test_file_path.unlink()
                agent.logger.info(f"MAIN_TEST - Fichier de test supprim√© : {test_file_path}")

        except Exception as e:
            print(f"‚ùå Erreur durant l'ex√©cution des tests de l'agent: {e}")
            agent.logger.error(f"MAIN_TEST - Erreur Exception dans run_tests(): {e}", exc_info=True)
        finally:
            await agent.shutdown()
            print("\n‚úÖ Tests termin√©s.")

    asyncio.run(run_tests())