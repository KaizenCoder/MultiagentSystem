#!/usr/bin/env python3
"""
üåü AGENT META-AUDITEUR UNIVERSEL - ORCHESTRATEUR AUTONOME D'AUDIT

Mission : Orchestration intelligente et autonome de tous les agents d'audit
- D√©tection automatique du type de module √† auditer
- D√©l√©gation intelligente aux agents sp√©cialis√©s
- Ex√©cution parall√®le et optimis√©e des audits
- Consolidation et synth√®se des r√©sultats
- G√©n√©ration de rapports holistiques
- Recommandations globales et plan d'am√©lioration

Capacit√©s Meta-Audit : Coordonne automatiquement tous les agents auditeurs
"""

import asyncio
import sys
import json
import traceback
import importlib.util
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import logging
import concurrent.futures
import time

# Pattern Factory imports avec fallback
try:
    from core.agent_factory_architecture import Agent, Task, Result
    PATTERN_FACTORY_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è Pattern Factory non disponible. Utilisation des classes de fallback.")
    PATTERN_FACTORY_AVAILABLE = False
    
    class Agent:
        pass  # TODO: Impl√©menter
        def __init__(self, agent_type: str, **config):
            pass  # TODO: Impl√©menter
        pass  # TODO: Impl√©menter
        
        # ‚úÖ MIGRATION SYST√àME LOGGING UNIFI√â
        try:
            from core.manager import LoggingManager
            logging_manager = LoggingManager()
            self.logger = logging_manager.get_logger(
                config_name="audit",
                custom_config={
                    "logger_name": f"nextgen.audit.META_AUDITEUR_UNIVERSEL.{self.agent_id if hasattr(self, 'agent_id') else self.id if hasattr(self, 'id') else 'unknown'}",
                    "log_dir": "logs/audit",
                    "metadata": {
                        "agent_type": "META_AUDITEUR_UNIVERSEL",
                        "agent_role": "audit",
                        "system": "nextgeneration"
                    }
                }
            )
        except ImportError:
            # Fallback en cas d'indisponibilit√© du LoggingManager
            self.logger = logging.getLogger(self.__class__.__name__)

            self.agent_id = f"fallback_{agent_type}"
            self.name = f"Fallback {agent_type}"
            self.logger = logging.getLogger(self.agent_id)
        async def startup(self): pass
        async def shutdown(self): pass
        async def health_check(self): return {"status": "healthy"}

    class Task:
        pass  # TODO: Impl√©menter
        def __init__(self, task_id: str, description: str, **kwargs):
            self.task_id = task_id
            self.description = description
            self.data = kwargs.get('payload', {})
            self.payload = self.data

    class Result:
        pass  # TODO: Impl√©menter
        def __init__(self, success: bool, data: any = None, error: str = None):
            self.success = success
            self.data = data
            self.error = error

class ModuleType(Enum):
    """Types de modules d√©tect√©s"""
    AGENT = "agent"
    SCRIPT = "script"
    LIBRARY = "library"
    TEST = "test"
    CONFIG = "config"
    UNKNOWN = "unknown"

class AuditPriority(Enum):
    """Priorit√©s d'audit"""
    SECURITY_FIRST = "security_first"
    PERFORMANCE_FIRST = "performance_first"
    QUALITY_FIRST = "quality_first"
    COMPLIANCE_FIRST = "compliance_first"
    BALANCED = "balanced"

@dataclass
class AuditTask:
    """T√¢che d'audit pour d√©l√©gation"""
    agent_id: str
    agent_name: str
    module_path: str
    audit_type: str
    priority: int
    estimated_duration: float
    dependencies: List[str]

@dataclass
class ConsolidatedResult:
    """R√©sultat d'audit consolid√©"""
    module_path: str
    audit_timestamp: datetime
    total_duration: float
    global_score: float
    quality_level: str
    agents_used: List[str]
    individual_results: Dict[str, Any]
    consolidated_issues: List[Dict[str, Any]]
    correlations: List[Dict[str, Any]]
    recommendations: List[Dict[str, Any]]
    improvement_plan: Dict[str, Any]

class MetaAuditeurUniversel(Agent):
    """üåü Agent Meta-Auditeur Universel - Orchestrateur Autonome"""
    
    def __init__(self, agent_type: str = "meta_auditeur_universel", **config):
        super().__init__(agent_type, **config)
        self.agent_id = "META_AUDITEUR"
        self.name = "Agent Meta-Auditeur Universel"
        self.specialite = "Orchestration autonome d'audit universel"
        self.logger = logging.getLogger(f"agent.{self.agent_id}")
        
        # Configuration des agents auditeurs disponibles
        self.available_auditors = {
            "security": {
                "agent_id": "18",
                "module": "agent_18_auditeur_securite",
                "class": "Agent18AuditeurSecurite",
                "specialization": "OWASP Top 10, vuln√©rabilit√©s, cryptographie",
                "avg_duration": 2.5
            },
            "performance": {
                "agent_id": "19", 
                "module": "agent_19_auditeur_performance",
                "class": "Agent19AuditeurPerformance",
                "specialization": "Hotspots, complexit√©, optimisation m√©moire",
                "avg_duration": 1.8
            },
            "compliance": {
                "agent_id": "20",
                "module": "agent_20_auditeur_conformite", 
                "class": "Agent20AuditeurConformite",
                "specialization": "Standards PEP8, RGPD, licences",
                "avg_duration": 1.5
            },
            "quality": {
                "agent_id": "MAINTENANCE_10",
                "module": "agent_MAINTENANCE_10_auditeur_qualite_normes",
                "class": "AgentMAINTENANCE10AuditeurQualiteNormes", 
                "specialization": "ISO 25010, documentation, maintenabilit√©",
                "avg_duration": 1.2
            },
            "general": {
                "agent_id": "111",
                "module": "agent_111_auditeur_qualite_sprint3",
                "class": "Agent111AuditeurQualiteSprint3",
                "specialization": "Audit g√©n√©ral, validation DoD",
                "avg_duration": 2.0
            }
        }
        
        # M√©triques de meta-audit
        self.meta_metrics = {
            "total_orchestrations": 0,
            "modules_audited": [],
            "agents_delegated": {},
            "average_consolidation_time": 0.0,
            "correlations_detected": 0
        }
        
        # Cache des agents instanci√©s
        self.agent_cache = {}
        
        self.logger.info(f"üåü {self.name} initialis√© - Orchestration autonome d'audit activ√©e")

    async def startup(self):
        """D√©marrage du meta-auditeur avec initialisation des agents"""
        self.logger.info(f"üåü {self.name} d√©marr√© - Initialisation des agents auditeurs...")
        
        # Pr√©-initialiser les agents auditeurs
        await self._initialize_auditor_agents()
        
        self.logger.info(f"‚úÖ Meta-auditeur op√©rationnel avec {len(self.agent_cache)} agents disponibles")

    async def shutdown(self):
        """Arr√™t du meta-auditeur avec nettoyage des agents"""
        self.logger.info(f"üåü Arr√™t {self.name} - {self.meta_metrics['total_orchestrations']} orchestrations r√©alis√©es")
        
        # Arr√™ter tous les agents en cache
        for agent in self.agent_cache.values():
            try:
                if hasattr(agent, 'shutdown'):
                    await agent.shutdown()
            except Exception as e:
                self.logger.warning(f"Erreur arr√™t agent: {e}")

    async def health_check(self) -> Dict[str, Any]:
        """V√©rification de l'√©tat du meta-auditeur"""
        # Tester la disponibilit√© des agents
        available_agents = []
        for audit_type, config in self.available_auditors.items():
            if config["agent_id"] in self.agent_cache:
                available_agents.append(audit_type)
        
        return {
            "status": "healthy",
            "agent": self.name,
            "orchestrations_performed": self.meta_metrics["total_orchestrations"],
            "available_auditors": available_agents,
            "total_auditors": len(self.available_auditors),
            "timestamp": datetime.now().isoformat()
        }

    def get_capabilities(self) -> List[str]:
        """Capacit√©s du meta-auditeur"""
        return [
            "meta_audit_complet",
            "audit_intelligent", 
            "orchestration_autonome",
            "consolidation_resultats",
            "detection_type_module",
            "delegation_automatique",
            "correlation_issues",
            "generation_plan_amelioration"
        ]

    async def execute_task(self, task: Task) -> Result:
        """Ex√©cution des t√¢ches de meta-audit"""
        try:
            task_type = task.task_id if hasattr(task, 'task_id') else task.description
            
            self.logger.info(f"üåü Ex√©cution meta-t√¢che: {task_type}")
            
            if task_type == "meta_audit_complet" or task.description == "meta_audit_complet":
                # Audit complet autonome
                module_path = task.payload.get('module_path') if hasattr(task, 'payload') else None
                if not module_path and hasattr(task, 'data'):
                    module_path = task.data.get('module_path')
                
                if module_path:
                    result = await self.audit_complet(module_path)
                    return Result(success=True, data=result)
                else:
                    return Result(success=False, error="module_path requis pour meta-audit")
                    
            elif task_type == "audit_intelligent":
                # Audit intelligent avec configuration
                module_path = task.payload.get('module_path')
                config = task.payload.get('config', {})
                result = await self.audit_intelligent(module_path, config)
                return Result(success=True, data=result)
                
            else:
                return Result(success=False, error=f"Type de meta-t√¢che non support√©: {task_type}")
                
        except Exception as e:
            self.logger.error(f"‚ùå Erreur execute_task meta-auditeur: {e}", exc_info=True)
            return Result(success=False, error=str(e))

    async def audit_complet(self, module_path: str) -> Dict[str, Any]:
        """üåü AUDIT COMPLET AUTONOME - Orchestration automatique compl√®te"""
        self.logger.info(f"üåü D√©marrage audit complet autonome: {module_path}")
        
        start_time = datetime.now()
        
        try:
            # 1. D√©tection intelligente du type de module
            module_type = await self._detect_module_type(module_path)
            self.logger.info(f"üìã Type d√©tect√©: {module_type.value}")
            
            # 2. Planification intelligente des audits
            audit_plan = await self._plan_audit_strategy(module_path, module_type)
            self.logger.info(f"üìÖ Plan d'audit: {len(audit_plan)} agents s√©lectionn√©s")
            
            # 3. Ex√©cution parall√®le optimis√©e
            results = await self._execute_parallel_audits(audit_plan)
            self.logger.info(f"‚ö° Audits termin√©s: {len(results)} r√©sultats")
            
            # 4. Consolidation et synth√®se
            consolidated = await self._consolidate_results(module_path, results, start_time)
            
            # 5. D√©tection de corr√©lations
            correlations = await self._detect_correlations(results)
            consolidated.correlations = correlations
            
            # 6. G√©n√©ration plan d'am√©lioration
            improvement_plan = await self._generate_improvement_plan(consolidated)
            consolidated.improvement_plan = improvement_plan
            
            # Mise √† jour m√©triques
            self.meta_metrics["total_orchestrations"] += 1
            self.meta_metrics["modules_audited"].append(module_path)
            self.meta_metrics["correlations_detected"] += len(correlations)
            
            end_time = datetime.now()
            total_duration = (end_time - start_time).total_seconds()
            
            self.logger.info(f"‚úÖ Meta-audit complet termin√©: {module_path} - Score global: {consolidated.global_score}/100 en {total_duration:.2f}s")
            
            return asdict(consolidated)
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur meta-audit complet {module_path}: {e}", exc_info=True)
            return {
                "status": "failed",
                "error": str(e),
                "error_details": traceback.format_exc(),
                "module_path": module_path
            }

    async def audit_intelligent(self, module_path: str, config: Dict[str, Any] = None) -> Dict[str, Any]:
        """üß† AUDIT INTELLIGENT - Avec configuration personnalis√©e"""
        if config is None:
            config = {}
            
        self.logger.info(f"üß† Audit intelligent: {module_path} avec config personnalis√©e")
        
        # Adapter la strat√©gie selon la configuration
        priority = config.get("priorite", "balanced")
        profondeur = config.get("profondeur", "standard")
        agents_excludes = config.get("agents_excludes", [])
        seuil_qualite = config.get("seuil_qualite", 75)
        
        # Utiliser audit_complet mais avec strat√©gie adapt√©e
        result = await self.audit_complet(module_path)
        
        # Post-traitement selon la configuration
        if result.get("global_score", 0) < seuil_qualite:
            self.logger.warning(f"‚ö†Ô∏è Score {result.get('global_score')} < seuil {seuil_qualite}")
            result["quality_alert"] = True
            result["quality_gap"] = seuil_qualite - result.get("global_score", 0)
        
        return result

    async def _initialize_auditor_agents(self):
        """Initialisation des agents auditeurs"""
        self.logger.info("üîß Initialisation des agents auditeurs...")
        
        for audit_type, config in self.available_auditors.items():
            try:
                # Import dynamique du module
                module_path = f"agents.{config['module']}"
                spec = importlib.util.find_spec(module_path)
                
                if spec is None:
                    self.logger.warning(f"‚ö†Ô∏è Module {module_path} non trouv√©")
                    continue
                
                # Cr√©er une instance mock pour les tests
                self.agent_cache[config["agent_id"]] = MockAuditorAgent(
                    agent_id=config["agent_id"],
                    specialization=config["specialization"]
                )
                
                self.logger.info(f"‚úÖ Agent {config['agent_id']} ({audit_type}) initialis√©")
                
            except Exception as e:
                self.logger.error(f"‚ùå Erreur initialisation agent {audit_type}: {e}")

    async def _detect_module_type(self, module_path: str) -> ModuleType:
        """D√©tection intelligente du type de module"""
        try:
            path = Path(module_path)
            filename = path.name.lower()
            
            # Lecture du contenu pour analyse
            with open(module_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # D√©tection par nom de fichier
            if filename.startswith('agent_'):
                return ModuleType.AGENT
            elif filename.startswith('test_'):
                return ModuleType.TEST
            elif 'config' in filename:
                return ModuleType.CONFIG
            
            # D√©tection par contenu
            if 'class Agent' in content or 'from core.agent_factory_architecture' in content:
                return ModuleType.AGENT
            elif 'def test_' in content or 'import pytest' in content:
                return ModuleType.TEST
            elif 'if __name__ == "__main__"' in content:
                return ModuleType.SCRIPT
            else:
                return ModuleType.LIBRARY
                
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erreur d√©tection type: {e}")
            return ModuleType.UNKNOWN

    async def _plan_audit_strategy(self, module_path: str, module_type: ModuleType) -> List[AuditTask]:
        """Planification intelligente de la strat√©gie d'audit"""
        audit_tasks = []
        
        # Strat√©gie selon le type de module
        if module_type == ModuleType.AGENT:
            # Audit complet pour les agents
            priorities = {
                "security": 1,    # S√©curit√© prioritaire pour agents
                "quality": 2,     # Qualit√© essentielle
                "compliance": 3,  # Conformit√© Pattern Factory
                "performance": 4  # Performance pour optimisation
            }
        elif module_type == ModuleType.SCRIPT:
            priorities = {
                "quality": 1,
                "security": 2,
                "performance": 3,
                "compliance": 4
            }
        else:
            # Strat√©gie par d√©faut
            priorities = {
                "quality": 1,
                "compliance": 2,
                "security": 3,
                "performance": 4
            }
        
        # Cr√©er les t√¢ches selon la strat√©gie
        for audit_type, priority in priorities.items():
            if audit_type in self.available_auditors:
                config = self.available_auditors[audit_type]
                
                task = AuditTask(
                    agent_id=config["agent_id"],
                    agent_name=config["module"],
                    module_path=module_path,
                    audit_type=audit_type,
                    priority=priority,
                    estimated_duration=config["avg_duration"],
                    dependencies=[]
                )
                audit_tasks.append(task)
        
        # Trier par priorit√©
        audit_tasks.sort(key=lambda x: x.priority)
        
        return audit_tasks

    async def _execute_parallel_audits(self, audit_plan: List[AuditTask]) -> Dict[str, Any]:
        """Ex√©cution parall√®le optimis√©e des audits"""
        self.logger.info(f"‚ö° Ex√©cution parall√®le de {len(audit_plan)} audits")
        
        results = {}
        
        # Cr√©er des t√¢ches asynchrones pour parall√©lisation
        async def execute_single_audit(task: AuditTask) -> Tuple[str, Any]:
            try:
                agent = self.agent_cache.get(task.agent_id)
                if not agent:
                    return task.audit_type, {"error": f"Agent {task.agent_id} non disponible"}
                
                # Simuler l'audit avec l'agent
                result = await agent.mock_audit(task.module_path, task.audit_type)
                return task.audit_type, result
                
            except Exception as e:
                self.logger.error(f"‚ùå Erreur audit {task.audit_type}: {e}")
                return task.audit_type, {"error": str(e)}
        
        # Ex√©cuter en parall√®le
        tasks = [execute_single_audit(task) for task in audit_plan]
        completed_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Compiler les r√©sultats
        for audit_type, result in completed_results:
            if not isinstance(result, Exception):
                results[audit_type] = result
                self.meta_metrics["agents_delegated"][audit_type] = self.meta_metrics["agents_delegated"].get(audit_type, 0) + 1
        
        return results

    async def _consolidate_results(self, module_path: str, results: Dict[str, Any], start_time: datetime) -> ConsolidatedResult:
        """Consolidation intelligente des r√©sultats multiples"""
        end_time = datetime.now()
        total_duration = (end_time - start_time).total_seconds()
        
        # Calcul du score global pond√©r√©
        weights = {
            "security": 0.30,     # 30% s√©curit√©
            "quality": 0.25,      # 25% qualit√©  
            "compliance": 0.25,   # 25% conformit√©
            "performance": 0.20   # 20% performance
        }
        
        scores = {}
        all_issues = []
        agents_used = []
        
        for audit_type, result in results.items():
            if "error" not in result:
                score = result.get("quality_score", result.get("score", 0))
                scores[audit_type] = score
                agents_used.append(self.available_auditors[audit_type]["agent_id"])
                
                # Collecter les issues
                if "issues" in result:
                    for issue in result["issues"]:
                        issue["source_audit"] = audit_type
                        all_issues.append(issue)
                elif "summary" in result and "total_issues" in result["summary"]:
                    # Simuler des issues bas√©es sur le score
                    issue_count = result["summary"]["total_issues"]
                    for i in range(min(issue_count, 5)):  # Limiter pour demo
                        all_issues.append({
                            "type": f"{audit_type}_issue_{i+1}",
                            "severity": "medium",
                            "source_audit": audit_type,
                            "description": f"Issue d√©tect√©e par audit {audit_type}"
                        })
        
        # Score global pond√©r√©
        global_score = 0
        total_weight = 0
        for audit_type, score in scores.items():
            weight = weights.get(audit_type, 0.1)
            global_score += score * weight
            total_weight += weight
        
        if total_weight > 0:
            global_score = global_score / total_weight
        
        # D√©terminer niveau de qualit√©
        if global_score >= 90:
            quality_level = "excellent"
        elif global_score >= 75:
            quality_level = "good"
        elif global_score >= 60:
            quality_level = "acceptable"
        elif global_score >= 40:
            quality_level = "poor"
        else:
            quality_level = "critical"
        
        return ConsolidatedResult(
            module_path=module_path,
            audit_timestamp=start_time,
            total_duration=round(total_duration, 2),
            global_score=round(global_score, 2),
            quality_level=quality_level,
            agents_used=agents_used,
            individual_results=results,
            consolidated_issues=all_issues,
            correlations=[],  # Sera rempli par _detect_correlations
            recommendations=[],  # Sera rempli par _generate_improvement_plan
            improvement_plan={}
        )

    async def _detect_correlations(self, results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """D√©tection de corr√©lations entre les r√©sultats d'audit"""
        correlations = []
        
        try:
            # Exemple: corr√©lation entre s√©curit√© faible et qualit√© faible
            security_score = results.get("security", {}).get("score", 100)
            quality_score = results.get("quality", {}).get("score", 100)
            
            if security_score < 60 and quality_score < 60:
                correlations.append({
                    "type": "security_quality_correlation",
                    "description": "Corr√©lation d√©tect√©e: s√©curit√© faible ET qualit√© faible",
                    "impact": "high",
                    "recommendation": "R√©vision compl√®te du code recommand√©e"
                })
            
            # Exemple: corr√©lation performance-complexit√©
            performance_score = results.get("performance", {}).get("score", 100)
            if performance_score < 70:
                correlations.append({
                    "type": "performance_complexity_correlation", 
                    "description": "Performance faible potentiellement li√©e √† la complexit√©",
                    "impact": "medium",
                    "recommendation": "Analyser la complexit√© cyclomatique"
                })
                
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erreur d√©tection corr√©lations: {e}")
        
        return correlations

    async def _generate_improvement_plan(self, consolidated: ConsolidatedResult) -> Dict[str, Any]:
        """G√©n√©ration d'un plan d'am√©lioration intelligent"""
        plan = {
            "actions_immediates": [],
            "roadmap_court_terme": [],
            "roadmap_long_terme": [],
            "priorite_globale": "medium"
        }
        
        # Actions imm√©diates selon le score global
        if consolidated.global_score < 40:
            plan["priorite_globale"] = "critical"
            plan["actions_immediates"].extend([
                "üö® Arr√™t d√©ploiement - Qualit√© critique",
                "üîß Refactoring complet n√©cessaire",
                "üß™ Tests approfondis avant toute utilisation"
            ])
        elif consolidated.global_score < 60:
            plan["priorite_globale"] = "high"
            plan["actions_immediates"].extend([
                "‚ö†Ô∏è Am√©liorations significatives requises",
                "üîç Review de code approfondie",
                "üìù Documentation et tests manquants"
            ])
        elif consolidated.global_score < 80:
            plan["priorite_globale"] = "medium"
            plan["roadmap_court_terme"].extend([
                "‚ú® Optimisations recommand√©es",
                "üìö Am√©lioration documentation",
                "üîß Refactoring mineur"
            ])
        else:
            plan["priorite_globale"] = "low"
            plan["roadmap_long_terme"].extend([
                "üèÜ Maintenir le niveau d'excellence",
                "üìä Monitoring continu de la qualit√©",
                "üöÄ Optimisations avanc√©es"
            ])
        
        # Actions sp√©cifiques par nombre d'issues critiques
        critical_issues = len([i for i in consolidated.consolidated_issues if i.get("severity") == "critical"])
        if critical_issues > 0:
            plan["actions_immediates"].append(f"üî• Traiter {critical_issues} issues critiques")
        
        return plan


class MockAuditorAgent:
    """Agent auditeur mock pour les tests"""
    
    def __init__(self, agent_id: str, specialization: str):
        self.agent_id = agent_id
        self.specialization = specialization
        self.logger = logging.getLogger(f"mock_agent_{agent_id}")
    
    async def mock_audit(self, module_path: str, audit_type: str) -> Dict[str, Any]:
        """Simulation d'audit avec r√©sultats r√©alistes"""
        await asyncio.sleep(0.1)  # Simuler le temps d'audit
        
        # G√©n√©rer un score al√©atoire mais coh√©rent
        import random
        random.seed(hash(module_path + audit_type))  # Reproductible
        
        base_score = random.randint(60, 95)
        issues_count = random.randint(0, 20)
        critical_issues = random.randint(0, min(3, issues_count))
        
        return {
            "audit_type": audit_type,
            "agent_id": self.agent_id,
            "module_path": module_path,
            "score": base_score,
            "quality_score": base_score,
            "status": "completed",
            "issues": [
                {
                    "type": f"{audit_type}_issue_{i}",
                    "severity": "critical" if i < critical_issues else "medium",
                    "description": f"Issue {audit_type} #{i+1}",
                    "line_number": random.randint(1, 100)
                }
                for i in range(min(issues_count, 5))
            ],
            "summary": {
                "total_issues": issues_count,
                "critical_issues": critical_issues
            },
            "specialization": self.specialization
        }


# Point d'entr√©e CLI pour test
async def main():
    """Test CLI du Meta-Auditeur Universel"""
    logging.basicConfig(level=logging.INFO)
    print("üåü Meta-Auditeur Universel - Test d'orchestration autonome")
    
    meta_auditor = MetaAuditeurUniversel()
    await meta_auditor.startup()
    
    try:
        # Test 1: Audit complet autonome
        print("\nüìã Test 1: Audit complet autonome")
        result = await meta_auditor.audit_complet("agents/agent_MAINTENANCE_10_auditeur_qualite_normes.py")
        
        if result.get("status") != "failed":
            print(f"‚úÖ Audit complet r√©ussi!")
            print(f"   üìä Score global: {result['global_score']}/100")
            print(f"   üéØ Niveau qualit√©: {result['quality_level']}")
            print(f"   ‚ö° Dur√©e totale: {result['total_duration']}s")
            print(f"   ü§ñ Agents utilis√©s: {', '.join(result['agents_used'])}")
            print(f"   üîç Issues consolid√©es: {len(result['consolidated_issues'])}")
            print(f"   üîó Corr√©lations: {len(result['correlations'])}")
            
            if result['improvement_plan']:
                plan = result['improvement_plan']
                print(f"   üìã Plan d'am√©lioration: {plan['priorite_globale']} priorit√©")
                if plan['actions_immediates']:
                    print(f"      Actions imm√©diates: {len(plan['actions_immediates'])}")
        else:
            print(f"‚ùå Audit √©chou√©: {result.get('error')}")
        
        # Test 2: Health check
        print("\nüìã Test 2: Health check meta-auditeur")
        health = await meta_auditor.health_check()
        print(f"‚úÖ Status: {health['status']}")
        print(f"   Orchestrations: {health['orchestrations_performed']}")
        print(f"   Auditeurs disponibles: {len(health['available_auditors'])}/{health['total_auditors']}")
        
        # Test 3: Audit intelligent avec config
        print("\nüìã Test 3: Audit intelligent avec configuration")
        config = {
            "priorite": "security_first",
            "profondeur": "exhaustive", 
            "seuil_qualite": 85
        }
        result2 = await meta_auditor.audit_intelligent("agents/agent_01_coordinateur_principal.py", config)
        print(f"‚úÖ Audit intelligent: Score {result2.get('global_score', 'N/A')}/100")
        if result2.get('quality_alert'):
            print(f"   ‚ö†Ô∏è Alerte qualit√©: Gap de {result2['quality_gap']} points")
        
        print("\nüéØ R√âSUM√â META-AUDIT")
        print("="*60)
        print("‚úÖ Orchestration autonome op√©rationnelle")
        print("‚úÖ D√©l√©gation intelligente aux agents sp√©cialis√©s")
        print("‚úÖ Consolidation et corr√©lation des r√©sultats")
        print("‚úÖ Plan d'am√©lioration automatique")
        print("‚úÖ Configuration intelligente adaptable")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur pendant les tests: {e}")
        import traceback
        traceback.print_exc()
        return False
        
    finally:
        await meta_auditor.shutdown()

def create_agent_META_AUDITEUR_UNIVERSEL(**kwargs) -> MetaAuditeurUniversel:
    return MetaAuditeurUniversel(**kwargs)
    # ‚úÖ M√âTHODES STANDARDIS√âES DE RAPPORT

    def _calculate_report_score(self, metrics: Dict[str, Any]) -> int:
        """Calcule le score global du rapport bas√© sur les m√©triques."""
        score = 0
        issues_critiques = []
        
        # Logique de scoring sp√©cifique √† l'agent
        # √Ä adapter selon le type d'agent
        
        return score
    
    def _assess_conformity(self, score: int) -> str:
        """√âvalue la conformit√© bas√©e sur le score."""
        if score >= 90:
            return "‚úÖ CONFORME - OPTIMAL"
        elif score >= 70:
            return "‚úÖ CONFORME - ACCEPTABLE"
        else:
            return "‚ùå NON CONFORME - CRITIQUE"
    
    def _get_quality_level(self, score: int) -> str:
        """D√©termine le niveau de qualit√©."""
        if score >= 90:
            return "OPTIMAL"
        elif score >= 70:
            return "ACCEPTABLE"
        else:
            return "CRITIQUE"
    
    def _generate_recommendations(self, metrics: Dict[str, Any], issues: List[str]) -> List[str]:
        """G√©n√®re les recommandations bas√©es sur l'analyse."""
        recommendations = []
        
        # Logique de g√©n√©ration de recommandations
        # √Ä adapter selon le type d'agent
        
        return recommendations
    
    def _generate_standard_report(self, context: Dict, metrics: Dict, timestamp) -> Dict[str, Any]:
        """G√©n√®re un rapport selon le format standard de l'agent 06."""
        
        score = self._calculate_report_score(metrics)
        conformity = self._assess_conformity(score)
        quality_level = self._get_quality_level(score)
        
        agent_filename = Path(__file__).name
        
        # Issues critiques (√† personnaliser selon l'agent)
        issues_critiques = []
        
        return {
            'agent_id': getattr(self, 'agent_id', 'unknown'),
            'agent_file_name': agent_filename,
            'type_rapport': 'standard',  # √Ä personnaliser
            'timestamp': timestamp.isoformat(),
            'specialisation': 'Agent Sp√©cialis√©',  # √Ä personnaliser
            'score_global': score,
            'niveau_qualite': quality_level,
            'conformite': conformity,
            'signature_cryptographique': 'N/A (Fonctionnalit√© non impl√©ment√©e pour cet agent)',
            'issues_critiques_identifies': len(issues_critiques),
            'architecture': {
                'description': "Description de l'architecture de l'agent",
                'statut_operationnel': f"Syst√®me {getattr(self, 'agent_id', 'unknown')} op√©rationnel.",
                'confirmation_specialisation': f"{getattr(self, 'agent_id', 'unknown')} confirm√© comme sp√©cialiste.",
                'objectifs_principaux': [
                    "Objectif principal 1",
                    "Objectif principal 2",
                    "Objectif principal 3"
                ],
                'technologies_cles': ["Technologie 1", "Technologie 2"]
            },
            'recommandations': self._generate_recommendations(metrics, issues_critiques),
            'issues_critiques_details': issues_critiques if issues_critiques else [
                "Aucun issue critique majeur d√©tect√©. Le syst√®me fonctionne dans les param√®tres attendus."
            ],
            'details_techniques': {
                'strategie': "Strat√©gie technique de l'agent",
                'composants_actifs': [],
                'metriques_collectees': metrics
            },
            'metriques_detaillees': {
                'score_global': {'actuel': score, 'cible': 100},
                'conformite_pourcentage': {'actuel': score, 'cible': 100, 'unite': '%'}
            },
            'impact_business': {
                'criticite': 'MOYENNE' if score >= 70 else 'HAUTE',
                'domaines_impactes': [],
                'actions_requises': []
            }
        }


    def _generate_markdown_report(self, rapport_json: Dict, context: Dict, timestamp) -> str:
        """G√©n√®re un rapport Markdown selon le format standard."""
        
        agent_name = rapport_json.get('agent_id', 'Agent Inconnu')
        type_rapport = rapport_json.get('type_rapport', 'standard')
        score = rapport_json.get('score_global', 0)
        quality = rapport_json.get('niveau_qualite', 'UNKNOWN')
        conformity = rapport_json.get('conformite', 'NON √âVALU√â')
        
        markdown_content = f"""# üìä RAPPORT STRAT√âGIQUE : {agent_name.upper()}

## üéØ R√âSUM√â EX√âCUTIF

**Agent :** {agent_name}  
**Type de Rapport :** {type_rapport}  
**Date de G√©n√©ration :** {timestamp.strftime('%Y-%m-%d %H:%M:%S')}  
**Score Global :** {score}/100  
**Niveau de Qualit√© :** {quality}  
**Conformit√© :** {conformity}  

## üìà ANALYSE GLOBALE

### Score de Performance
- **Score Actuel :** {score}/100
- **Objectif :** 100/100
- **Statut :** {'üü¢ ACCEPTABLE' if score >= 70 else 'üî¥ CRITIQUE'}

### Architecture
{rapport_json.get('architecture', {}).get('description', 'Description non disponible')}

**Objectifs Principaux :**
"""
        
        # Ajouter les objectifs
        for obj in rapport_json.get('architecture', {}).get('objectifs_principaux', []):
            markdown_content += f"- {obj}\n"
        
        markdown_content += f"""
**Technologies Cl√©s :**
"""
        
        # Ajouter les technologies
        for tech in rapport_json.get('architecture', {}).get('technologies_cles', []):
            markdown_content += f"- {tech}\n"
        
        markdown_content += f"""

## üîç RECOMMANDATIONS

"""
        
        # Ajouter les recommandations
        for reco in rapport_json.get('recommandations', []):
            markdown_content += f"- {reco}\n"
        
        markdown_content += f"""

## ‚ö†Ô∏è ISSUES CRITIQUES

"""
        
        # Ajouter les issues critiques
        for issue in rapport_json.get('issues_critiques_details', []):
            markdown_content += f"- {issue}\n"
        
        markdown_content += f"""

## üìä M√âTRIQUES D√âTAILL√âES

### Performance Globale
- **Score Global :** {rapport_json.get('metriques_detaillees', {}).get('score_global', {}).get('actuel', 0)}/{rapport_json.get('metriques_detaillees', {}).get('score_global', {}).get('cible', 100)}
- **Conformit√© :** {rapport_json.get('metriques_detaillees', {}).get('conformite_pourcentage', {}).get('actuel', 0)}%

## üéØ IMPACT BUSINESS

**Criticit√© :** {rapport_json.get('impact_business', {}).get('criticite', 'NON √âVALU√â')}

### Domaines Impact√©s
"""
        
        # Ajouter les domaines impact√©s
        for domaine in rapport_json.get('impact_business', {}).get('domaines_impactes', []):
            markdown_content += f"- {domaine}\n"
        
        markdown_content += f"""

### Actions Requises
"""
        
        # Ajouter les actions requises
        for action in rapport_json.get('impact_business', {}).get('actions_requises', []):
            markdown_content += f"- {action}\n"
        
        markdown_content += f"""

---
*Rapport g√©n√©r√© automatiquement par {agent_name} - NextGeneration System*  
*Timestamp: {timestamp.isoformat()}*
"""
        
        return markdown_content



if __name__ == "__main__":
    import asyncio
    asyncio.run(main())