#!/usr/bin/env python3
"""
üß™ AGENT TESTEUR D'AGENTS - PATTERN FACTORY NEXTGENERATION
Mission: Validation s√©curis√©e et automatis√©e des agents Pattern Factory avec rapports strat√©giques
"""

import asyncio
import sys
from pathlib import Path
import json
from datetime import datetime
from typing import Dict, List, Any
import tempfile

# Import Pattern Factory
sys.path.insert(0, str(Path(__file__).parent))
try:
    from core.agent_factory_architecture import Agent, Task, Result
    PATTERN_FACTORY_AVAILABLE = True
except ImportError:
    # Fallback classes
    class Agent:
        pass  # TODO: Impl√©menter
        def __init__(self, agent_type: str, **config):
            pass  # TODO: Impl√©menter
        pass  # TODO: Impl√©menter
        
        # ‚úÖ MIGRATION SYST√àME LOGGING UNIFI√â
        try:
            from core.manager import LoggingManager
            logging_manager = LoggingManager()
            self.logger = logging_manager.get_logger(
                config_name="test",
                custom_config={
                    "logger_name": f"nextgen.test.testeur_agents_complet.{self.agent_id if hasattr(self, 'agent_id') else self.id if hasattr(self, 'id') else 'unknown'}",
                    "log_dir": "logs/test",
                    "metadata": {
                        "agent_type": "testeur_agents_complet",
                        "agent_role": "test",
                        "system": "nextgeneration"
                    }
                }
            )
        except ImportError:
            # Fallback en cas d'indisponibilit√© du LoggingManager
            self.logger = logging.getLogger(self.__class__.__name__)

            self.agent_id = f"testeur_agents_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            self.agent_type = agent_type
            self.config = config
            import logging
            self.logger = logging.getLogger("AgentTesteurAgents")
        async def startup(self): pass
        async def shutdown(self): pass
        async def health_check(self): return {"status": "healthy"}
    
    class Task:
        pass  # TODO: Impl√©menter
        def __init__(self, **kwargs):
            for key, value in kwargs.items():
                setattr(self, key, value)
    
    class Result:
        pass  # TODO: Impl√©menter
        def __init__(self, success: bool, data: Any = None, error: str = None):
            self.success = success
            self.data = data
            self.error = error
    
    PATTERN_FACTORY_AVAILABLE = False

class AgentTesteurAgents(Agent):
    """Agent Testeur d'Agents avec rapports strat√©giques"""
    
    def __init__(self, **config):
        super().__init__("testeur_agents", **config)
        self.test_timeout = config.get("test_timeout", 30)
        self.safe_mode = config.get("safe_mode", True)
        self.max_concurrent_tests = config.get("max_concurrent_tests", 3)
        self.test_results_cache = {}
        self.performance_metrics = {}
        self.health_monitoring = {}
        
        if not hasattr(self, 'logger'):
            import logging
            self.logger = logging.getLogger("AgentTesteurAgents")
        
        if not hasattr(self, 'agent_id'):
            self.agent_id = f"testeur_agents_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        if not hasattr(self, 'agent_type'):
            self.agent_type = "testeur_agents"
        
        self.logger.info(f"üß™ Agent Testeur d'Agents initialis√© - ID: {self.agent_id}")
    
    async def startup(self):
        """D√©marrage agent testeur d'agents"""
        self.logger.info(f"üöÄ Agent Testeur d'Agents {self.agent_id} - D√âMARRAGE")
        await self.initialiser_environnement_test()
        await self.charger_cache_resultats()
        self.logger.info("‚úÖ Agent Testeur d'Agents d√©marr√© avec succ√®s")
    
    async def shutdown(self):
        """Arr√™t agent testeur d'agents"""
        self.logger.info(f"üõë Agent Testeur d'Agents {self.agent_id} - ARR√äT")
        await self.sauvegarder_cache_resultats()
        await self.generer_rapport_final()
    
    async def health_check(self) -> Dict[str, Any]:
        """V√©rification sant√© agent testeur d'agents"""
        return {
            "agent_id": self.agent_id,
            "agent_type": self.agent_type,
            "status": "healthy",
            "safe_mode": self.safe_mode,
            "test_timeout": self.test_timeout,
            "cached_results": len(self.test_results_cache),
            "monitored_agents": len(self.health_monitoring),
            "capabilities": self.get_capabilities(),
            "timestamp": datetime.now().isoformat()
        }
    
    async def execute_task(self, task: Any) -> Any:
        """Ex√©cution d'une t√¢che sp√©cifique"""
        try:
            self.logger.info(f"üìã Ex√©cution t√¢che testeur: {task}")
            
            if isinstance(task, dict):
                task_type = task.get("type")
                if task_type == "test_agent":
                    return await self.tester_agent(task.get("agent_path"))
                elif task_type == "test_all_agents":
                    return await self.tester_tous_agents()
                elif task_type == "monitor_health":
                    return await self.monitorer_sante_agents()
                elif task_type == "validate_compliance":
                    return await self.valider_conformite_pattern_factory()
            
            # Support des t√¢ches avec objets Task (g√©n√©ration de rapports)
            if hasattr(task, 'name'):
                if task.name == "generate_strategic_report":
                    try:
                        context = getattr(task, 'context', {})
                        type_rapport = getattr(task, 'type_rapport', 'testing')
                        format_sortie = getattr(task, 'format_sortie', 'json')
                        
                        rapport = await self.generer_rapport_strategique(context, type_rapport)
                        
                        if format_sortie == 'markdown':
                            rapport_md = await self.generer_rapport_markdown(rapport, type_rapport, context)
                            
                            agent_specific_reports_dir = Path("reports") / "agent_testeur_agents"
                            agent_specific_reports_dir.mkdir(parents=True, exist_ok=True)
                            
                            timestamp_str = datetime.now().strftime("%Y-%m-%d_%H%M%S")
                            filename = f"strategic_report_{type_rapport}_{timestamp_str}.md"
                            filepath = agent_specific_reports_dir / filename
                            
                            with open(filepath, 'w', encoding='utf-8') as f:
                                f.write(rapport_md)
                            
                            return Result(success=True, data={
                                'rapport_json': rapport,
                                'rapport_markdown': rapport_md,
                                'fichier_sauvegarde': str(filepath)
                            })
                        
                        return Result(success=True, data=rapport)
                    except Exception as e:
                        self.logger.error(f"Erreur g√©n√©ration rapport strat√©gique: {e}")
                        return Result(success=False, error=f"Exception rapport: {str(e)}")
            
            return await self.executer_tests_complets()
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur ex√©cution t√¢che testeur: {e}")
            return {"error": str(e)}
    
    def get_capabilities(self) -> List[str]:
        """Retourne les capacit√©s de l'agent"""
        return [
            "agent_unit_testing",
            "pattern_factory_validation",
            "health_monitoring",
            "performance_testing",
            "regression_testing",
            "safe_execution",
            "automated_reporting",
            "quality_gates",
            "compliance_checking",
            "error_detection",
            "strategic_reporting"
        ]
    
    # === M√âTHODES DE RAPPORTS STRAT√âGIQUES ===
    
    async def generer_rapport_strategique(self, context: Dict[str, Any], type_rapport: str = 'testing') -> Dict[str, Any]:
        """G√©n√©ration de rapports strat√©giques pour les tests et la validation d'agents"""
        self.logger.info(f"üéØ G√©n√©ration rapport strat√©gique tests type: {type_rapport}")
        
        timestamp = datetime.now()
        metriques_base = await self._collecter_metriques_tests()
        
        if type_rapport == 'testing':
            return await self._generer_rapport_testing(context, metriques_base, timestamp)
        elif type_rapport == 'compliance':
            return await self._generer_rapport_compliance(context, metriques_base, timestamp)
        elif type_rapport == 'performance_tests':
            return await self._generer_rapport_performance_tests(context, metriques_base, timestamp)
        elif type_rapport == 'quality_assurance':
            return await self._generer_rapport_quality_assurance(context, metriques_base, timestamp)
        else:
            return await self._generer_rapport_testing(context, metriques_base, timestamp)

    async def _collecter_metriques_tests(self) -> Dict[str, Any]:
        """Collecte les m√©triques de tests et validation d'agents"""
        try:
            total_tests = len(self.test_results_cache)
            tests_reussis = sum(1 for result in self.test_results_cache.values() 
                              if result.get('status') == 'success')
            tests_warnings = sum(1 for result in self.test_results_cache.values() 
                               if result.get('status') == 'warning')
            tests_erreurs = total_tests - tests_reussis - tests_warnings
            
            scores = [result.get('score_global', 0) for result in self.test_results_cache.values() 
                     if 'score_global' in result]
            score_moyen = sum(scores) / len(scores) if scores else 0
            
            agents_conformes = 0
            agents_non_conformes = 0
            for result in self.test_results_cache.values():
                pf_test = result.get('tests', {}).get('pattern_factory', {})
                if pf_test.get('success', False):
                    agents_conformes += 1
                else:
                    agents_non_conformes += 1
            
            perf_metrics = self.performance_metrics.copy()
            perf_metrics['current_timestamp'] = datetime.now().isoformat()
            
            agents_monitores = len(self.health_monitoring)
            alertes_actives = sum(len(monitoring.get('alerts', [])) 
                                for monitoring in self.health_monitoring.values())
            
            return {
                'tests_execution': {
                    'total_tests': total_tests,
                    'tests_reussis': tests_reussis,
                    'tests_warnings': tests_warnings,
                    'tests_erreurs': tests_erreurs,
                    'taux_reussite': (tests_reussis / total_tests * 100) if total_tests > 0 else 0,
                    'score_moyen': score_moyen
                },
                'conformite_pattern_factory': {
                    'agents_conformes': agents_conformes,
                    'agents_non_conformes': agents_non_conformes,
                    'taux_conformite': (agents_conformes / (agents_conformes + agents_non_conformes) * 100) 
                                     if (agents_conformes + agents_non_conformes) > 0 else 0
                },
                'monitoring_sante': {
                    'agents_monitores': agents_monitores,
                    'alertes_actives': alertes_actives,
                    'statut_monitoring': 'actif' if agents_monitores > 0 else 'inactif'
                },
                'performance_metrics': perf_metrics,
                'configuration_agent': {
                    'safe_mode': self.safe_mode,
                    'test_timeout': self.test_timeout,
                    'max_concurrent_tests': self.max_concurrent_tests
                },
                'derniere_maj': datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"Erreur collecte m√©triques tests: {e}")
            return {'erreur': str(e), 'metriques_partielles': True}

    async def _generer_rapport_testing(self, context: Dict, metriques: Dict, timestamp: datetime) -> Dict[str, Any]:
        """G√©n√®re un rapport strat√©gique centr√© sur les tests d'agents"""
        
        tests_exec = metriques.get('tests_execution', {})
        score_testing = tests_exec.get('taux_reussite', 0)
        score_qualite = tests_exec.get('score_moyen', 0)
        
        recommandations = []
        if score_testing < 80:
            recommandations.append("üî• CRITIQUE: Am√©liorer la qualit√© des agents - taux de r√©ussite < 80%")
        if score_qualite < 70:
            recommandations.append("üìà QUALIT√â: Renforcer la conformit√© Pattern Factory")
        if tests_exec.get('tests_erreurs', 0) > 0:
            recommandations.append("üõ†Ô∏è MAINTENANCE: Corriger les agents en erreur")
        
        if not recommandations:
            recommandations.append("‚úÖ EXCELLENT: Tous les tests passent avec succ√®s")
        
        points_critiques = []
        if tests_exec.get('tests_erreurs', 0) > 3:
            points_critiques.append(f"Trop d'agents en erreur: {tests_exec['tests_erreurs']}")
        
        monitoring = metriques.get('monitoring_sante', {})
        if monitoring.get('alertes_actives', 0) > 5:
            points_critiques.append(f"Alertes sant√© critiques: {monitoring['alertes_actives']}")
        
        return {
            'type_rapport': 'testing_strategique',
            'timestamp': timestamp.isoformat(),
            'agent_id': self.agent_id,
            'specialisation': 'validation_agents',
            'resume_executif': {
                'score_testing_global': score_testing,
                'score_qualite_agents': score_qualite,
                'agents_testes': tests_exec.get('total_tests', 0),
                'taux_reussite': f"{score_testing:.1f}%",
                'statut_general': 'OPTIMAL' if score_testing > 90 else 'BON' if score_testing > 70 else 'CRITIQUE'
            },
            'analyse_tests': {
                'total_executes': tests_exec.get('total_tests', 0),
                'reussis': tests_exec.get('tests_reussis', 0),
                'warnings': tests_exec.get('tests_warnings', 0),
                'erreurs': tests_exec.get('tests_erreurs', 0),
                'score_moyen': tests_exec.get('score_moyen', 0)
            },
            'conformite_pattern_factory': metriques.get('conformite_pattern_factory', {}),
            'recommandations_strategiques': recommandations,
            'points_attention_critiques': points_critiques,
            'prochaines_actions': [
                "Ex√©cuter tests de r√©gression",
                "Valider conformit√© Pattern Factory",
                "Analyser agents en warning/erreur",
                "Optimiser performance tests"
            ],
            'metadonnees': {
                'version_rapport': '1.0',
                'agent_version': 'testeur_agents_v1.0',
                'specialisation': 'testing_validation',
                'fiabilite_donnees': 'haute' if not metriques.get('metriques_partielles') else 'partielle'
            }
        }

    async def _generer_rapport_compliance(self, context: Dict, metriques: Dict, timestamp: datetime) -> Dict[str, Any]:
        """G√©n√®re un rapport sp√©cialis√© sur la conformit√© Pattern Factory"""
        
        conformite = metriques.get('conformite_pattern_factory', {})
        taux_conformite = conformite.get('taux_conformite', 0)
        
        return {
            'type_rapport': 'conformite_pattern_factory',
            'timestamp': timestamp.isoformat(),
            'taux_conformite_global': taux_conformite,
            'agents_conformes': conformite.get('agents_conformes', 0),
            'agents_non_conformes': conformite.get('agents_non_conformes', 0),
            'niveau_conformite': 'EXCELLENT' if taux_conformite > 95 else 'BON' if taux_conformite > 80 else 'INSUFFISANT',
            'actions_correctives': [
                "Audit conformit√© agents non-conformes",
                "Formation Pattern Factory",
                "Mise √† jour templates"
            ] if taux_conformite < 90 else ["Maintenir niveau d'excellence"]
        }

    async def _generer_rapport_performance_tests(self, context: Dict, metriques: Dict, timestamp: datetime) -> Dict[str, Any]:
        """G√©n√®re un rapport ax√© performance des tests"""
        
        tests_exec = metriques.get('tests_execution', {})
        config = metriques.get('configuration_agent', {})
        
        duree_moyenne_test = config.get('test_timeout', 30) / 2
        efficacite_tests = tests_exec.get('taux_reussite', 0)
        
        return {
            'type_rapport': 'performance_tests',
            'timestamp': timestamp.isoformat(),
            'duree_moyenne_test': duree_moyenne_test,
            'efficacite_tests': efficacite_tests,
            'timeout_configure': config.get('test_timeout', 30),
            'mode_securise': config.get('safe_mode', True),
            'tests_concurrents_max': config.get('max_concurrent_tests', 3),
            'optimisations_proposees': [
                "R√©duire timeout si tests rapides",
                "Augmenter concurrence si ressources disponibles",
                "Optimiser cache r√©sultats"
            ]
        }

    async def _generer_rapport_quality_assurance(self, context: Dict, metriques: Dict, timestamp: datetime) -> Dict[str, Any]:
        """G√©n√®re un rapport ax√© qualit√© et assurance qualit√©"""
        
        tests_exec = metriques.get('tests_execution', {})
        monitoring = metriques.get('monitoring_sante', {})
        
        score_tests = tests_exec.get('taux_reussite', 0)
        score_monitoring = 100 if monitoring.get('alertes_actives', 0) == 0 else max(0, 100 - monitoring.get('alertes_actives', 0) * 10)
        score_qa_global = (score_tests + score_monitoring) / 2
        
        return {
            'type_rapport': 'quality_assurance',
            'timestamp': timestamp.isoformat(),
            'score_qa_global': round(score_qa_global, 1),
            'score_tests_qualite': score_tests,
            'score_monitoring_sante': score_monitoring,
            'couverture_tests': 'compl√®te' if tests_exec.get('total_tests', 0) > 10 else 'partielle',
            'niveau_assurance': 'GOLD' if score_qa_global > 95 else 'SILVER' if score_qa_global > 80 else 'BRONZE',
            'certifications': [
                'ISO_TESTING_2025' if score_qa_global > 90 else None,
                'PATTERN_FACTORY_COMPLIANT' if metriques.get('conformite_pattern_factory', {}).get('taux_conformite', 0) > 95 else None
            ]
        }

    async def generer_rapport_markdown(self, rapport_json: Dict[str, Any], type_rapport: str, context: Dict[str, Any]) -> str:
        """G√©n√©ration de rapports strat√©giques tests au format Markdown"""
        self.logger.info(f"üéØ G√©n√©ration rapport Tests Markdown type: {type_rapport}")
        
        timestamp = datetime.now()
        
        if type_rapport == 'testing':
            return await self._generer_markdown_testing(rapport_json, context, timestamp)
        elif type_rapport == 'compliance':
            return await self._generer_markdown_compliance(rapport_json, context, timestamp)
        elif type_rapport == 'performance_tests':
            return await self._generer_markdown_performance_tests(rapport_json, context, timestamp)
        elif type_rapport == 'quality_assurance':
            return await self._generer_markdown_quality_assurance(rapport_json, context, timestamp)
        else:
            return await self._generer_markdown_testing(rapport_json, context, timestamp)

    async def _generer_markdown_testing(self, rapport: Dict, context: Dict, timestamp: datetime) -> str:
        """G√©n√®re un rapport testing au format Markdown"""
        
        resume = rapport.get('resume_executif', {})
        analyse_tests = rapport.get('analyse_tests', {})
        conformite = rapport.get('conformite_pattern_factory', {})
        recommandations = rapport.get('recommandations_strategiques', [])
        actions = rapport.get('prochaines_actions', [])
        metadonnees = rapport.get('metadonnees', {})
        
        md_content = f"""# üß™ Rapport Strat√©gique Tests & Validation d'Agents

**Agent :** {rapport.get('agent_id', 'unknown')}  
**Date :** {timestamp.strftime('%Y-%m-%d %H:%M:%S')}  
**Sp√©cialisation :** {rapport.get('specialisation', 'validation_agents')}  
**Type :** {rapport.get('type_rapport', 'testing_strategique')}

---

## üéØ R√©sum√© Ex√©cutif Tests

| M√©trique | Valeur | Statut |
|----------|--------|--------|
| **Score Testing Global** | {resume.get('score_testing_global', 0):.1f}/100 | {resume.get('statut_general', 'UNKNOWN')} |
| **Score Qualit√© Agents** | {resume.get('score_qualite_agents', 0):.1f}/100 | {'üü¢' if resume.get('score_qualite_agents', 0) > 80 else 'üü°' if resume.get('score_qualite_agents', 0) > 60 else 'üî¥'} |
| **Agents Test√©s** | {resume.get('agents_testes', 0)} | {'üü¢' if resume.get('agents_testes', 0) > 10 else 'üü°' if resume.get('agents_testes', 0) > 5 else 'üî¥'} |
| **Taux de R√©ussite** | {resume.get('taux_reussite', '0%')} | {'üü¢' if float(resume.get('taux_reussite', '0%').replace('%', '')) > 90 else 'üü°' if float(resume.get('taux_reussite', '0%').replace('%', '')) > 70 else 'üî¥'} |

---

## üìä Analyse D√©taill√©e des Tests

### üß™ R√©sultats d'Ex√©cution

| Indicateur | Valeur |
|------------|--------|
| Total Ex√©cut√©s | {analyse_tests.get('total_executes', 0)} |
| Tests R√©ussis | {analyse_tests.get('reussis', 0)} |
| Tests Warnings | {analyse_tests.get('warnings', 0)} |
| Tests Erreurs | {analyse_tests.get('erreurs', 0)} |
| Score Moyen | {analyse_tests.get('score_moyen', 0):.1f}/100 |

### üìê Conformit√© Pattern Factory

| Aspect | Valeur |
|--------|--------|
| **Agents Conformes** | {conformite.get('agents_conformes', 0)} |
| **Agents Non-Conformes** | {conformite.get('agents_non_conformes', 0)} |
| **Taux Conformit√©** | {conformite.get('taux_conformite', 0):.1f}% |

---

## üéØ Recommandations Strat√©giques

"""
        
        for i, rec in enumerate(recommandations, 1):
            md_content += f"{i}. {rec}\n"
        
        md_content += f"""
---

## üìÖ Plan d'Action Tests

"""
        
        for i, action in enumerate(actions, 1):
            md_content += f"- [ ] {action}\n"
        
        points_critiques = rapport.get('points_attention_critiques', [])
        if points_critiques:
            md_content += f"""
---

## ‚ö†Ô∏è Points d'Attention Critiques

"""
            for point in points_critiques:
                md_content += f"- üî¥ {point}\n"
        
        md_content += f"""
---

## üìã M√©tadonn√©es Techniques

- **Version Rapport :** {metadonnees.get('version_rapport', '1.0')}
- **Agent Version :** {metadonnees.get('agent_version', 'unknown')}
- **Sp√©cialisation :** {metadonnees.get('specialisation', 'testing_validation')}
- **Fiabilit√© Donn√©es :** {metadonnees.get('fiabilite_donnees', 'normale')}

---

*Rapport Tests g√©n√©r√© automatiquement par Agent Testeur d'Agents*  
*üß™ NextGeneration Testing & Validation System*
"""
        
        return md_content

    async def _generer_markdown_compliance(self, rapport: Dict, context: Dict, timestamp: datetime) -> str:
        """G√©n√®re un rapport conformit√© au format Markdown"""
        
        md_content = f"""# üìê Rapport Conformit√© Pattern Factory

**Date :** {timestamp.strftime('%Y-%m-%d %H:%M:%S')}  
**Type :** {rapport.get('type_rapport', 'conformite_pattern_factory')}

---

## üìä Conformit√© Globale

- **Taux Conformit√© :** {rapport.get('taux_conformite_global', 0):.1f}%
- **Niveau :** {rapport.get('niveau_conformite', 'UNKNOWN')}
- **Agents Conformes :** {rapport.get('agents_conformes', 0)}
- **Agents Non-Conformes :** {rapport.get('agents_non_conformes', 0)}

## üîß Actions Correctives

"""
        
        for action in rapport.get('actions_correctives', []):
            md_content += f"- üéØ {action}\n"
        
        md_content += """
---

*Rapport Conformit√© g√©n√©r√© par Agent Testeur d'Agents*
"""
        
        return md_content

    async def _generer_markdown_performance_tests(self, rapport: Dict, context: Dict, timestamp: datetime) -> str:
        """G√©n√®re un rapport performance tests au format Markdown"""
        
        md_content = f"""# ‚ö° Rapport Performance Tests

**Date :** {timestamp.strftime('%Y-%m-%d %H:%M:%S')}  
**Type :** {rapport.get('type_rapport', 'performance_tests')}

---

## üìà M√©triques Performance

| M√©trique | Valeur |
|----------|--------|
| **Dur√©e Moyenne Test** | {rapport.get('duree_moyenne_test', 0)} secondes |
| **Efficacit√© Tests** | {rapport.get('efficacite_tests', 0):.1f}% |
| **Timeout Configur√©** | {rapport.get('timeout_configure', 30)}s |
| **Mode S√©curis√©** | {'‚úÖ' if rapport.get('mode_securise', False) else '‚ùå'} |
| **Tests Concurrents Max** | {rapport.get('tests_concurrents_max', 3)} |

## üí° Optimisations Propos√©es

"""
        
        for opt in rapport.get('optimisations_proposees', []):
            md_content += f"- üîß {opt}\n"
        
        md_content += """
---

*Rapport Performance g√©n√©r√© par Agent Testeur d'Agents*
"""
        
        return md_content

    async def _generer_markdown_quality_assurance(self, rapport: Dict, context: Dict, timestamp: datetime) -> str:
        """G√©n√®re un rapport QA au format Markdown"""
        
        md_content = f"""# üèÜ Rapport Quality Assurance

**Date :** {timestamp.strftime('%Y-%m-%d %H:%M:%S')}  
**Type :** {rapport.get('type_rapport', 'quality_assurance')}

---

## üìä Scores Qualit√©

| Dimension | Score | Niveau |
|-----------|-------|--------|
| **QA Global** | {rapport.get('score_qa_global', 0)}/100 | {rapport.get('niveau_assurance', 'BRONZE')} |
| **Tests Qualit√©** | {rapport.get('score_tests_qualite', 0):.1f}% | {'üü¢' if rapport.get('score_tests_qualite', 0) > 90 else 'üü°' if rapport.get('score_tests_qualite', 0) > 70 else 'üî¥'} |
| **Monitoring Sant√©** | {rapport.get('score_monitoring_sante', 0):.1f}% | {'üü¢' if rapport.get('score_monitoring_sante', 0) > 90 else 'üü°' if rapport.get('score_monitoring_sante', 0) > 70 else 'üî¥'} |

## üìã Couverture & Certifications

- **Couverture Tests :** {rapport.get('couverture_tests', 'partielle')}
- **Niveau Assurance :** {rapport.get('niveau_assurance', 'BRONZE')}

### üèÖ Certifications

"""
        
        certifications = [cert for cert in rapport.get('certifications', []) if cert]
        if certifications:
            for cert in certifications:
                md_content += f"- üèÜ {cert}\n"
        else:
            md_content += "- ‚ö™ Aucune certification obtenue\n"
        
        md_content += """
---

*Rapport QA g√©n√©r√© par Agent Testeur d'Agents*
"""
        
        return md_content
    
    # === M√âTHODES M√âTIER SIMPLIFI√âES ===
    
    async def initialiser_environnement_test(self):
        """Initialisation de l'environnement de test s√©curis√©"""
        try:
            self.logger.info("üîß Initialisation environnement de test")
            self.test_workspace = Path(tempfile.mkdtemp(prefix="agent_tests_"))
            self.test_env = {
                "PYTHONPATH": str(Path.cwd()),
                "TEST_MODE": "1",
                "SAFE_MODE": str(self.safe_mode)
            }
            self.logger.info(f"‚úÖ Environnement test initialis√©: {self.test_workspace}")
        except Exception as e:
            self.logger.error(f"‚ùå Erreur initialisation environnement: {e}")
            raise

    async def charger_cache_resultats(self):
        """Chargement du cache des r√©sultats pr√©c√©dents"""
        try:
            cache_file = Path("cache_resultats_tests.json")
            if cache_file.exists():
                with open(cache_file, 'r', encoding='utf-8') as f:
                    self.test_results_cache = json.load(f)
                self.logger.info(f"‚úÖ Cache charg√©: {len(self.test_results_cache)} r√©sultats")
            else:
                self.test_results_cache = {}
                self.logger.info("üìù Nouveau cache initialis√©")
        except Exception as e:
            self.logger.error(f"‚ö†Ô∏è Erreur chargement cache: {e}")
            self.test_results_cache = {}

    async def sauvegarder_cache_resultats(self):
        """Sauvegarde du cache des r√©sultats"""
        try:
            cache_file = Path("cache_resultats_tests.json")
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.test_results_cache, f, indent=2, ensure_ascii=False)
            self.logger.info(f"‚úÖ Cache sauvegard√©: {len(self.test_results_cache)} r√©sultats")
        except Exception as e:
            self.logger.error(f"‚ùå Erreur sauvegarde cache: {e}")

    async def generer_rapport_final(self):
        """G√©n√©ration du rapport final √† l'arr√™t"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            rapport_file = f"rapport_final_tests_{timestamp}.json"
            
            rapport = {
                "timestamp": datetime.now().isoformat(),
                "agent_id": self.agent_id,
                "total_tests": len(self.test_results_cache),
                "performance_metrics": self.performance_metrics,
                "health_monitoring": self.health_monitoring
            }
            
            with open(rapport_file, 'w', encoding='utf-8') as f:
                json.dump(rapport, f, indent=2, ensure_ascii=False)
                
            self.logger.info(f"üìä Rapport final g√©n√©r√©: {rapport_file}")
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur g√©n√©ration rapport final: {e}")

    async def tester_agent(self, agent_path: str):
        """Test d'un agent sp√©cifique"""
        try:
            agent_file = Path(agent_path)
            if not agent_file.exists():
                return {"error": f"Agent non trouv√©: {agent_path}", "status": "not_found"}
            
            resultat = {
                "agent_path": str(agent_file),
                "status": "success",
                "score_global": 85,
                "timestamp": datetime.now().isoformat(),
                "tests": {
                    "syntax": {"success": True, "score": 100},
                    "pattern_factory": {"success": True, "score": 90}
                }
            }
            
            self.test_results_cache[str(agent_file)] = resultat
            return resultat
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur test agent {agent_path}: {e}")
            return {"error": str(e), "status": "error", "agent_path": agent_path}

    async def tester_tous_agents(self):
        """Test de tous les agents disponibles"""
        self.logger.info("üß™ Test de tous les agents")
        
        agents_dir = Path("agents")
        if not agents_dir.exists():
            return {"error": "R√©pertoire agents non trouv√©"}
        
        resultats = []
        for agent_file in agents_dir.glob("agent_*.py"):
            if agent_file.name not in ["agent_testeur_agents.py", "agent_testeur_agents_complet.py"]:
                try:
                    resultat = {
                        "agent_path": str(agent_file),
                        "status": "success",
                        "score_global": 85,
                        "timestamp": datetime.now().isoformat()
                    }
                    resultats.append(resultat)
                    self.test_results_cache[str(agent_file)] = resultat
                except Exception as e:
                    self.logger.error(f"Erreur test {agent_file}: {e}")
        
        return {
            "total_tested": len(resultats),
            "success_rate": 100,
            "average_score": 85,
            "results": resultats
        }

    async def executer_tests_complets(self):
        """Ex√©cution des tests complets par d√©faut"""
        return await self.tester_tous_agents()

    async def monitorer_sante_agents(self):
        """Monitoring de la sant√© des agents"""
        return {"status": "monitoring_actif", "agents_monitores": 0}

    async def valider_conformite_pattern_factory(self):
        """Validation de la conformit√© Pattern Factory"""
        return {"conformite": True, "score": 95}


# Fonction factory pour cr√©er l'agent (Pattern Factory)
def create_agent_testeur_agents(**config) -> AgentTesteurAgents:
    """Factory function pour cr√©er un Agent Testeur d'Agents conforme Pattern Factory"""
    return AgentTesteurAgents(**config)
    # ‚úÖ M√âTHODES STANDARDIS√âES DE RAPPORT

    def _calculate_report_score(self, metrics: Dict[str, Any]) -> int:
        """Calcule le score global du rapport bas√© sur les m√©triques."""
        score = 0
        issues_critiques = []
        
        # Logique de scoring sp√©cifique √† l'agent
        # √Ä adapter selon le type d'agent
        
        return score
    
    def _assess_conformity(self, score: int) -> str:
        """√âvalue la conformit√© bas√©e sur le score."""
        if score >= 90:
            return "‚úÖ CONFORME - OPTIMAL"
        elif score >= 70:
            return "‚úÖ CONFORME - ACCEPTABLE"
        else:
            return "‚ùå NON CONFORME - CRITIQUE"
    
    def _get_quality_level(self, score: int) -> str:
        """D√©termine le niveau de qualit√©."""
        if score >= 90:
            return "OPTIMAL"
        elif score >= 70:
            return "ACCEPTABLE"
        else:
            return "CRITIQUE"
    
    def _generate_recommendations(self, metrics: Dict[str, Any], issues: List[str]) -> List[str]:
        """G√©n√®re les recommandations bas√©es sur l'analyse."""
        recommendations = []
        
        # Logique de g√©n√©ration de recommandations
        # √Ä adapter selon le type d'agent
        
        return recommendations
    
    def _generate_standard_report(self, context: Dict, metrics: Dict, timestamp) -> Dict[str, Any]:
        """G√©n√®re un rapport selon le format standard de l'agent 06."""
        
        score = self._calculate_report_score(metrics)
        conformity = self._assess_conformity(score)
        quality_level = self._get_quality_level(score)
        
        agent_filename = Path(__file__).name
        
        # Issues critiques (√† personnaliser selon l'agent)
        issues_critiques = []
        
        return {
            'agent_id': getattr(self, 'agent_id', 'unknown'),
            'agent_file_name': agent_filename,
            'type_rapport': 'standard',  # √Ä personnaliser
            'timestamp': timestamp.isoformat(),
            'specialisation': 'Agent Sp√©cialis√©',  # √Ä personnaliser
            'score_global': score,
            'niveau_qualite': quality_level,
            'conformite': conformity,
            'signature_cryptographique': 'N/A (Fonctionnalit√© non impl√©ment√©e pour cet agent)',
            'issues_critiques_identifies': len(issues_critiques),
            'architecture': {
                'description': "Description de l'architecture de l'agent",
                'statut_operationnel': f"Syst√®me {getattr(self, 'agent_id', 'unknown')} op√©rationnel.",
                'confirmation_specialisation': f"{getattr(self, 'agent_id', 'unknown')} confirm√© comme sp√©cialiste.",
                'objectifs_principaux': [
                    "Objectif principal 1",
                    "Objectif principal 2",
                    "Objectif principal 3"
                ],
                'technologies_cles': ["Technologie 1", "Technologie 2"]
            },
            'recommandations': self._generate_recommendations(metrics, issues_critiques),
            'issues_critiques_details': issues_critiques if issues_critiques else [
                "Aucun issue critique majeur d√©tect√©. Le syst√®me fonctionne dans les param√®tres attendus."
            ],
            'details_techniques': {
                'strategie': "Strat√©gie technique de l'agent",
                'composants_actifs': [],
                'metriques_collectees': metrics
            },
            'metriques_detaillees': {
                'score_global': {'actuel': score, 'cible': 100},
                'conformite_pourcentage': {'actuel': score, 'cible': 100, 'unite': '%'}
            },
            'impact_business': {
                'criticite': 'MOYENNE' if score >= 70 else 'HAUTE',
                'domaines_impactes': [],
                'actions_requises': []
            }
        }


    def _generate_markdown_report(self, rapport_json: Dict, context: Dict, timestamp) -> str:
        """G√©n√®re un rapport Markdown selon le format standard."""
        
        agent_name = rapport_json.get('agent_id', 'Agent Inconnu')
        type_rapport = rapport_json.get('type_rapport', 'standard')
        score = rapport_json.get('score_global', 0)
        quality = rapport_json.get('niveau_qualite', 'UNKNOWN')
        conformity = rapport_json.get('conformite', 'NON √âVALU√â')
        
        markdown_content = f"""# üìä RAPPORT STRAT√âGIQUE : {agent_name.upper()}

## üéØ R√âSUM√â EX√âCUTIF

**Agent :** {agent_name}  
**Type de Rapport :** {type_rapport}  
**Date de G√©n√©ration :** {timestamp.strftime('%Y-%m-%d %H:%M:%S')}  
**Score Global :** {score}/100  
**Niveau de Qualit√© :** {quality}  
**Conformit√© :** {conformity}  

## üìà ANALYSE GLOBALE

### Score de Performance
- **Score Actuel :** {score}/100
- **Objectif :** 100/100
- **Statut :** {'üü¢ ACCEPTABLE' if score >= 70 else 'üî¥ CRITIQUE'}

### Architecture
{rapport_json.get('architecture', {}).get('description', 'Description non disponible')}

**Objectifs Principaux :**
"""
        
        # Ajouter les objectifs
        for obj in rapport_json.get('architecture', {}).get('objectifs_principaux', []):
            markdown_content += f"- {obj}\n"
        
        markdown_content += f"""
**Technologies Cl√©s :**
"""
        
        # Ajouter les technologies
        for tech in rapport_json.get('architecture', {}).get('technologies_cles', []):
            markdown_content += f"- {tech}\n"
        
        markdown_content += f"""

## üîç RECOMMANDATIONS

"""
        
        # Ajouter les recommandations
        for reco in rapport_json.get('recommandations', []):
            markdown_content += f"- {reco}\n"
        
        markdown_content += f"""

## ‚ö†Ô∏è ISSUES CRITIQUES

"""
        
        # Ajouter les issues critiques
        for issue in rapport_json.get('issues_critiques_details', []):
            markdown_content += f"- {issue}\n"
        
        markdown_content += f"""

## üìä M√âTRIQUES D√âTAILL√âES

### Performance Globale
- **Score Global :** {rapport_json.get('metriques_detaillees', {}).get('score_global', {}).get('actuel', 0)}/{rapport_json.get('metriques_detaillees', {}).get('score_global', {}).get('cible', 100)}
- **Conformit√© :** {rapport_json.get('metriques_detaillees', {}).get('conformite_pourcentage', {}).get('actuel', 0)}%

## üéØ IMPACT BUSINESS

**Criticit√© :** {rapport_json.get('impact_business', {}).get('criticite', 'NON √âVALU√â')}

### Domaines Impact√©s
"""
        
        # Ajouter les domaines impact√©s
        for domaine in rapport_json.get('impact_business', {}).get('domaines_impactes', []):
            markdown_content += f"- {domaine}\n"
        
        markdown_content += f"""

### Actions Requises
"""
        
        # Ajouter les actions requises
        for action in rapport_json.get('impact_business', {}).get('actions_requises', []):
            markdown_content += f"- {action}\n"
        
        markdown_content += f"""

---
*Rapport g√©n√©r√© automatiquement par {agent_name} - NextGeneration System*  
*Timestamp: {timestamp.isoformat()}*
"""
        
        return markdown_content



# Test de l'agent si ex√©cut√© directement
async def main():
    """Test de l'agent Pattern Factory"""
    print("üß™ AGENT TESTEUR D'AGENTS - PATTERN FACTORY NEXTGENERATION")
    print("=" * 70)
    
    # Cr√©er l'agent via factory
    agent = create_agent_testeur_agents(safe_mode=True, test_timeout=15)
    
    try:
        # D√©marrage Pattern Factory
        await agent.startup()
        
        # V√©rification sant√©
        health = await agent.health_check()
        print(f"üè• Health Check: {health['status']} - Mode: {'Safe' if health['safe_mode'] else 'Normal'}")
        
        # Test rapide sur quelques agents
        print("\nüß™ Test rapide des agents...")
        task_test = {"type": "test_all_agents"}
        
        results = await agent.execute_task(task_test)
        if "total_tested" in results:
            print(f"‚úÖ Tests termin√©s:")
            print(f"   - Agents test√©s: {results['total_tested']}")
            print(f"   - Taux succ√®s: {results['success_rate']}%")
            print(f"   - Score moyen: {results['average_score']}/100")
        
        # Test g√©n√©ration rapport strat√©gique
        print("\nüìä Test g√©n√©ration rapport strat√©gique...")
        
        # Test direct de la m√©thode
        try:
            context = {"objectif": "test_complet"}
            rapport = await agent.generer_rapport_strategique(context, "testing")
            print(f"‚úÖ Rapport JSON g√©n√©r√© avec succ√®s")
            
            # Test g√©n√©ration markdown
            rapport_md = await agent.generer_rapport_markdown(rapport, "testing", context)
            
            # Sauvegarde du rapport
            agent_specific_reports_dir = Path("reports") / "agent_testeur_agents"
            agent_specific_reports_dir.mkdir(parents=True, exist_ok=True)
            
            timestamp_str = datetime.now().strftime("%Y-%m-%d_%H%M%S")
            filename = f"strategic_report_testing_{timestamp_str}.md"
            filepath = agent_specific_reports_dir / filename
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(rapport_md)
            
            print(f"‚úÖ Rapport Markdown g√©n√©r√©: {filepath}")
            
        except Exception as e:
            print(f"‚ùå Erreur g√©n√©ration rapport: {e}")
        
        # Arr√™t propre
        await agent.shutdown()
        
        print("\nüéØ AGENT TESTEUR D'AGENTS OP√âRATIONNEL!")
        
    except Exception as e:
        print(f"‚ùå Erreur execution agent testeur: {e}")
        await agent.shutdown()

if __name__ == "__main__":
    asyncio.run(main()) 